{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # List all files in the directory\n    files = []\n    try:\n        files = ftp.nlst()\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # Download each file using wget\n    downloaded_files = []\n    for file in files:\n        try:\n            # Construct the wget command\n            wget_command = f\"wget -O {file} ftp://{ftp_user}:{ftp_password}@{ftp_server}/{ftp_dir}/{file}\"\n            # Execute the wget command using subprocess\n            subprocess.run(wget_command, shell=True, check=True)\n            downloaded_files.append(file)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {file}: {str(e)}\")\n    \n    return downloaded_files"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        commands = [row[0] for row in reader]\n    \n    output_files = []\n    for index, command in enumerate(commands):\n        output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n        with open(output_file_path, 'w') as output_file:\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                output_file.write(result.stdout.decode())\n            except subprocess.CalledProcessError as e:\n                output_file.write(f\"Command failed with exit code {e.returncode}:\\n{e.stderr.decode()}\")\n            output_files.append(output_file_path)\n    \n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, terminate and restart it\n            proc.terminate()\n            time.sleep(2)  # Wait for the process to terminate\n            # Start the process again\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # Process is not running, start it\n    subprocess.Popen([process_name])\n    return f\"Process not found. Starting {process_name}.\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    files = glob.glob(os.path.join(directory, '*'))\n    if not files:\n        return None\n    \n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in files:\n            if os.path.isfile(file):\n                zipf.write(file, os.path.basename(file))\n    \n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Check if there are any words left after removing URLs\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate word cloud\n    wordcloud = WordCloud().generate(' '.join(words))\n    \n    # Display the word cloud\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    # Split the dataframe into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the Random Forest Classifier\n    clf = RandomForestClassifier()\n    clf.fit(X, y)\n    \n    # Get feature importances\n    importances = clf.feature_importances_\n    \n    # Create a dataframe to hold feature names and their importances\n    feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n    \n    # Sort the dataframe by importance in descending order\n    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n    \n    # Plot the bar plot\n    plt.figure(figsize=(10,6))\n    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n    \n    # Return the trained classifier and the axes object of the plot\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = password\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\n\nusers = [\n    User(1, 'user1', generate_password_hash('password1')),\n    User(2, 'user2', generate_password_hash('password2'))\n]\n\n@login_manager.user_loader\ndef load_user(user_id):\n    for user in users:\n        if user.id == int(user_id):\n            return user\n    return None\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    login_manager.init_app(app)\n\n    @app.route('/')\n    def home():\n        return render_template('home.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            for user in users:\n                if user.username == form.username.data and check_password_hash(user.password, form.password.data):\n                    login_user(user)\n                    return redirect(url_for('protected'))\n            return 'Invalid username or password'\n        return render_template('login.html', form=form)\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return 'Logged in as: ' + current_user.username\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return 'Logged out'\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Standardize the specified column\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data_scaled))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)\n    \n    # Remove outliers\n    data_no_outliers = data.drop(data.index[outliers])\n    \n    # Plotting\n    plt.figure(figsize=(12, 6))\n    \n    # Plot with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data_scaled, c='blue', label='Original Data')\n    plt.scatter(outliers, data_scaled[outliers], c='red', label='Outliers')\n    plt.title('Data with Outliers')\n    plt.legend()\n    \n    # Plot without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_no_outliers.index, scaler.transform(data_no_outliers[[column]]), c='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.legend()\n    \n    plt.show()\n    \n    # Return original data, data without outliers, and outliers indices\n    return (data, data_no_outliers, outliers)\n\n# Test the function"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters < 2:\n        raise ValueError(\"n_clusters must be an integer greater than 1\")\n    \n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n    centroids = kmeans.cluster_centers_\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n    \n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Create a DataFrame for the transformed data\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n    \n    # Create a scatter plot\n    plt.figure(figsize=(8,6))\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c='b', s=50, alpha=0.7)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.title('PCA of the Dataset')\n    plt.grid(True)\n    \n    # Get the current axes\n    ax = plt.gca()\n    \n    return transformed_df, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target_names[iris.target]\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    pairplot = sns.pairplot(df, hue='species')\n\n    # Set the title\n    pairplot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n\n    # Label the axes\n    for ax in pairplot.axes.flatten():\n        ax.set_xlabel(ax.get_xlabel(), fontsize=10)\n        ax.set_ylabel(ax.get_ylabel(), fontsize=10)\n\n    # Show the plot\n    plt.show()\n\n    # Return the figure object\n    return pairplot.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        today = datetime.today()\n        dates = [today - pd.DateOffset(days=i) for i in range(30)]\n        \n        # Generate random values\n        values = [random.randint(0, 100) for _ in range(30)]\n        \n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        \n        # Plot the data\n        plt.figure(figsize=(10, 5))\n        plt.plot(df['Date'], df['Value'], label='Random Time Series Data')\n        plt.xlabel('Date', fontname='Arial')\n        plt.ylabel('Value', fontname='Arial')\n        plt.title('Random Time Series Data', fontname='Arial')\n        plt.legend()\n        \n        # Show the plot\n        plt.show()\n        \n        # Return the Axes object\n        return plt.gca()\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n# Test the function"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    try:\n        # Load the Boston Housing dataset\n        data = pd.read_csv(data_url, header=None)\n        \n        # Assign column names to the dataset\n        column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n        data.columns = column_names\n        \n        # Compute the correlation matrix\n        corr = data.corr()\n        \n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n        \n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(11, 9))\n        \n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n        \n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n        \n        # Display the plot\n        plt.show()\n        \n        # Save the plot to a specified file\n        # plt.savefig('heatmap.png')\n        \n        # Return the Axes object containing the heatmap plot\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred in generating or saving the plot: {e}\")\n\n# Test the function\ntask_func()"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Parameters:\n    - df (pd.DataFrame): DataFrame containing the time series data with a 'value' column.\n    - freq (str): Frequency string for the time series.\n    - decomposition_model (str): Model type for decomposition, either 'additive' or 'multiplicative'.\n\n    Returns:\n    - tuple: A tuple containing the decomposition result (DecomposeResult object) and the matplotlib Axes object.\n\n    Raises:\n    - ValueError: If 'df' is not a DataFrame, lacks required columns, or contains invalid data types.\n    - ValueError: If 'freq' is not a valid frequency string.\n    - ValueError: If 'decomposition_model' is not 'additive' or 'multiplicative'.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df must be a pandas DataFrame.\")\n    \n    # Check if 'value' column exists\n    if 'value' not in df.columns:\n        raise ValueError(\"df must contain a 'value' column.\")\n    \n    # Check data types in 'value' column\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data.\")\n    \n    # Check if freq is a valid frequency string\n    valid_freqs = ['D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S']\n    if freq not in valid_freqs:\n        raise ValueError(f\"Invalid frequency string. Valid frequencies are: {', '.join(valid_freqs)}\")\n    \n    # Check if decomposition_model is either 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"decomposition_model must be either 'additive' or 'multiplicative'.\")\n    \n    # Set the index to datetime with the specified frequency\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    df.index.freq = freq\n    \n    # Perform decomposition\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model)\n    \n    # Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(10, 12), sharex=True)\n    decomposition.plot(ax=axes)\n    plt.tight_layout()\n    \n    return decomposition, axes"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date\")\n    \n    random_seed(seed)\n    delta = end_date - start_date\n    days = delta.days + 1  # inclusive of both start and end dates\n    random_days = [randint(0, days - 1) for _ in range(days)]\n    random_dates = [start_date + timedelta(days=d) for d in random_days]\n    \n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(\"Directory not found\")\n    \n    # Add '12' to the list\n    my_list.append('12')\n    \n    # Sum the numbers in the list\n    num_files = sum(int(item) for item in my_list if item.isdigit())\n    \n    # Get the list of CSV files in the directory\n    csv_files = glob.glob(os.path.join(file_dir, '*' + file_ext))\n    \n    if not csv_files:\n        raise FileNotFoundError(\"No CSV files found in the directory\")\n    \n    # Read and concatenate the CSV files\n    dfs = [pd.read_csv(file) for file in csv_files[:num_files]]\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric values (int or float)\")\n    \n    # Enhance 'my_list' by appending the number 12\n    my_list.append(12)\n    \n    # Calculate the sum of elements in 'my_list'\n    total_sum = sum(my_list)\n    \n    # Limit the size by 'size'\n    if total_sum > size:\n        total_sum = size\n    \n    # Generate a list of random integers\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    \n    # Measure the time taken\n    start_time = time.time()\n    # Generate the list of random numbers\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    # Plot a histogram of the generated random numbers\n    plt.hist(random_numbers, bins=range(1, 101), align='left', rwidth=0.8)\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    plt.show()\n    \n    # Return the time taken and the Axes object for the histogram\n    return time_taken, plt.gca()\n\n# Test cases\ndef test_task_func():\n    # Test case 1: my_list is a list of numbers\n    time_taken, ax = task_func([1, 2, 3], size=100, seed=100)\n    assert isinstance(time_taken, float)\n    assert isinstance(ax, plt.Axes)\n    \n    # Test case 2: my_list contains non-numeric values\n    try:\n        task_func([1, 2, 'a'])\n    except ValueError as e:\n        assert str(e) == \"my_list must contain only numeric values (int or float)\"\n    \n    # Test case 3: my_list is not a list\n    try:\n        task_func('not a list')\n    except TypeError as e:\n        assert str(e) == \"my_list must be a list\"\n    \n    # Test case 4: size is less than the sum of my_list\n    time_taken, ax = task_func([1, 2, 3], size=5, seed=100)\n    assert time_taken > 0\n    assert isinstance(ax, plt.Axes)\n    \n    # Test case 5: seed is set\n    time_taken, ax = task_func([1, 2, 3], size=100, seed=42)\n    assert time_taken > 0\n    assert isinstance(ax, plt.Axes)\n\ntest_task_func()"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException as e:\n        raise ConnectionError(f\"Error connecting to {url}: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    tables = soup.find_all('table')\n\n    if not tables:\n        raise ValueError(\"No table data found on the page\")\n\n    first_table = tables[0]\n    headers = []\n    rows = []\n\n    header_row = first_table.find('tr')\n    if header_row:\n        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n    else:\n        # If no header, use default column names\n        headers = [f\"Column {i+1}\" for i in range(len(first_table.find('tr').find_all('td')))]\n\n    for row in first_table.find_all('tr'):\n        cells = row.find_all(['th', 'td'])\n        if cells:\n            rows.append([cell.get_text(strip=True) for cell in cells])\n\n    df = pd.DataFrame(rows, columns=headers)\n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if numeric_cols.empty:\n        raise ValueError(\"No numeric columns found in the DataFrame.\")\n    \n    axes = []\n    for col in numeric_cols:\n        ax = plt.subplot(len(numeric_cols), 1, numeric_cols.get_loc(col) + 1)\n        df[col].hist(ax=ax, bins=30)\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans a specified IP address range and checks if a specified port is open on each IP.\n    The function returns a dictionary with IP addresses as keys and a boolean indicating the port's status (True if open, False otherwise).\n\n    Args:\n    ip_range (str): The IP address range to scan, in CIDR notation (e.g., '192.168.1.0/24').\n    port (int): The port number to check on each IP address.\n\n    Returns:\n    dict: A dictionary mapping IP addresses to their port status (True if open, False otherwise).\n    \"\"\"\n    # Create a dictionary to store the results\n    results = {}\n\n    # Create a list to hold threads\n    threads = []\n\n    # Iterate over each IP in the IP range\n    for ip in IPv4Network(ip_range):\n        # Create a thread for each IP to check the port\n        t = Thread(target=check_port, args=(str(ip), port, results))\n        threads.append(t)\n        t.start()\n\n    # Wait for all threads to complete\n    for t in threads:\n        t.join()\n\n    return results\n\ndef check_port(ip, port, results):\n    \"\"\"\n    Checks if a specified port is open on a given IP address.\n\n    Args:\n    ip (str): The IP address to check.\n    port (int): The port number to check.\n    results (dict): The dictionary to store the results.\n    \"\"\"\n    try:\n        # Create a socket object\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        # Set a timeout for the socket\n        s.settimeout(1)\n        # Attempt to connect to the IP and port\n        result = s.connect_ex((ip, port))\n        # If the connection was successful, the port is open\n        if result == 0:\n            results[ip] = True\n        else:\n            results[ip] = False\n        # Close the socket\n        s.close()\n    except Exception as e:\n        # If there was an error, assume the port is closed\n        results[ip] = False\n\n# Test the function"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n    \n    # Initialize lists to hold extracted data\n    types = []\n    timestamps = []\n    messages = []\n    \n    # Read the log file\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = re.match(pattern, line)\n            if match:\n                types.append(match.group(1))\n                timestamps.append(match.group(2))\n                messages.append(match.group(3))\n            else:\n                # If no match, raise ValueError\n                raise ValueError(\"Invalid log entry format\")\n    \n    # Check if any valid log entries were found\n    if not types:\n        raise ValueError(\"No valid log entries found\")\n    \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n    \n    # Convert Timestamp to datetime format\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n    \n    # Create a new CSV file with the structured log data\n    csv_file = 'structured_logs.csv'\n    df.to_csv(csv_file, index=False)\n    \n    # Return the file path to the newly created CSV file\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Remove punctuation and split text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    if word_lengths:\n        plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), rwidth=rwidth)\n        plt.xlabel('Word Length')\n        plt.ylabel('Frequency')\n        plt.title('Distribution of Word Lengths')\n        plt.show()\n    else:\n        print(\"No words found in the text.\")\n    \n    return plt.gca()\n\n# Test cases\ndef test_task_func():\n    # Test case 1: Normal text\n    text1 = \"Hello world! This is a test.\"\n    result1 = task_func(text1)\n    assert isinstance(result1, plt.Axes), \"Result should be a matplotlib.axes.Axes object\"\n    \n    # Test case 2: Text with only one word\n    text2 = \"Single\"\n    result2 = task_func(text2)\n    assert isinstance(result2, plt.Axes), \"Result should be a matplotlib.axes.Axes object\"\n    \n    # Test case 3: Text with no words\n    text3 = \"!!!\"\n    result3 = task_func(text3)\n    assert isinstance(result3, plt.Axes), \"Result should be a matplotlib.axes.Axes object\"\n    # Since there are no words, the histogram should be empty\n    \n    # Test case 4: Text with words of varying lengths\n    text4 = \"Short long longer longest\"\n    result4 = task_func(text4)\n    assert isinstance(result4, plt.Axes), \"Result should be a matplotlib.axes.Axes object\"\n    \n    # Test case 5: Empty string\n    text5 = \"\"\n    result5 = task_func(text5)\n    assert isinstance(result5, plt.Axes), \"Result should be a matplotlib.axes.Axes object\"\n    # No words, so histogram should be empty\n    \n    print(\"All test cases passed.\")\n\n# Run test cases\ntest_task_func()"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\n\ndef task_func(df):\n    # Check if the DataFrame is empty or missing necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or missing necessary columns 'Title' and 'Content'\")\n\n    # Define the keywords to look for in titles\n    keywords = ['like', 'what']\n\n    # Initialize a list to hold articles that contain the keywords\n    matching_articles = []\n\n    # Iterate through each row in the DataFrame\n    for index, row in df.iterrows():\n        title = row['Title'].lower()\n        for keyword in keywords:\n            if keyword in title:\n                matching_articles.append(row)\n                break  # No need to check other keywords if one is found\n\n    # If no articles match the criteria, raise an exception\n    if not matching_articles:\n        raise ValueError(\"No articles found with the specified keywords in their titles\")\n\n    # Combine the content of all matching articles into a single string\n    combined_content = ' '.join([article['Content'] for article in matching_articles])\n\n    # Remove punctuation and convert to lowercase\n    combined_content = re.sub(f'[{re.escape(punctuation)}]', '', combined_content).lower()\n\n    # Tokenize the content into words\n    words = nltk.word_tokenize(combined_content)\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Return the dictionary of word frequencies\n    return dict(word_counts)\n\n# Test cases\ndef test_task_func():\n    # Test case 1: DataFrame with matching articles\n    df1 = pd.DataFrame({\n        'Title': ['I like apples', 'What is your name?', 'I love coding'],\n        'Content': ['This is the content of the first article.', 'This is the content of the second article.', 'This is the content of the third article.']\n    })\n    expected1 = {'this': 3, 'is': 3, 'the': 3, 'content': 3, 'of': 3, 'first': 1, 'second': 1, 'third': 1, 'article': 3, 'like': 1, 'apples': 1, 'what': 1, 'your': 1, 'name': 1, 'love': 1, 'coding': 1}\n    assert task_func(df1) == expected1, \"Test case 1 failed\"\n\n    # Test case 2: DataFrame with no matching articles\n    df2 = pd.DataFrame({\n        'Title': ['No match here', 'Another title'],\n        'Content': ['Content of first article.', 'Content of second article.']\n    })\n    try:\n        task_func(df2)\n        assert False, \"Test case 2 did not raise ValueError\"\n    except ValueError as e:\n        assert str(e) == \"No articles found with the specified keywords in their titles\", \"Test case 2 failed\"\n\n    # Test case 3: Empty DataFrame\n    df3 = pd.DataFrame()\n    try:\n        task_func(df3)\n        assert False, \"Test case 3 did not raise ValueError\"\n    except ValueError as e:\n        assert str(e) == \"DataFrame is empty or missing necessary columns 'Title' and 'Content'\", \"Test case 3 failed\"\n\n    # Test case 4: Missing 'Title' column\n    df4 = pd.DataFrame({\n        'Content': ['Content of first article.', 'Content of second article.']\n    })\n    try:\n        task_func(df4)\n        assert False, \"Test case 4 did not raise ValueError\"\n    except ValueError as e:\n        assert str(e) == \"DataFrame is empty or missing necessary columns 'Title' and 'Content'\", \"Test case 4 failed\"\n\n    # Test case 5: Missing 'Content' column\n    df5 = pd.DataFrame({\n        'Title': ['Title 1', 'Title 2']\n    })\n    try:\n        task_func(df5)\n        assert False, \"Test case 5 did not raise ValueError\"\n    except ValueError as e:\n        assert str(e) == \"DataFrame is empty or missing necessary columns 'Title' and 'Content'\", \"Test case 5 failed\"\n\n    print(\"All test cases passed!\")\n\n# Run test cases\ntest_task_func()"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Preprocess the text data\n    def preprocess_text(text):\n        # Remove numbers and punctuation\n        text = re.sub(r'[\\d\\W_]+', ' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        # Join the words back into a string\n        return ' '.join(words)\n    \n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    \n    # Vectorize the text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert the vectorized data into a DataFrame\n    feature_names = vectorizer.get_feature_names_out()\n    df_vectorized = pd.DataFrame(X.toarray(), columns=feature_names)\n    \n    return df_vectorized"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Missing 'Lon' or 'Lat' keys in the dictionary.\")\n    lon_range = dic['Lon']\n    lat_range = dic['Lat']\n    if not isinstance(lon_range, tuple) or not isinstance(lat_range, tuple):\n        raise ValueError(\"'Lon' and 'Lat' values must be tuples.\")\n    \n    city_coords = []\n    for city in cities:\n        lon = np.random.uniform(lon_range[0], lon_range[1])\n        lat = np.random.uniform(lat_range[0], lat_range[1])\n        city_coords.append((city, Point(lon, lat)))\n    \n    gdf = gpd.GeoDataFrame(city_coords, columns=['City', 'Coordinates'])\n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions must be strings\")\n    if not all(isinstance(tz, str) for tz in timezones.values()):\n        raise ValueError(\"All timezones must be strings\")\n    if not isinstance(seed, int):\n        raise ValueError(\"Seed must be an integer\")\n    \n    set_seed(seed)\n    report = []\n    for city in cities:\n        local_time = utc_datetime.astimezone(pytz.timezone(timezones[city]))\n        condition = weather_conditions[randint(0, len(weather_conditions)-1)]\n        report.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': condition\n        })\n    return pd.DataFrame(report)"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    random_walk = np.cumsum(steps)\n    \n    stats = {\n        'count': len(random_walk),\n        'mean': np.mean(random_walk),\n        'std': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th percentile': np.percentile(random_walk, 5),\n        '25th percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th percentile': np.percentile(random_walk, 75),\n        '95th percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk)\n    }\n    \n    fig, ax = plt.subplots()\n    ax.plot(random_walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Value')\n    \n    return stats, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Download the zip file from the URL\n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download file from {url}\")\n\n    # Save the zip file to a temporary location\n    temp_zip_path = os.path.join(destination_directory, 'temp.zip')\n    with open(temp_zip_path, 'wb') as f:\n        f.write(response.content)\n\n    # Extract the contents to the specified directory\n    with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Get the list of extracted files\n    extracted_files = [os.path.join(destination_directory, name) for name in zip_ref.namelist()]\n\n    # Remove the temporary zip file\n    os.remove(temp_zip_path)\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Parameters:\n    - seed (int): Seed for random number generation.\n    - image_size (tuple): Size of the image (height, width, channels).\n    - range_low (int): Lower bound for random pixel values.\n    - range_high (int): Upper bound for random pixel values.\n\n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object of the plot.\n    - image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n    - ValueError: If range_low is not less than range_high.\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n    ax = plt.imshow(image)\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file {audio_file} does not exist.\")\n    \n    # Read the audio file\n    audio_data, sr = sf.read(audio_file)\n    \n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(audio_data**2)))\n    \n    # Reshape the list L into an MxN matrix\n    matrix = np.array(L).reshape(M, N)\n    \n    # Normalize the matrix based on SPL\n    normalized_matrix = matrix / (10**(spl/20))\n    \n    # Generate a spectrogram from the normalized matrix\n    # Assuming the matrix represents time series data\n    # We can use librosa to generate the spectrogram\n    # First, we need to convert the matrix to a time series signal\n    # For simplicity, let's assume each row of the matrix represents a time series\n    # We can stack the rows to create a multi-channel signal\n    multi_channel_signal = np.array(matrix)\n    \n    # Generate the spectrogram\n    # We can use librosa's stft function to compute the short-time Fourier transform\n    # Then, we can take the magnitude and convert it to decibels\n    # Finally, we can plot the spectrogram using matplotlib\n    # But since the matrix is already normalized based on SPL, we can directly use it\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    \n    # Create a figure and axis for the spectrogram\n    fig, ax = plt.subplots()\n    \n    # Display the spectrogram\n    # We can use pcolormesh to display the matrix\n    # Since the matrix is normalized based on SPL, we can use it directly\n    # But to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    # But since the matrix is already normalized based on SPL, we can use it directly\n    # However, to make it more realistic, let's assume that the matrix represents the magnitude spectrogram\n    # So, we can plot it directly\n    #"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = []\n    for item in original:\n        if isinstance(item, tuple) and len(item) == 2:\n            # Assuming the tuple contains two numeric values\n            value1, value2 = item\n            if isinstance(value1, (int, float)) and isinstance(value2, (int, float)):\n                numeric_values.append(value1)\n                numeric_values.append(value2)\n    \n    # Convert the list of numeric values to a numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_array),\n        'std_dev': np.std(numeric_array),\n        'min': np.min(numeric_array),\n        'max': np.max(numeric_array)\n    }\n    \n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, density=True, alpha=0.6, bins='auto')\n    \n    # Fit a normal distribution to the data\n    mu, std = stats.norm.fit(numeric_array)\n    # Generate the PDF\n    pdf = stats.norm.pdf(numeric_array, mu, std)\n    # Plot the PDF\n    ax.plot(numeric_array, pdf, 'k-', linewidth=2)\n    \n    # Return the numpy array, statistics dictionary, and the axes object\n    return numeric_array, stats_dict, ax\n\n# Test the function\noriginal = [(1, 2), (3, 4), (5, 6)]\nnumeric_array, stats_dict, ax = task_func(original)"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using MinMaxScaler from sklearn\n    scaler = preprocessing.MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array.reshape(-1,1)).flatten()\n    \n    # Plotting the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array, label='Normalized')\n    ax.legend()\n    \n    # Return the original array, normalized array, and the axes object\n    return original_array, normalized_array, ax\n\n# Test the function\noriginal_data = [1, 2, 3, 4, 5]\noriginal, normalized, ax = task_func(original_data)\nplt.show()"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n    \n    # Generate a signal based on the values in \"data\"\n    # Assuming the keys in \"data\" are time points and values are signal values\n    time_points = sorted(data.keys())\n    signal = [data[tp] for tp in time_points]\n    \n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    \n    # Plot and return the FFT of the signal\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n    plt.plot(freqs, np.abs(fft_result))\n    plt.xlabel('Frequency [Hz]')\n    plt.ylabel('Amplitude')\n    plt.title('FFT of the Signal')\n    plt.show()\n    \n    return fft_result, plt.gca()\n\n# Test the function\ndata = {0: 1, 1: 2, 2: 3, 3: 4, 4: 3, 5: 2, 6: 1}\nfft_result, plot = task_func(data)"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_type = self.headers.get('Content-Type')\n        if content_type != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Content-Type header is not application/json')\n            return\n\n        content_length = int(self.headers.get('Content-Length', 0))\n        post_data = self.rfile.read(content_length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'No data key in request')\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\n\ndef task_func():\n    return RequestHandler\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            try:\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(b'Invalid JSON')\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            for key in required_keys:\n                if key not in email_data:\n                    self.send_response(400)\n                    self.end_headers()\n                    self.wfile.write(f'Missing {key} in email data'.encode())\n                    return\n\n            try:\n                server = smtplib.SMTP(smtp_server, smtp_port)\n                server.starttls()\n                server.login(smtp_username, smtp_password)\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n                server.sendmail(smtp_username, [email_data['to']], msg.as_string())\n                server.quit()\n                self.send_response(200)\n                self.end_headers()\n                self.wfile.write(b'Email sent successfully')\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.end_headers()\n                self.wfile.write(b'Authentication Failed')\n            except Exception as e:\n                self.send_response(500)\n                self.end_headers()\n                self.wfile.write(f'Internal Server Error: {str(e)}'.encode())\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    total_words = 0\n    file_counts = {}\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".txt\"):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    words = f.read().split()\n                    total_words += len(words)\n                    file_counts[file] = len(words)\n    with open(filename, 'w') as f:\n        json.dump(file_counts, f)\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns,\n    and optionally visualizes the correlation matrix using a heatmap.\n\n    Parameters:\n    - df (pd.DataFrame): Input DataFrame with columns 'Date' and 'Value' where 'Value' contains lists.\n    - plot (bool): If True, returns a matplotlib Axes object containing the heatmap plot.\n\n    Returns:\n    - pd.DataFrame: DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n    - Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n\n    Raises:\n    - ValueError: If the DataFrame is empty or 'Value' column contains invalid data.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n    \n    if 'Value' not in df.columns or 'Date' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Value' columns.\")\n    \n    if not df['Value'].apply(lambda x: isinstance(x, list)).all():\n        raise ValueError(\"'Value' column must contain lists.\")\n    \n    # Split lists in 'Value' column into separate columns\n    df = df.explode('Value').reset_index(drop=True)\n    \n    # Calculate Pearson correlation matrix\n    corr_matrix = df['Value'].corr(method='pearson')\n    \n    # Create a DataFrame from the correlation matrix\n    corr_df = pd.DataFrame(corr_matrix).reset_index()\n    corr_df.columns = ['Column1', 'Column2', 'Pearson_Correlation']\n    \n    if plot:\n        # Create a heatmap of the correlation matrix\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n        plt.title('Correlation Heatmap')\n        plt.show()\n        return corr_df, plt.gca()\n    else:\n        return corr_df"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Create a dictionary to hold the data\n    data = {field: [] for field in FIELDS}\n    for student in STUDENTS:\n        for field in FIELDS:\n            grade = random.randint(0, 100)\n            data[field].append(grade)\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject\n    avg_subject = df.mean()\n    avg_subject.name = 'Average'\n    df = df.append(avg_subject)\n    \n    # Add additional fields if provided\n    for field in additional_fields:\n        if field not in FIELDS:\n            df[field] = [random.randint(0, 100) for _ in range(len(STUDENTS) + 1)]\n    \n    return df"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate data for 100 people\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{random.randint(1, 1000)}\"\n        age = random.randint(18, 80)\n        height = random.uniform(1.5, 2.0)\n        weight = random.uniform(50, 100)\n        data.append([name, age, height, weight])\n    \n    # Calculate averages\n    ages = [person[1] for person in data]\n    heights = [person[2] for person in data]\n    weights = [person[3] for person in data]\n    avg_age = mean(ages)\n    avg_height = mean(heights)\n    avg_weight = mean(weights)\n    \n    # Append averages to the data\n    data.append([f\"Average\", avg_age, avg_height, avg_weight])\n    \n    # Write to CSV\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n    \n    return filename\n\n# Test the function"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Create a dictionary to hold the files organized by their first text outside of square brackets\n    organized_files = {}\n    \n    # Walk through all files in the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Read the first line of the file\n            with open(file_path, 'r') as f:\n                first_line = f.readline()\n            # Use regex to find the first text not enclosed in square brackets\n            match = re.search(r'(?<=\\[).+?(?=\\])', first_line)\n            if match:\n                # Extract the text\n                text = match.group().strip()\n                # If the text is already a key in the dictionary, append the file path to the list\n                if text in organized_files:\n                    organized_files[text].append(file_path)\n                else:\n                    # Otherwise, create a new key with the text and set the value to a list containing the file path\n                    organized_files[text] = [file_path]\n            else:\n                # If no matching text is found, skip the file\n                continue\n    \n    # Create subdirectories and move files\n    for subdir, files in organized_files.items():\n        subdir_path = os.path.join(directory, subdir)\n        os.makedirs(subdir_path, exist_ok=True)\n        for file in files:\n            shutil.move(file, subdir_path)\n    \n    # Return the directory path with organized files and the dictionary of organized files\n    return (directory, organized_files)"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    exit_codes = []\n    def run_file(file):\n        try:\n            result = subprocess.run(['python', file], capture_output=True, text=True)\n            exit_codes.append(result.returncode)\n        except Exception as e:\n            exit_codes.append(str(e))\n    \n    threads = []\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n    \n    for thread in threads:\n        thread.join()\n    \n    return exit_codes"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    # List to store the results\n    results = []\n    \n    # Find all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    for file in bat_files:\n        try:\n            # Run the .bat file\n            result = subprocess.run(file, capture_output=True, text=True, check=True)\n            # Append the file name and exit code to the results list\n            results.append((os.path.basename(file), result.returncode))\n        except subprocess.CalledProcessError as e:\n            # If the file could not be executed, append the file name and None\n            results.append((os.path.basename(file), None))\n    \n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' is not in the DataFrame.\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Histogram with KDE\n    if df[col].dtype in [int, float]:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(x=col, data=df, ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n    \n    # Box plot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n    \n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script not found: {script_path}\")\n    \n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, text=True)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr)\n\n# Test cases\ndef test_task_func():\n    # Test with a valid script and wait=True\n    try:\n        result = task_func('valid_script.py', wait=True, 'arg1', 'arg2')\n        assert result == 0, \"Expected return code 0\"\n        print(\"Test 1 passed\")\n    except Exception as e:\n        print(f\"Test 1 failed: {e}\")\n\n    # Test with a valid script and wait=False\n    try:\n        result = task_func('valid_script.py', wait=False, 'arg1', 'arg2')\n        assert result is None, \"Expected None\"\n        print(\"Test 2 passed\")\n    except Exception as e:\n        print(f\"Test 2 failed: {e}\")\n\n    # Test with a non-existent script\n    try:\n        task_func('nonexistent_script.py', wait=True)\n        print(\"Test 3 failed: Expected ValueError\")\n    except ValueError as e:\n        print(\"Test 3 passed\")\n    except Exception as e:\n        print(f\"Test 3 failed: Unexpected exception {e}\")\n\n    # Test with a script that raises an exception\n    try:\n        task_func('script_with_exception.py', wait=True)\n        print(\"Test 4 failed: Expected CalledProcessError\")\n    except subprocess.CalledProcessError as e:\n        print(\"Test 4 passed\")\n    except Exception as e:\n        print(f\"Test 4 failed: Unexpected exception {e}\")\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    try:\n        # Load data from Excel spreadsheet\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The Excel file does not exist at the specified path.\")\n    except KeyError:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n    \n    # Calculate mean and standard deviation of each column\n    mean = df.mean()\n    std_dev = df.std()\n    \n    # Create a dictionary with mean and standard deviation\n    stats = {'mean': mean, 'std_dev': std_dev}\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    x = np.arange(len(mean))\n    width = 0.35\n    rects1 = ax.bar(x - width/2, mean, width, label='Mean')\n    rects2 = ax.bar(x + width/2, std_dev, width, label='Standard Deviation')\n    \n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df.columns)\n    ax.legend()\n    \n    fig.tight_layout()\n    \n    return stats, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n    \n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n    \n    days = list(day_counts.keys())\n    counts = list(day_counts.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    random.seed(seed)\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        return \"No files to move.\"\n    file_to_move = random.choice(files)\n    src_file_path = os.path.join(src_dir, file_to_move)\n    dest_file_path = os.path.join(dest_dir, file_to_move)\n    shutil.move(src_file_path, dest_file_path)\n    return file_to_move"}
{"task_id": "BigCodeBench/374", "solution": "import re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Initialize counter for processed files\n    processed_files = 0\n\n    # Use glob to find all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    for file_path in xlsx_files:\n        try:\n            # Load the workbook\n            wb = load_workbook(file_path, data_only=True)\n            # Iterate over all sheets in the workbook\n            for sheet in wb.worksheets:\n                # Iterate over all cells in the sheet\n                for row in sheet.iter_rows(values_only=True):\n                    for cell in row:\n                        # Check if the cell contains a string\n                        if isinstance(cell, str):\n                            # Use regex to find all double quotes and replace with double backslashes\n                            new_cell = re.sub(r'\\\"', r'\\\\\\\"', cell)\n                            # Update the cell with the modified value\n                            sheet[cell.coordinate] = new_cell\n            # Save the changes back to the file\n            wb.save(file_path)\n            processed_files += 1\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n\n    return processed_files"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n    \n    t = np.linspace(0, 1, sample_size)\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves')\n    ax.legend()\n    \n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.getenv('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = bool(os.getenv('MAIL_USE_TLS', False))\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', None)\n    mail = Mail(app)\n    return (mail, app.config['MAIL'])"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the path {excel_file_path}\")\n    \n    # Read the Excel file\n    df = pd.read_excel(file_path)\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file\")\n    \n    # Calculate mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n    \n    # Return the results in a dictionary\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"standard_deviation\": std_dev\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Create a Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n    \n    # Compile the model\n    optimizer = SGD(learning_rate=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose=0, validation_data=(X_test, Y_test))\n    \n    # Plot the model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n    \n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Create a Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid')\n    ])\n    \n    # Compile the model with binary cross-entropy loss and an SGD optimizer\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=learning_rate), metrics=['accuracy'])\n    \n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n    \n    # Predict probabilities on the test set\n    Y_pred = model.predict(X_test)\n    \n    # Compute ROC curve and ROC area\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    # Return the trained model and the axes object for the plot\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"Failed to read the image file {image_path}.\")\n    \n    # Reshape the image to a 2D array of pixels\n    pixels = image.reshape((-1, 3))\n    \n    # Initialize KMeans\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    \n    # Fit the model and predict the cluster labels\n    labels = kmeans.fit_predict(pixels)\n    \n    # Replace each pixel with its centroid\n    segmented_pixels = kmeans.cluster_centers_[labels]\n    \n    # Reshape back to the original image shape\n    segmented_image = segmented_pixels.reshape(image.shape)\n    \n    return image, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', flatten the result,\n    apply KMeans clustering to the flattened data, and visualize it.\n\n    Parameters:\n    - P (np.ndarray): A matrix of shape (m, n).\n    - T (np.ndarray): A 3D tensor of shape (m, n, k).\n    - n_clusters (int): The number of clusters for KMeans.\n    - random_state (int): Random state for KMeans.\n    - n_init (int): Number of times the KMeans algorithm will be run with different centroid seeds.\n\n    Returns:\n    - cluster_result (np.ndarray): The result of KMeans clustering.\n    - ax (plt.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n    \n    # Flatten the result\n    flattened = product.flatten()\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n    \n    # Visualize the clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n    \n    Parameters:\n    - points: A list of points, where each point is a tuple (x, y).\n    - seed: An integer seed for the random number generator.\n    \n    Returns:\n    - A tuple (vor, ax): A tuple containing:\n        - vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n        - ax (Axes): The axes of the plotted Voronoi diagram.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Convert points to a numpy array\n    points = np.array(points)\n    \n    # Check if points is a 2D array\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Points must be a 2D array with shape (n, 2)\")\n    \n    # Create Voronoi diagram\n    vor = Voronoi(points)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    # Apply jittering to the points\n    jittered_points = points + np.random.normal(0, 0.1, points.shape)\n    ax.scatter(jittered_points[:, 0], jittered_points[:, 1], color='red', label='Jittered Points')\n    \n    # Set title and labels\n    ax.set_title('Voronoi Diagram with Jittered Points')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n    \n    moved_files = []\n    for file in glob.glob(os.path.join(src_dir, f'*{ext}')):\n        file_name = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, file_name)\n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_file)\n            moved_files.append(file)\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    \n    if not data:\n        return pd.DataFrame()\n    \n    def normalize_value(value):\n        if isinstance(value, list):\n            return [normalize_value(item) for item in value]\n        elif isinstance(value, str):\n            try:\n                return float(value)\n            except ValueError:\n                return value\n        elif isinstance(value, (int, float)):\n            return value * 2\n        else:\n            return value\n    \n    normalized_data = {key: normalize_value(value) for key, value in data.items()}\n    \n    df = pd.DataFrame(normalized_data)\n    return df"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script path {script_path} does not exist.\")\n    \n    try:\n        # Start the subprocess\n        process = subprocess.Popen(script_path, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        \n        # Initialize psutil process object\n        p = psutil.Process(process.pid)\n        \n        # Initialize variables to track CPU and memory usage\n        total_cpu_usage = 0\n        total_memory_usage = 0\n        start_time = time.time()\n        \n        while process.poll() is None:\n            # Check if the process has become a zombie\n            if p.status() == psutil.STATUS_ZOMBIE:\n                raise RuntimeError(\"Process has become a zombie.\")\n            \n            # Check if the process is not found\n            if not p.is_running():\n                raise RuntimeError(\"Process not found.\")\n            \n            # Get current CPU and memory usage\n            current_cpu_usage = p.cpu_percent(interval=1)\n            current_memory_usage = p.memory_info().rss  # Resident Set Size\n            \n            # Accumulate usage\n            total_cpu_usage += current_cpu_usage\n            total_memory_usage += current_memory_usage\n            \n            # Check for timeout\n            if time.time() - start_time > timeout:\n                process.terminate()\n                raise TimeoutError(\"Script execution timed out.\")\n        \n        # Wait for the process to finish\n        process.wait()\n        \n        # Return the accumulated usage\n        return {\n            'CPU Usage': total_cpu_usage,\n            'Memory Usage': total_memory_usage\n        }\n    \n    except Exception as e:\n        # Terminate the process if it's still running\n        if process.poll() is None:\n            process.terminate()\n        raise e"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    if N < len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n    else:\n        categories = np.random.choice(CATEGORIES, size=N, replace=True)\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    df = pd.DataFrame({\"x\": x, \"y\": y, \"category\": categories})\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(x, y, c=categories, cmap='viridis')\n    return df, ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time to datetime objects\n    start_time = datetime.fromisoformat(start_time)\n    end_time = datetime.fromisoformat(end_time)\n    \n    # Calculate the number of steps\n    delta = end_time - start_time\n    num_steps = int(delta.total_seconds() / step) + 1\n    \n    # Generate time series\n    times = [start_time + timedelta(seconds=i*step) for i in range(num_steps)]\n    values = np.random.normal(0, 1, num_steps) + np.arange(num_steps) * trend\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': times, 'Value': values})\n    \n    # Plot the time series\n    ax = df.plot(x='Time', y='Value', figsize=(10,5))\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    \n    return ax\n\n# Test cases"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(epoch_milliseconds, random_seed=0, products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"]):\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n    if not isinstance(random_seed, int) or random_seed < 0:\n        raise ValueError(\"random_seed must be a non-negative integer\")\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise ValueError(\"products must be a list of strings\")\n    \n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Convert epoch milliseconds to datetime\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    \n    # Calculate the number of days between start_time and current_time\n    delta = current_time - start_time\n    num_days = delta.days + 1  # inclusive of both start and end dates\n    \n    # Initialize an empty list to store sales data\n    sales_data = []\n    \n    # Generate sales data for each day\n    for day in range(num_days):\n        date = start_time + timedelta(days=day)\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({'Product': product, 'Date': date, 'Sales': sales})\n    \n    # Create a DataFrame from the sales data\n    df = pd.DataFrame(sales_data)\n    \n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import json\nimport pandas as pd\nimport os\n\ndef json_to_excel(json_str, filename, sheet_name=\"sheet1\"):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string\")\n    except TypeError:\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")\n\n    if not isinstance(data, list):\n        raise ValueError(\"JSON string must represent a list\")\n\n    if not data:\n        # If the list is empty, create an empty DataFrame\n        df = pd.DataFrame()\n    else:\n        # Assuming all elements in the list are dictionaries with the same keys\n        df = pd.DataFrame(data)\n\n    # Write the DataFrame to an Excel file\n    df.to_excel(filename, sheet_name=sheet_name, index=False)\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Create a list to hold the data\n    data = []\n    \n    # Calculate the start date\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    \n    # Generate data for each day\n    current_date = start_date\n    while current_date <= end_date:\n        # Generate a random duration for each activity\n        durations = {activity: random.randint(0, 120) for activity in activities}\n        \n        # Append the data for this day\n        data.append({\n            \"Date\": current_date.strftime(\"%Y-%m-%d\"),\n            **durations\n        })\n        \n        # Move to the next day\n        current_date += timedelta(days=1)\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n    \n    # Melt the DataFrame to have 'Activity' and 'Duration' columns\n    df_melted = df.melt(var_name='Activity', value_name='Duration')\n    \n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(x='Date', y='Duration', hue='Activity', data=df_melted)\n    \n    # Set the title and labels\n    ax.set_title('Daily Activity Durations')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Duration (minutes)')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the axes and the DataFrame\n    return ax, df\n\n# Example usage:\n# ax, df = task_func(days_in_past=7, random_seed=0)"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Calculate the start date\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    # Create a DataFrame with dates as the index\n    df = pd.DataFrame(index=date_range)\n    \n    # Add stock columns with random prices\n    for stock in stock_names:\n        df[stock] = np.random.rand(days_in_past)\n    \n    return df"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r', newline='', encoding='utf-8') as f1, open(file_path2, 'r', newline='', encoding='utf-8') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            lines1 = list(reader1)\n            lines2 = list(reader2)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both files not found.\")\n    except csv.Error as e:\n        raise ValueError(f\"Error reading CSV files: {e}\")\n    \n    if not lines1 or not lines2:\n        raise ValueError(\"One or both files are empty.\")\n    \n    differences = []\n    max_lines = max(len(lines1), len(lines2))\n    for i in range(max_lines):\n        line1 = lines1[i] if i < len(lines1) else []\n        line2 = lines2[i] if i < len(lines2) else []\n        \n        if line1 == line2:\n            status = ' '\n            content = ' '.join(line1)\n        else:\n            if line1:\n                status = '-'\n                content = ' '.join(line1)\n            else:\n                status = '+'\n                content = ' '.join(line2)\n        \n        differences.append({\n            'Line Number': i + 1,\n            'Status': status,\n            'Content': content\n        })\n    \n    df = pd.DataFrame(differences)\n    return df"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, plt.gca())\n    \n    df = pd.DataFrame(data, columns=[column])\n    stats = df[column].agg(['sum', 'mean', 'min', 'max']).to_dict()\n    \n    fig, ax = plt.subplots()\n    ax.pie(df[column], labels=df[column], autopct='%1.1f%%')\n    ax.axis('equal')\n    \n    return (stats, ax)"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    df = pd.DataFrame(data)\n    \n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n    \n    if not df[column].dtype in [np.int64, np.float64]:\n        raise ValueError(f\"The column '{column}' is not numeric.\")\n    \n    if df[column].min() < 0:\n        raise ValueError(f\"The column '{column}' contains negative values.\")\n    \n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    plt.plot(df['Date'], df[column])\n    plt.title(f'Line Chart of {column}')\n    plt.xlabel('Date')\n    plt.ylabel(column)\n    \n    return (stats, plt.gca())"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize dictionaries to hold the sums and counts for each key\n    sums = defaultdict(float)\n    counts = defaultdict(int)\n    \n    # Initialize a list to hold the keys\n    keys = []\n    \n    # Iterate over each dictionary in the list\n    for item in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in item.items():\n            # Check if the value is numeric and not None\n            if isinstance(value, (int, float)) and value is not None:\n                # Add the value to the sum for this key\n                sums[key] += value\n                # Increment the count for this key\n                counts[key] += 1\n                # Add the key to the list if it's not already there\n                if key not in keys:\n                    keys.append(key)\n    \n    # Initialize lists to hold the means and medians\n    means = []\n    medians = []\n    \n    # Iterate over each key\n    for key in keys:\n        # Calculate the mean for this key\n        if counts[key] > 0:\n            mean = sums[key] / counts[key]\n        else:\n            mean = None\n        means.append(mean)\n        \n        # Collect all numeric values for this key\n        values = [item[key] for item in data if key in item and isinstance(item[key], (int, float)) and item[key] is not None]\n        \n        # Calculate the median for this key\n        if values:\n            median = np.median(values)\n        else:\n            median = None\n        medians.append(median)\n    \n    # Create a DataFrame from the means and medians\n    df = pd.DataFrame({\n        'mean': means,\n        'median': medians\n    }, index=keys)\n    \n    # Sort the DataFrame by the index\n    df = df.sort_index()\n    \n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be a CSV file with .csv extension.\")\n    \n    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n        reader = csv.DictReader(file)\n        rows = list(reader)\n    \n    # Convert list of dictionaries to a DataFrame\n    df = pd.DataFrame(rows)\n    \n    # Identify duplicate rows\n    duplicate_rows = df.duplicated(keep=False)\n    \n    # Count occurrences of each duplicate row\n    duplicate_counts = Counter(tuple(row) for row in df[duplicate_rows].to_dict(orient='records'))\n    \n    # Convert duplicates to a DataFrame\n    duplicates_df = pd.DataFrame(list(duplicate_counts.items()), columns=['Duplicate Row', 'Count'])\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.bar(duplicates_df['Duplicate Row'], duplicates_df['Count'], color='skyblue')\n    plt.xlabel('Duplicate Row')\n    plt.ylabel('Count')\n    plt.title('Duplicate Rows in CSV File')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Return the dictionary of duplicates and the Axes object\n    return dict(duplicate_counts), plt.gca()\n\n# Test the function"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Ensure 'age' is not negative\n    if df['age'].min() < 0:\n        raise ValueError(\"Age cannot be negative\")\n    \n    # Round down ages to nearest integer\n    df['age'] = df['age'].apply(np.floor).astype(int)\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    \n    if duplicates.empty:\n        return Counter(), None\n    \n    # Record age distribution for duplicates\n    age_distribution = duplicates['age'].value_counts().sort_index()\n    \n    # Create histogram plot\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)  # Adjust bins to ensure integer ages fall within bins\n    plt.figure()\n    sns.histplot(duplicates['age'], bins=bins, kde=False)\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution of Duplicates')\n    plt.show()\n    \n    return age_distribution, plt.gca()"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Extract the 'value' column from the DataFrame\n    values = df['value']\n    \n    # Count the frequency of each value\n    value_counts = Counter(values)\n    \n    # Identify duplicates (values that appear more than once)\n    duplicates = {value: count for value, count in value_counts.items() if count > 1}\n    \n    # Create a histogram of the 'value' column\n    plt.figure()\n    n, bins, patches = plt.hist(values, bins=bins, color='green', alpha=0.6, label='Data')\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(values)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n    \n    # Set plot properties\n    plt.title('Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n    \n    # Return the Counter of duplicates and the plot axes\n    return (Counter(duplicates), plt.gca())"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Generate random values for the DataFrame\n    data = np.random.rand(len(a), len(b))\n    \n    # Create the DataFrame with the given row indices and column names\n    df = pd.DataFrame(data, index=a, columns=b)\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Convert the 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Extract year and month from the 'date' column\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    \n    # Filter the data for the specified year\n    year = df['year'].unique()[0]  # Assuming all data is from the same year\n    monthly_data = df[df['year'] == year]\n    \n    # Group by month and sum the 'value' column\n    monthly_sum = monthly_data.groupby('month')['value'].sum().reset_index()\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(monthly_sum['month'], monthly_sum['value'], color='skyblue')\n    plt.title(f'Monthly Data for {year}')\n    plt.xlabel('Month')\n    plt.ylabel('Value')\n    plt.xticks(monthly_sum['month'], [datetime(2000, m, 1).strftime('%B') for m in monthly_sum['month']])\n    plt.grid(axis='y')\n    \n    # Get the current axes\n    ax = plt.gca()\n    \n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input string to a list of floats\n    data_list = list(map(float, data.split()))\n    \n    # Create a pandas DataFrame from the list\n    df = pd.DataFrame(data_list, columns=['Value'])\n    \n    # Calculate the frequency distribution\n    frequency = df['Value'].value_counts().sort_index()\n    \n    # Create a histogram\n    ax = frequency.plot(kind='bar', title='Histogram of Values', figsize=(10,6))\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 10, array_length)\n    \n    # Generate y values as a sine wave with noise\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n    \n    # Define the sine function to fit\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x + c)\n    \n    # Use curve_fit to fit the sine function to the data\n    params, _ = curve_fit(sine_func, x, y)\n    \n    # Generate y values for the fitted curve\n    y_fit = sine_func(x, *params)\n    \n    # Plot the noisy sine wave and the adjusted curve\n    plt.figure()\n    plt.plot(x, y, 'b-', label='Noisy Sine Wave')\n    plt.plot(x, y_fit, 'r-', label='Fitted Curve')\n    plt.legend()\n    plt.show()\n\n# Test the function\ntask_func()"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text = ' '.join([row[0] for row in reader])\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"An error occurred while reading the file {csv_file}.\")\n\n    normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n    words = normalized_text.split()\n    word_counts = Counter(words)\n    most_common = word_counts.most_common(10)\n\n    fig, ax = plt.subplots()\n    words, counts = zip(*most_common)\n    ax.bar(words, counts)\n    ax.set_title('Top 10 Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    return (ax, most_common)"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n    \n    # Plot histogram\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    \n    # Plot PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, loc=0, scale=1)\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    \n    # Add labels and legend\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n    \n    # Generate a random password for AES encryption\n    password = get_random_bytes(16)  # AES key size is 16, 24, or 32 bytes\n    \n    # Generate a random nonce for AES encryption\n    nonce = get_random_bytes(16)  # AES nonce size is 16 bytes\n    \n    # Encrypt the private key with AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, _ = cipher.encrypt_and_digest(privkey.save_pkcs1().encode())\n    \n    # Encode the ciphertext and nonce for storage\n    encrypted_privkey = b64encode(ciphertext).decode('utf-8')\n    encoded_nonce = b64encode(nonce).decode('utf-8')\n    \n    # Generate a filename based on 8 random bytes\n    random_bytes = os.urandom(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n    \n    # Save the encrypted private key to a file\n    with open(filename, 'w') as f:\n        f.write(encrypted_privkey)\n    \n    # Return the public key, filename, password, and nonce\n    return pubkey, filename, password, nonce\n\n# Test the function"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n    \n    # Read the file to be encrypted\n    with open(file_path, 'rb') as f:\n        data = f.read()\n    \n    # Generate a random AES key\n    aes_key = os.urandom(32)  # 256 bits\n    \n    # Encrypt the file using AES\n    iv = os.urandom(16)  # 128 bits\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n    \n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(iv + encrypted_data)\n    \n    # Save the encrypted AES key\n    encrypted_key_path = file_path + '.key.enc'\n    with open(encrypted_key_path, 'wb') as f:\n        f.write(encrypted_aes_key)\n    \n    return pubkey, encrypted_file_path, encrypted_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL cannot be empty\")\n    try:\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n    except urllib.error.URLError as e:\n        raise e\n    doc = pq(html)\n    anchors = doc('a')\n    data = []\n    for anchor in anchors.items():\n        text = anchor.text()\n        href = anchor.attr('href')\n        data.append({'text': text, 'href': href})\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        current_time = datetime.now()\n        for _ in range(hours * 60):\n            time = current_time + timedelta(minutes=_)\n            temperature = randint(20, 30)\n            humidity = randint(30, 60)\n            pressure = randint(1000, 1020)\n            writer.writerow([time.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for hour in range(hours):\n        row = {'Time': current_time + timedelta(hours=hour)}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(10, 100)\n        data.append(row)\n    \n    # Save to CSV\n    csv_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time'] + VEHICLE_TYPES)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Plot data\n    df = pd.DataFrame(data)\n    df['Time'] = pd.to_datetime(df['Time'])\n    df.set_index('Time', inplace=True)\n    \n    ax = df.plot(kind='line')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    \n    return csv_path, ax\n\n# Test the function"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Create backup directory if it doesn't exist\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n    \n    # Generate weather data\n    current_time = datetime.now()\n    data = []\n    for _ in range(hours):\n        time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n        condition = choice(WEATHER_CONDITIONS)\n        data.append({'Time': time, 'Condition': condition})\n        current_time += timedelta(hours=1)\n    \n    # Save data to CSV\n    csv_file_path = os.path.join(output_dir, f'weather_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv')\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Condition'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Backup the file\n    backup_file_path = os.path.join(BACKUP_DIR, os.path.basename(csv_file_path))\n    shutil.copy2(csv_file_path, backup_file_path)\n    \n    return csv_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Generate match results\n    match_results = []\n    for _ in range(goals):\n        team1 = TEAMS[randint(0, len(TEAMS)-1)]\n        team2 = TEAMS[randint(0, len(TEAMS)-1)]\n        while team1 == team2:\n            team2 = TEAMS[randint(0, len(TEAMS)-1)]\n        goal_team1 = randint(0, 5)\n        goal_team2 = randint(0, 5)\n        penalty_team1 = randint(0, penalties)\n        penalty_team2 = randint(0, penalties)\n        match_results.append({\n            'Team 1': team1,\n            'Team 2': team2,\n            'Goals Team 1': goal_team1,\n            'Goals Team 2': goal_team2,\n            'Penalties Team 1': penalty_team1,\n            'Penalties Team 2': penalty_team2\n        })\n    \n    # Convert penalties to fines\n    for match in match_results:\n        match['Fines Team 1'] = match['Penalties Team 1'] * PENALTY_COST\n        match['Fines Team 2'] = match['Penalties Team 2'] * PENALTY_COST\n    \n    # Create DataFrame\n    df = pd.DataFrame(match_results)\n    \n    # Visualize goals and penalty costs\n    goals_plot = sns.barplot(x='Team', y='Goals', data=df)\n    penalty_plot = sns.barplot(x='Team', y='Penalties', data=df)\n    \n    return df, [goals_plot, penalty_plot]\n\n# Test the function\ndf, plots = task_func(10, 3)"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count the non-zero values in each column\n    non_zero_counts = df.replace(0, np.nan).count()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(COLUMNS, non_zero_counts)\n    ax.set_title('Non-zero values per column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of non-zero values')\n    \n    return df, ax\n\n# Test the function\ndf, ax = task_func(10)\nplt.show()"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define the courses\n    courses = ['Math', 'Science', 'History', 'English']\n    \n    # Generate a list of student names\n    students = [f'Student {i+1}' for i in range(num_students)]\n    \n    # Generate random grades for each student in each course\n    grades = {course: [np.random.randint(0, 101) for _ in range(num_students)] for course in courses}\n    \n    # Create a DataFrame\n    df = pd.DataFrame(grades, index=students)\n    \n    # Calculate the average grade in each course\n    avg_grades = df.mean()\n    \n    # Calculate the number of students with a passing grade (>=60) in each course\n    passing_counts = (df >= 60).sum()\n    \n    # Combine the average grades and passing counts into a single DataFrame\n    summary_df = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Students': passing_counts\n    })\n    \n    # Create a bar plot for the summary DataFrame\n    ax = summary_df.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Course')\n    ax.set_ylabel('Value')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the DataFrame and the Axes object\n    return df, ax\n\n# Example usage:\n# df, ax = task_func(10)"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Filter the array to get indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n    \n    # Extract the second column for fitting\n    y = array[indices, 1]\n    \n    # Generate x values based on the indices\n    x = np.arange(len(y))\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Perform the curve fitting\n    popt, pcov = optimize.curve_fit(exp_decay, x, y)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'o', label='data')\n    ax.plot(x, exp_decay(x, *popt), 'r-', label='fit')\n    ax.legend()\n    \n    return popt, ax\n\n# Test the function\ndata = np.array([\n    [1, 10],\n    [2, 8],\n    [3, 6],\n    [4, 4],\n    [5, 2],\n    [1, 12],\n    [2, 10],\n    [3, 8],\n    [4, 6],\n    [5, 4]\n])\n\npopt, ax = task_func(data, 1)\nplt.show()"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    def preprocess(text):\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n    processed_texts = [preprocess(text) for text in texts]\n    \n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=1000)\n    X = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    W = nmf.fit_transform(X)\n    H = nmf.components_\n    \n    # Get the top words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(H):\n        print(f\"Topic {topic_idx + 1}:\")\n        top_features = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(top_features)\n        print(\" \".join(top_features))\n    \n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    # Download necessary NLTK data\n    nltk.download('stopwords')\n    \n    # Load stop words\n    if stopwords is None:\n        stopwords = set(stopwords.words('english'))\n    \n    # Preprocess the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase\n        cleaned_text = cleaned_text.lower()\n        # Tokenize\n        words = cleaned_text.split()\n        # Remove stop words\n        words = [word for word in words if word not in stopwords]\n        # Join back into a string\n        cleaned_text = ' '.join(words)\n        cleaned_texts.append(cleaned_text)\n    \n    # Tokenize the cleaned texts\n    tokenized_texts = [text.split() for text in cleaned_texts]\n    \n    # Train Word2Vec model\n    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Create a list to hold all the dataframes\n    all_dfs = []\n    \n    # List all files in the directory\n    files = os.listdir(path)\n    \n    # Filter only JSON files\n    json_files = [f for f in files if f.endswith('.json')]\n    \n    # Sort the JSON files alphabetically\n    json_files.sort()\n    \n    # Process each JSON file\n    for file in json_files:\n        # Read the JSON file\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n        \n        # Convert the data to a DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add a 'Source' column with the filename\n        df['Source'] = file\n        \n        # Append the DataFrame to the list\n        all_dfs.append(df)\n    \n    # Concatenate all DataFrames into one\n    df = pd.concat(all_dfs, ignore_index=True)\n    \n    # Create a 'processed' directory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n    \n    # Move each JSON file to the 'processed' directory\n    for file in json_files:\n        shutil.move(os.path.join(path, file), processed_path)\n    \n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the file path\n    file_path = \"task_func_data/Output.txt\"\n    \n    # Create the directory if it doesn't exist\n    dir_path = os.path.dirname(file_path)\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    \n    # Generate random temperature and humidity data\n    temperature = random.uniform(20, 30)\n    humidity = random.uniform(30, 70)\n    \n    # Get current timestamp\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Write data to the CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        # Write header if the file is empty\n        if os.path.getsize(file_path) == 0:\n            writer.writerow([\"Timestamp\", \"Temperature\", \"Humidity\"])\n        # Write data row\n        writer.writerow([timestamp, temperature, humidity])\n    \n    # Return the path to the CSV file before deletion\n    return file_path\n\n# Test the function"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    # Send a request to the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n    except urllib.error.URLError as e:\n        print(f\"Error opening URL: {e}\")\n        return None\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find all the paragraphs in the HTML\n    paragraphs = soup.find_all('p')\n\n    # Open a CSV file to write the scraped data\n    with open(CSV_FILE_PATH, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Paragraphs'])  # Write header\n\n        # Write each paragraph to the CSV file\n        for para in paragraphs:\n            writer.writerow([para.get_text()])\n\n    return CSV_FILE_PATH\n\n# Test the function"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pandas DataFrame\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"data is empty\")\n    \n    # Check if target_column is in data\n    if target_column not in data.columns:\n        raise ValueError(f\"target_column '{target_column}' not found in data\")\n    \n    # Check if data contains only numeric values\n    if not data.select_dtypes(include=[np.number]).columns.equals(data.columns):\n        raise ValueError(\"data contains non-numeric values\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state must be an integer\")\n    \n    # Check if test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"test_size must be between 0 and 1\")\n    \n    # Split the data into features and target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model's score on the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nfrom datetime import datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n              rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Generate IDs\n    ids = np.arange(1, 101)\n    \n    # Generate Names\n    names = []\n    for _ in range(100):\n        if np.random.rand() < 0.7:  # 70% chance of being a Latin name\n            name = np.random.choice(latin_names)\n        else:\n            name = np.random.choice(other_names)\n        # Correct improper encoding\n        name = name.encode('utf-8', 'replace').decode('utf-8')\n        names.append(name)\n    \n    # Generate Date of Birth\n    dates = []\n    for _ in range(100):\n        year = np.random.randint(start_year, end_year + 1)\n        month = np.random.randint(1, 13)\n        day = np.random.randint(1, 29)  # Assuming all months have 28 days for simplicity\n        date = datetime(year, month, day)\n        dates.append(date.strftime('%y-%m-%d %H:%M:%S'))\n    \n    # Generate Email\n    emails = []\n    for name, date in zip(names, dates):\n        # Extract year from date\n        year = date.split('-')[0]\n        # Construct email\n        email = re.sub(r'[^a-zA-Z0-9]', '', name.lower()) + year + '@' + email_domain\n        emails.append(email)\n    \n    # Create DataFrame\n    data = {\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates,\n        'Email': emails\n    }\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize a dictionary to hold the results\n    results = defaultdict(lambda: {'mean': [], 'median': []})\n    \n    # Iterate over each dictionary in the list\n    for item in data:\n        for key, value in item.items():\n            # Append the value to the corresponding list in the results dictionary\n            results[key]['mean'].append(value)\n            results[key]['median'].append(value)\n    \n    # Calculate mean and median for each key\n    for key in results:\n        mean_values = results[key]['mean']\n        median_values = results[key]['median']\n        results[key]['mean'] = np.mean(mean_values)\n        results[key]['median'] = np.median(median_values)\n    \n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header\n        writer.writerow(['key', 'mean', 'median'])\n        # Write the data\n        for key, values in results.items():\n            writer.writerow([key, values['mean'], values['median']])"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    for file_path in kwargs.get('files', []):\n        if os.path.exists(file_path):\n            if os.path.getsize(file_path) > 0:\n                target_path = os.path.join(target_dir, os.path.basename(file_path))\n                shutil.copy(file_path, target_path)\n                copied_files.append(target_path)\n    return {'copied_files': copied_files}"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    dir_path = Path(directory)\n    if not dir_path.is_dir():\n        raise NotADirectoryError(f\"The provided directory {directory} does not exist.\")\n    \n    # Compile the regex pattern for efficiency\n    regex = re.compile(pattern)\n    \n    for file in dir_path.iterdir():\n        if file.is_file() and file.suffix == '.csv':\n            match = regex.match(file.name)\n            if match:\n                # Extract the base name without the number and extension\n                base_name = match.group(1)\n                # Create a new filename with the base name and .csv extension\n                new_filename = f\"{base_name}.csv\"\n                # Create the new file path\n                new_file_path = dir_path / new_filename\n                # Copy the content from the original file to the new file\n                with open(file, 'r', newline='') as infile, open(new_file_path, 'w', newline='') as outfile:\n                    reader = csv.reader(infile)\n                    writer = csv.writer(outfile)\n                    for row in reader:\n                        writer.writerow(row)\n                new_files.append(new_filename)\n    \n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    # List to store the directories where files were extracted\n    extracted_dirs = []\n    \n    # Compile the regex pattern for efficiency\n    regex = re.compile(pattern)\n    \n    # Walk through all files in the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file matches the pattern\n            match = regex.match(file)\n            if match:\n                # Extract the prefix part of the filename\n                prefix = match.group(1)\n                # Construct the path for the extraction directory\n                extract_dir = os.path.join(root, prefix)\n                # Create the directory if it doesn't exist\n                if not os.path.exists(extract_dir):\n                    os.makedirs(extract_dir)\n                # Construct the full path of the zip file\n                zip_path = os.path.join(root, file)\n                # Extract the zip file to the extraction directory\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(extract_dir)\n                # Add the extraction directory to the list\n                extracted_dirs.append(extract_dir)\n    \n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport tempfile\nimport shutil\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n    \n    # Find files matching the pattern\n    files = glob.glob(pattern)\n    \n    if not files:\n        print(f\"No files found matching the pattern: {pattern}\")\n        return None\n    \n    # Create a temporary directory to hold the files to be archived\n    with tempfile.TemporaryDirectory() as temp_dir:\n        # Move the files to the temporary directory\n        for file in files:\n            shutil.move(file, temp_dir)\n        \n        # Create an archive file\n        archive_name = os.path.join(ARCHIVE_DIR, f\"archive_{os.path.basename(pattern)}.tar.gz\")\n        subprocess.run(['tar', '-czf', archive_name, '-C', temp_dir, '.'])\n        \n        # Remove the original files\n        for file in files:\n            os.remove(file)\n    \n    return archive_name"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize a Counter to hold the total counts of goals and penalties\n    count = Counter()\n    \n    # Read the existing CSV file\n    with open(csv_file_path, mode='r', newline='') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            # Convert goals and penalties to integers and add to the counter\n            count['goals'] += int(row['goals'])\n            count['penalties'] += int(row['penalties'])\n    \n    # Add the new goals and penalties to the counter\n    count['goals'] += goals\n    count['penalties'] += penalties\n    \n    return count\n\n# Test the function"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    moved_files_count = 0\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            source_file = os.path.join(source_dir, filename)\n            target_file = os.path.join(target_dir, filename)\n            shutil.move(source_file, target_file)\n            moved_files_count += 1\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Clean the texts\n    text1_clean = ALPHANUMERIC.sub(' ', text1).lower()\n    text2_clean = ALPHANUMERIC.sub(' ', text2).lower()\n    \n    # Tokenize the texts\n    words1 = text1_clean.split()\n    words2 = text2_clean.split()\n    \n    # Compute term frequencies\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n    \n    # Get the unique terms\n    unique_terms = set(tf1.keys()) | set(tf2.keys())\n    \n    # Create vectors for each text\n    vector1 = [tf1.get(term, 0) for term in unique_terms]\n    vector2 = [tf2.get(term, 0) for term in unique_terms]\n    \n    # Compute cosine similarity\n    dot_product = sum(a*b for a, b in zip(vector1, vector2))\n    magnitude1 = np.sqrt(sum(a*a for a in vector1))\n    magnitude2 = np.sqrt(sum(b*b for b in vector2))\n    if magnitude1 == 0 or magnitude2 == 0:\n        cosine_similarity = 0\n    else:\n        cosine_similarity = dot_product / (magnitude1 * magnitude2)\n    \n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(text1_clean, text2_clean)\n    \n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of integers.\")\n    if not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"All elements in the list must be integers.\")\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"All numbers must be non-negative.\")\n    \n    if not numbers:\n        return [], []\n    \n    # Calculate factorial for each number\n    factorials = [math.factorial(num) for num in numbers]\n    \n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n    \n    # Calculate the sum of factorials for each permutation\n    sum_fac_perms = [sum(factorials) for _ in perms]\n    \n    return sum_fac_perms, perms"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n    for extension in EXTENSIONS:\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*{extension}'))\n        for file in files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item for item, _, _ in data]\n    counts = [count for _, count, _ in data]\n    weights = [weight for _, _, weight in data]\n    \n    # Calculate z-scores for counts\n    normalized_counts = zscore(counts)\n    \n    # Scale weights using Min-Max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(np.array(weights).reshape(-1,1)).flatten()\n    \n    # Create a DataFrame with the results\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize a list to hold the mean values for each position\n    means = []\n    \n    # Determine the number of positions based on the first tuple\n    num_positions = len(data_list[0]) if data_list else 0\n    \n    # Iterate over each position\n    for i in range(num_positions):\n        # Extract the values at the current position from all tuples\n        values = [item[i] for item in data_list if len(item) > i and isinstance(item[i], (int, float))]\n        \n        # Calculate the mean of the extracted values\n        if values:\n            mean_val = np.mean(values)\n        else:\n            mean_val = np.nan\n        \n        # Append the mean value to the list\n        means.append(mean_val)\n    \n    # Create a DataFrame with the mean values\n    df = pd.DataFrame({'Mean Value': means}, index=[f'Position {i}' for i in range(num_positions)])\n    \n    return df\n\n# Test the function with the provided data\ndata = [('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Column(s) {col1} and/or {col2} not found in the DataFrame.\")\n    \n    # Check if both columns are categorical\n    if not data[col1].dtype.name.startswith('category') or not data[col2].dtype.name.startswith('category'):\n        raise TypeError(\"Both columns must be categorical.\")\n    \n    # Check if both columns have multiple categories\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"Both columns must have multiple categories.\")\n    \n    # Check if any category has less than 5 observations\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    if (contingency_table < 5).sum().sum() > 0:\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Perform chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    results = [random.randint(1, 6) for _ in range(rolls)]\n    frequency = np.bincount(results, minlength=7)[1:]\n    plt.hist(results, bins=6, range=(0.5, 6.5), align='mid', rwidth=0.8)\n    plt.title('Histogram of Dice Rolls')\n    plt.xlabel('Dice Value')\n    plt.ylabel('Frequency')\n    plt.xticks(range(1, 7))\n    plt.yticks(range(0, max(frequency)+1, 1))\n    plt.show()\n    return (frequency, plt.gca())\n\n# Test cases\ndef test_task_func():\n    # Test case 1: 10 rolls\n    freq, ax = task_func(10)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 2: 100 rolls\n    freq, ax = task_func(100)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 3: 1000 rolls\n    freq, ax = task_func(1000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 4: 10000 rolls\n    freq, ax = task_func(10000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 5: 100000 rolls\n    freq, ax = task_func(100000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 6: 1000000 rolls\n    freq, ax = task_func(1000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 7: 10000000 rolls\n    freq, ax = task_func(10000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 8: 100000000 rolls\n    freq, ax = task_func(100000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 9: 1000000000 rolls\n    freq, ax = task_func(1000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 10: 10000000000 rolls\n    freq, ax = task_func(10000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 11: 100000000000 rolls\n    freq, ax = task_func(100000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 12: 1000000000000 rolls\n    freq, ax = task_func(1000000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 13: 10000000000000 rolls\n    freq, ax = task_func(10000000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 14: 100000000000000 rolls\n    freq, ax = task_func(100000000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 15: 1000000000000000 rolls\n    freq, ax = task_func(1000000000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 16: 10000000000000000 rolls\n    freq, ax = task_func(10000000000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 17: 100000000000000000 rolls\n    freq, ax = task_func(100000000000000000)\n    assert len(freq) == 6\n    assert all(isinstance(x, int) for x in freq)\n    # Test case 18: 1000000000000000000 rolls\n    freq, ax = task_func(1"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Create a zip file in the target directory\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the source directory\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                # Check if the file has '_processed' in its name\n                if '_processed' in file:\n                    # Construct the full path of the file\n                    file_path = os.path.join(root, file)\n                    # Add the file to the zip archive\n                    zipf.write(file_path, os.path.relpath(file_path, source_dir))\n    \n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Sort the dataframe by date\n    df = df.sort_values(by='Date')\n    \n    # Create a new column 'Day' which is the day number since the first date\n    df['Day'] = (df['Date'] - df['Date'].min()).dt.days\n    \n    # Prepare the data for the linear regression model\n    X = df[['Day']]\n    y = df['Close']\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Predict the next 7 days\n    last_day = df['Day'].max()\n    future_days = np.array(range(last_day + 1, last_day + 8)).reshape(-1, 1)\n    future_prices = model.predict(future_days)\n    \n    # Plot the data\n    plt.figure(figsize=(10,5))\n    plt.plot(df['Date'], df['Close'], label='Historical Prices')\n    plt.plot(df['Date'].max() + pd.to_timedelta(np.arange(1,8), unit='d'), future_prices, label='Predicted Prices', color='red')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.title('Stock Prices')\n    plt.legend()\n    plt.show()\n    \n    # Return the predicted prices and the plot axes\n    return (future_prices.tolist(), plt.gca())\n\n# Test the function\n# Assuming df is a pandas DataFrame with 'Date' and 'Close' columns\n# df = pd.read_csv('stock_data.csv')\n# task_func(df)"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    # Calculate Z-Scores\n    z_scores = np.abs(zscore(df['closing_price']))\n    \n    # Identify outliers\n    outliers = df[z_scores > z_threshold]\n    \n    # Plot outliers\n    plt.figure(figsize=(10, 6))\n    plt.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers')\n    plt.plot(df.index, df['closing_price'], color='blue', label='Data')\n    plt.xlabel('Index')\n    plt.ylabel('Closing Price')\n    plt.title('Outliers in Closing Prices')\n    plt.legend()\n    plt.grid(True)\n    plot = plt.gca()\n    \n    return outliers, plot\n\n# Test the function\ndf1 = pd.DataFrame({'closing_price': [10, 20, 30, 40, 50, 100]})\noutliers1, plot1 = task_func(df1, z_threshold=1.5)"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \n    Parameters:\n    - df: pandas DataFrame containing stock data with a 'Close' column.\n    \n    Returns:\n    - A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot and the second for the histogram.\n    \"\"\"\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Box plot\n    sns.boxplot(x=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    \n    # Histogram\n    sns.histplot(df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return axes[0], axes[1]\n\n# Test the function"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Assuming the DataFrame has a column named 'Close' for closing prices\n    # and an index that is a datetime object.\n    \n    # Fit an ARIMA model to the data\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit()\n    \n    # Make predictions for the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    \n    # Plot the forecast\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['Close'], label='Historical Prices')\n    ax.plot(forecast.index, forecast, label='Forecasted Prices', color='red')\n    ax.legend()\n    \n    return forecast.tolist(), ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of lowercase English alphabets\n    alphabet = string.ascii_lowercase\n    combinations = list(itertools.product(alphabet, repeat=2))\n    combinations = [''.join(pair) for pair in combinations]\n    \n    # Count the frequency of each combination in the given word\n    word_counter = Counter(word)\n    result = {}\n    for combo in combinations:\n        count = 0\n        for i in range(len(word) - 1):\n            if word[i:i+2] == combo:\n                count += 1\n        result[combo] = count\n    \n    return result"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with 'Date' and 'Category' columns\n    df = pd.DataFrame({'Date': np.tile(dates, len(categories)),\n                       'Category': np.repeat(categories, periods),\n                       'Sales': np.random.rand(periods * len(categories)) * 1000})\n    \n    # Pivot the DataFrame to have 'Category' as columns and 'Date' as index\n    pivot_df = df.pivot(index='Date', columns='Category', values='Sales')\n    \n    # Plot the data\n    ax = pivot_df.plot(kind='line', figsize=(10, 6))\n    plt.title('Sales Report by Category Over Time')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.legend(title='Category')\n    \n    return df, ax\n\n# Test the function\ndf, ax = task_func()"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        # Generate sales data if not provided\n        dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales = np.random.randint(100, 500, size=periods)\n        sales_data = pd.DataFrame({'date': dates, 'sales': sales})\n    else:\n        sales_data = pd.DataFrame(sales_data)\n    \n    # Convert date to datetime\n    sales_data['date'] = pd.to_datetime(sales_data['date'])\n    \n    # Sort data by date\n    sales_data = sales_data.sort_values(by='date')\n    \n    # Reset index\n    sales_data = sales_data.reset_index(drop=True)\n    \n    # Create a feature column for linear regression\n    sales_data['time'] = sales_data.index\n    \n    # Fit linear regression model\n    X = sales_data[['time']]\n    y = sales_data['sales']\n    model = LinearRegression().fit(X, y)\n    \n    # Predict future sales\n    future_time = np.array(range(len(sales_data), len(sales_data) + periods)).reshape(-1, 1)\n    future_sales = model.predict(future_time)\n    \n    return future_sales"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    \n    if seed is not None:\n        random.seed(seed)\n    \n    current_date = datetime.today().date()\n    \n    assigned_tasks = []\n    for _ in range(n_tasks):\n        if not task_list:\n            break\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        task_name = task.replace(\" \", \"_\")\n        assigned_tasks.append({\n            'Task Name': task_name,\n            'Assigned To': employee,\n            'Due Date': current_date\n        })\n        task_list.remove(task)\n    \n    df = pd.DataFrame(assigned_tasks)\n    return df"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Convert text to lowercase to ensure case-insensitivity\n    text = text.lower()\n    \n    # Replace spaces with underscores\n    text = re.sub(r' ', '_', text)\n    \n    # Split the text into words\n    words = text.split('_')\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n    \n    # Extract words and their frequencies\n    words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    \n    # Set labels and title\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n    \n    # Rotate labels for better visibility\n    plt.xticks(rotation=45)\n    \n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if source_directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n    \n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create a list to hold the files to be zipped\n    files_to_zip = []\n    \n    # Walk through the source directory\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            # Check if the file has a valid extension\n            if any(file.endswith(ext) for ext in valid_extensions):\n                # Construct the full path to the file\n                file_path = os.path.join(root, file)\n                # Add the file to the list\n                files_to_zip.append(file_path)\n    \n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    \n    # Construct the full path to the zip file\n    zip_path = os.path.join(target_directory, zip_name + '.zip')\n    \n    # Create a zip file\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in files_to_zip:\n            # Add each file to the zip\n            zipf.write(file, os.path.relpath(file, source_directory))\n    \n    # Return the full path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if source directory exists\n    source_path = Path(source_directory)\n    if not source_path.exists():\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n    \n    # Create target directory if it doesn't exist\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # List of supported extensions\n    supported_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Counter for successful conversions\n    success_count = 0\n    \n    # Iterate over files in source directory\n    for file in source_path.iterdir():\n        if file.is_file():\n            file_extension = file.suffix.lower()\n            if file_extension in supported_extensions:\n                # Read file content based on extension\n                if file_extension == '.txt':\n                    with open(file, 'r', encoding='utf-8') as f:\n                        text = f.read()\n                elif file_extension == '.docx':\n                    doc = docx.Document(file)\n                    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n                elif file_extension == '.xlsx':\n                    df = pd.read_excel(file)\n                    text = df.to_csv(index=False)\n                elif file_extension == '.csv':\n                    with open(file, 'r', encoding='utf-8') as f:\n                        text = f.read()\n                \n                # Save text to CSV in target directory\n                target_file = target_path / (file.stem + '.csv')\n                with open(target_file, 'w', newline='', encoding='utf-8') as f:\n                    writer = csv.writer(f)\n                    writer.writerow(['Text'])\n                    writer.writerow([text])\n                \n                success_count += 1\n    \n    return success_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    \n    if df.isnull().any().any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    \n    scaler = MinMaxScaler()\n    for col in df.columns:\n        df[col] = scaler.fit_transform(df[[col]]).flatten()\n    \n    return df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # List all entries in the directory\n    entries = os.listdir(directory_path)\n    \n    result = []\n    for entry in entries:\n        entry_path = os.path.join(directory_path, entry)\n        # Check if it's a file\n        if os.path.isfile(entry_path):\n            # Get file size\n            file_size = os.path.getsize(entry_path)\n            # Get creation time\n            creation_time = os.path.getctime(entry_path)\n            # Get modification time\n            modification_time = os.path.getmtime(entry_path)\n            # Convert times to ISO format\n            creation_time_iso = datetime.fromtimestamp(creation_time, timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, timezone.utc).isoformat()\n            # Append to result\n            result.append((entry, file_size, creation_time_iso, modification_time_iso))\n    \n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Malformed JSON data\")\n    \n    if not data:\n        raise ValueError(\"Empty JSON data\")\n    \n    countries = []\n    populations = []\n    \n    for item in data:\n        if not isinstance(item, dict) or len(item) != 2:\n            raise ValueError(\"Invalid JSON format\")\n        \n        country = item.get(\"country\")\n        population = item.get(\"population\")\n        \n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name\")\n        \n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Invalid population value\")\n        \n        countries.append(country)\n        populations.append(math.floor(population))\n    \n    df = pd.DataFrame({\"Country\": countries, \"Population\": populations})\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    file_path = os.path.join(output_dir, file_name)\n    \n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Cannot write to the specified directory\")\n    \n    return file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if directory exists and is a directory\n    dir_path = Path(dir_path)\n    if not dir_path.is_dir():\n        raise FileNotFoundError(f\"The specified directory does not exist or is not a directory: {dir_path}\")\n\n    # Deduplicate predicates\n    unique_predicates = list(set(predicates))\n\n    # Check if there are any valid predicates\n    if not unique_predicates:\n        raise ValueError(\"No valid predicates provided\")\n\n    # Function to evaluate predicates against file/directory names\n    def evaluate_predicates(item_name: str, predicates: list) -> bool:\n        for predicate in predicates:\n            if re.match(predicate, item_name):\n                return True\n        return False\n\n    # Evaluate each item in the directory\n    results = {}\n    for item in dir_path.iterdir():\n        item_name = item.name\n        results[item_name] = evaluate_predicates(item_name, unique_predicates)\n\n    return results"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hex to bytes\n    try:\n        bytes_data = binascii.unhexlify(hex_string)\n    except binascii.Error:\n        return {'error': 'Invalid hexadecimal string'}\n    \n    # Convert bytes to UTF-8 string\n    try:\n        utf8_str = bytes_data.decode('utf-8')\n    except UnicodeDecodeError:\n        return {'error': 'Cannot decode bytes to UTF-8'}\n    \n    # Encode UTF-8 string to various formats\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_data).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_str.encode('utf-32').decode('utf-32'),\n        'ASCII': 'Not representable in ASCII' if not utf8_str.isascii() else utf8_str,\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n    \n    return result"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        print(f\"Downloaded {TARGET_TAR_FILE} successfully.\")\n    except Exception as e:\n        print(f\"Failed to download {TARGET_TAR_FILE}: {e}\")\n        return False\n\n    # Calculate MD5 checksum\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        file_hash = hashlib.md5(f.read()).hexdigest()\n\n    # Validate MD5 checksum\n    if file_hash == EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum matches the expected value.\")\n    else:\n        print(\"MD5 checksum does not match the expected value.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        print(f\"Extracted {TARGET_TAR_FILE} successfully.\")\n    except Exception as e:\n        print(f\"Failed to extract {TARGET_TAR_FILE}: {e}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Cleanup\n    os.remove(TARGET_TAR_FILE)\n    print(f\"Deleted {TARGET_TAR_FILE}.\")\n\n    return True\n\n# Test cases\ndef test_task_func():\n    # Test case 1: Valid URL with matching checksum\n    url1 = \"https://example.com/downloaded_files.tar.gz\"\n    assert task_func(url1) == True\n\n    # Test case 2: Valid URL with mismatching checksum\n    url2 = \"https://example.com/downloaded_files.tar.gz\"\n    global EXPECTED_MD5_CHECKSUM\n    EXPECTED_MD5_CHECKSUM = \"invalid_checksum\"\n    assert task_func(url2) == False\n\n    # Test case 3: Invalid URL\n    url3 = \"https://example.com/nonexistent_file.tar.gz\"\n    assert task_func(url3) == False\n\n    print(\"All test cases passed.\")\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Open the CSV file and read its contents\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            \n            # Check if the specified column_name exists in the CSV file\n            if column_name not in reader.fieldnames:\n                # If not, delete the downloaded file and raise a ValueError\n                os.remove(csv_file_path)\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count the occurrences of each value in the specified column\n            value_counts = collections.Counter()\n            for row in reader:\n                value = row.get(column_name)\n                if value is not None:\n                    value_counts[value] += 1\n            \n            # Delete the downloaded file after processing\n            os.remove(csv_file_path)\n            \n            # Return the dictionary of value counts\n            return dict(value_counts)\n    \n    except Exception as e:\n        # Handle any exceptions that occur during the process\n        # Delete the downloaded file if it exists\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n        # Re-raise the exception\n        raise e"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch XML from URL: {e.reason}\")\n\n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the root element is 'root' and contains 'item' elements\n    if root.tag != 'root':\n        raise ValueError(\"XML structure does not match expected format. Root element should be 'root'.\")\n\n    items = []\n    for item in root.findall('item'):\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        items.append(item_data)\n\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(items)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file from the URL\n    with urllib.request.urlopen(url) as response:\n        text = response.read().decode('utf-8')\n    \n    # Use a regular expression to find all words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    most_common = word_counts.most_common(10)\n    \n    # Extract words and their counts for plotting\n    words, counts = zip(*most_common)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Words')\n    \n    # Return the Counter object and the Axes object\n    return word_counts, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Check if the URL is valid\n        response = requests.head(url)\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Download the file\n        response = requests.get(url)\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Check if the file is a ZIP file\n        if 'application/zip' not in response.headers.get('Content-Type', ''):\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Save the file\n        file_path = os.path.join(download_path, 'downloaded.zip')\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n        \n        # Extract the ZIP file\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        # Return the path to the extracted directory\n        extracted_path = os.path.join(download_path, 'extracted')\n        if not os.path.exists(extracted_path):\n            os.makedirs(extracted_path)\n        return extracted_path\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Test cases"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n    except requests.exceptions.HTTPError as e:\n        raise ValueError(f\"HTTP request failed: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()\n\n    headers = []\n    for th in rows[0].find_all('th'):\n        headers.append(th.text.strip())\n\n    data = []\n    for row in rows[1:]:\n        cells = row.find_all(['th', 'td'])\n        row_data = []\n        for cell in cells:\n            row_data.append(cell.text.strip())\n        data.append(row_data)\n\n    df = pd.DataFrame(data, columns=headers)\n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    # Create download directory if it doesn't exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    # Create unzipped files directory if it doesn't exist\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    # Download the zip file\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception occurred: \" + str(e), [])\n    # Save the zip file\n    zip_path = DOWNLOAD_DIR / filename\n    try:\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n    except IOError as e:\n        return (\"Error: File-related exception occurred: \" + str(e), [])\n    # Extract the zip file\n    try:\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n    except zipfile.BadZipFile as e:\n        return (\"Error: Invalid zip file: \" + str(e), [])\n    except zipfile.LargeZipFile as e:\n        return (\"Error: Zip file too large: \" + str(e), [])\n    except Exception as e:\n        return (\"Error: An error occurred during extraction: \" + str(e), [])\n    # List files in the unzipped directory\n    try:\n        files = [f.name for f in ZIP_DIR.glob('*') if f.is_file()]\n    except Exception as e:\n        return (\"Error: An error occurred while listing files: \" + str(e), [])\n    return (\"Success\", files)"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(url: str, base_url: str = \"https://www.example.com\", csv_file: str = \"scraped_data.csv\") -> int:\n    # Send a GET request to the URL\n    response = requests.get(url)\n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n        return 0\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all <a> tags and extract href attributes\n    links = []\n    for a_tag in soup.find_all('a', href=True):\n        absolute_url = urljoin(base_url, a_tag['href'])\n        links.append(absolute_url)\n\n    # Remove duplicates by converting the list to a set\n    unique_links = set(links)\n\n    # Write the unique links to a CSV file\n    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content from the URL\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    try:\n        # Parse the HTML content\n        tree = html.fromstring(response.content)\n        # Find the table in the HTML\n        table = tree.xpath('//table')[0]\n    except Exception as e:\n        print(f\"Error parsing the HTML: {e}\")\n        return 0\n\n    try:\n        # Convert the table to a pandas DataFrame\n        df = pd.read_html(html.tostring(table).decode())[0]\n    except Exception as e:\n        print(f\"Error converting table to DataFrame: {e}\")\n        return 0\n\n    if df.empty:\n        print(\"The table is empty.\")\n        return 0\n\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        # Replace the table with the new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error connecting to or writing to the database: {e}\")\n        return 0\n    finally:\n        conn.close()\n\n    return len(df)"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nimport io\nimport sys\nimport logging\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n        \n        # Extract text using OCR\n        text = pytesseract.image_to_string(image)\n        \n        # If OCR extraction fails, try to get the comment from the image\n        if not text:\n            # Get the comment from the image\n            text = image.info.get('comment', '')\n        \n        # Convert the text encoding\n        try:\n            # Decode the text from the original encoding\n            decoded_text = text.decode(from_encoding)\n        except (UnicodeDecodeError, LookupError) as e:\n            # If decoding fails, raise a ValueError\n            raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\") from e\n        \n        # Encode the text to the target encoding\n        try:\n            encoded_text = decoded_text.encode(to_encoding)\n        except (UnicodeEncodeError, LookupError) as e:\n            # If encoding fails, raise a ValueError\n            raise ValueError(\"UnicodeEncodeError or LookupError occurred during conversion\") from e\n        \n        # Return the converted text\n        return encoded_text.decode(to_encoding)\n    \n    except Exception as e:\n        # Log the exception\n        logging.error(f\"An error occurred: {e}\")\n        # Return an empty string if OCR extraction and comment processing both fail\n        return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        raise Exception(f\"Failed to fetch data from {url}: {e}\")\n\n    content = response.content\n    if not content:\n        return {}\n\n    if from_encoding is None:\n        try:\n            encoding = chardet.detect(content)['encoding']\n            if encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n        except Exception as e:\n            raise Exception(f\"Error detecting encoding: {e}\")\n    else:\n        encoding = from_encoding\n\n    try:\n        decoded_content = content.decode(encoding)\n    except UnicodeDecodeError as e:\n        raise Exception(f\"Decoding error: {e}\")\n\n    try:\n        reencoded_content = decoded_content.encode(to_encoding)\n    except UnicodeEncodeError as e:\n        raise Exception(f\"Re-encoding error: {e}\")\n\n    try:\n        data = json.loads(reencoded_content.decode(to_encoding))\n    except json.JSONDecodeError as e:\n        raise Exception(f\"JSON parsing error: {e}\")\n\n    return data"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    # Read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()  # Return an empty DataFrame for empty files\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n    \n    # Convert the date column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    \n    # Get the current date\n    current_date = datetime.now().date()\n    \n    # Filter rows where the date is equal to the current date\n    filtered_df = df[df[column_name].dt.date == current_date]\n    \n    # Sort the filtered data by the date column in ascending order\n    sorted_df = filtered_df.sort_values(by=column_name)\n    \n    return sorted_df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL/TLS\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(cert_file, key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode()\n        \n        # Check if the file exists\n        if not os.path.exists(file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n        \n        # Calculate the SHA256 hash of the file\n        with open(file_path, 'rb') as file:\n            file_hash = hashlib.sha256()\n            while chunk := file.read(buffer_size):\n                file_hash.update(chunk)\n            hash_value = file_hash.hexdigest()\n        \n        # Send the hash back to the client\n        secure_socket.sendall(hash_value.encode())\n        \n        return hash_value\n    except Exception as e:\n        # Handle any exceptions that occur during processing\n        error_message = f'An error occurred: {str(e)}'\n        secure_socket.sendall(error_message.encode())\n        return error_message"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5):\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n    server_socket.setblocking(False)\n    inputs = [server_socket]\n    outputs = []\n    message_queues = {}\n    try:\n        while datetime.now() < end_time:\n            readable, writable, exceptional = select.select(inputs, outputs, inputs, 1)\n            for s in readable:\n                if s is server_socket:\n                    connection, client_address = s.accept()\n                    connection.setblocking(False)\n                    inputs.append(connection)\n                    message_queues[connection] = queue.Queue()\n                else:\n                    data = s.recv(buffer_size)\n                    if data:\n                        message_queues[s].put(data)\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n            for s in writable:\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    outputs.remove(s)\n                else:\n                    s.send(next_msg)\n                    time_now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    s.send(time_now.encode())\n            for s in exceptional:\n                inputs.remove(s)\n                if s in outputs:\n                    outputs.remove(s)\n                s.close()\n                del message_queues[s]\n    finally:\n        server_socket.close()\n        status_message = f\"Server has stopped after {run_duration} seconds.\"\n        return status_message"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Receive message from client socket\n    message = client_socket.recv(BUFFER_SIZE).decode()\n    # Ask for sender's email, recipient's email, and sender's email password\n    sender_email = input(\"Enter sender's email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    sender_password = getpass.getpass(\"Enter sender's email password: \")\n    # Create email message\n    msg = EmailMessage()\n    msg.set_content(message)\n    msg['Subject'] = 'Message from client'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    # Connect to SMTP server and send email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(sender_email, sender_password)\n        server.send_message(msg)\n        print(\"Email sent successfully!\")\n    except Exception as e:\n        print(f\"Failed to send email: {e}\")\n    finally:\n        server.quit()"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        # Check if the CSV has a header\n        if df.columns[0] == 'Text':\n            text_data = df['Text']\n        else:\n            text_data = df.iloc[:, 0]\n        # Initialize the CountVectorizer with the predefined stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        # Fit and transform the text data\n        X = vectorizer.fit_transform(text_data)\n        # Get the feature names (words)\n        feature_names = vectorizer.get_feature_names_out()\n        # Sum the counts across all documents\n        counts = X.sum(axis=0)\n        # Create a DataFrame with words and their counts\n        word_counts = pd.DataFrame(counts.T, index=feature_names, columns=['Count'])\n        # Sort the DataFrame by count in descending order\n        word_counts = word_counts.sort_values(by='Count', ascending=False)\n        # Get the top 10 words\n        top_words = word_counts.head(10)\n        # Plot the histogram\n        plt.figure(figsize=(10, 6))\n        plt.barh(top_words.index, top_words['Count'], color='skyblue')\n        plt.xlabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        if save_path:\n            plt.savefig(save_path)\n            print(f\"Plot saved to {save_path}\")\n        else:\n            plt.show()\n            return plt.gca()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Define default lists if animals or foods are not provided\n    default_animals = ['dog', 'cat', 'bird']\n    default_foods = ['meat', 'fish', 'vegetables']\n    \n    # Use default lists if animals or foods are not provided\n    if animals is None:\n        animals = default_animals\n    if foods is None:\n        foods = default_foods\n    \n    # Generate all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations to randomize the order\n    np.random.shuffle(combinations)\n    \n    # Create a DataFrame with the shuffled combinations\n    df = pd.DataFrame(combinations, columns=['animal', 'food'])\n    \n    # Format the cells to 'animal:food'\n    df['animal:food'] = df['animal'] + ':' + df['food']\n    \n    # Drop the original 'animal' and 'food' columns\n    df = df[['animal:food']]\n    \n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert each timestamp to the specified timezone\n    converted_times = []\n    for time_str in time_strings:\n        # Assuming the input timestamps are in UTC\n        naive_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')\n        utc_time = pytz.utc.localize(naive_time)\n        target_tz = pytz.timezone(timezone)\n        converted_time = utc_time.astimezone(target_tz)\n        converted_times.append(converted_time)\n\n    # Calculate the absolute time difference in seconds between each consecutive pair\n    time_differences = []\n    for i in range(len(converted_times) - 1):\n        diff = abs(converted_times[i+1] - converted_times[i])\n        time_differences.append(diff.total_seconds())\n\n    # Calculate the mean of the time differences\n    if len(time_differences) == 0:\n        return 0.0\n    else:\n        return np.mean(time_differences)"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation using regular expression\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_words = word_counts.most_common(10)\n    \n    # Extract words and their counts for plotting\n    words, counts = zip(*top_words)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    \n    # Return the list of top words and their counts, and the Axes object\n    return list(top_words), ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    urls = url_pattern.findall(myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    # Try to fetch the first URL\n    url = urls[0]\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the content with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find the title tag\n    title_tag = soup.find('title')\n    if title_tag:\n        return title_tag.text.strip()\n    else:\n        return \"No title tag found in the webpage.\"\n\n# Test cases"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON string to find the URL associated with the specified key\n    data = json.loads(json_data)\n    url = data.get(unknown_key)\n    \n    if url is None:\n        return \"URL not found for the specified key\"\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    \n    if response.status_code != 200:\n        return f\"Failed to download file. Status code: {response.status_code}\"\n    \n    # Generate a timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    else:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n    \n    # Save the file with the timestamped filename\n    file_path = os.path.join(save_dir, filename)\n    with open(file_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Return the absolute path of the downloaded file\n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    # Fetch the content from the URL\n    response = requests.get(url)\n    content = response.text\n\n    # Use BeautifulSoup to parse the HTML content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n\n    # Find all script tags\n    script_tags = soup.find_all('script')\n\n    # Initialize a list to hold phone numbers\n    phone_numbers = []\n\n    # Regular expression pattern for matching phone numbers\n    pattern = re.compile(r'(\\+?\\d{1,3}[-\\.\\s]??\\d{1,4}[-\\.\\s]??\\d{1,4}[-\\.\\s]??\\d{1,9})')\n\n    # Iterate through each script tag\n    for script in script_tags:\n        # Extract text from the script tag\n        text = script.string\n        if text:\n            # Find all matches of the phone number pattern in the text\n            matches = pattern.findall(text)\n            # Extend the list with the found phone numbers\n            phone_numbers.extend(matches)\n\n    # Write the list of phone numbers to a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    # Return the list of phone numbers\n    return phone_numbers"}

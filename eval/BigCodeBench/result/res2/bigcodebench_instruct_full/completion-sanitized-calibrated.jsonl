{"task_id": "BigCodeBench/0", "solution": "import itertools\nfrom random import shuffle\ndef task_func(numbers=list(range(1, 3))):\n    # Generate all permutations of the input list\n    perms = itertools.permutations(numbers)\n    \n    # Initialize a list to hold the sums of absolute differences for each permutation\n    sum_diffs = []\n    \n    # Iterate through each permutation\n    for perm in perms:\n        # Shuffle the permutation\n        shuffle(list(perm))\n        \n        # Calculate the sum of absolute differences between consecutive numbers\n        sum_diff = sum(abs(perm[i] - perm[i+1]) for i in range(len(perm)-1))\n        \n        # Append the sum to the list\n        sum_diffs.append(sum_diff)\n    \n    # Calculate the average of the sums\n    avg_sum_diff = sum(sum_diffs) / len(sum_diffs)\n    \n    # Return the average\n    return avg_sum_diff"}
{"task_id": "BigCodeBench/1", "solution": "import collections\nimport random\nimport string\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n    count = collections.Counter(random_string)\n    return dict(count)"}
{"task_id": "BigCodeBench/2", "solution": "import random\nimport statistics\ndef task_func(LETTERS):\n    # Create a dictionary with random letters as keys and lists of random integers as values\n    random_dict = {}\n    for _ in range(LETTERS):\n        key = chr(random.randint(97, 122))  # Generate a random lowercase letter\n        value = [random.randint(1, 100) for _ in range(random.randint(1, 10))]  # Generate a list of random integers\n        random_dict[key] = value\n\n    # Sort the dictionary by the mean of the values in descending order\n    sorted_dict = dict(sorted(random_dict.items(), key=lambda item: statistics.mean(item[1]), reverse=True))\n\n    return sorted_dict"}
{"task_id": "BigCodeBench/3", "solution": "import random\nimport numpy as np\ndef task_func(letters):\n    result = {}\n    for letter in letters:\n        random_list = [random.randint(0, 100) for _ in range(random.randint(1, 10))]\n        mean = np.mean(random_list)\n        result[letter] = mean\n    return result"}
{"task_id": "BigCodeBench/4", "solution": "from collections import Counter\nimport itertools\ndef task_func(d):\n    # Flatten the list of lists into a single list\n    flat_list = list(itertools.chain.from_iterable(d.values()))\n    # Count the occurrences of each integer in the flattened list\n    count_dict = Counter(flat_list)\n    return dict(count_dict)"}
{"task_id": "BigCodeBench/5", "solution": "import random\nimport math\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    result = {}\n    for letter in LETTERS:\n        random_list = [random.randint(0, 100) for _ in range(random.randint(1, 10))]\n        mean = sum(random_list) / len(random_list)\n        variance = sum((x - mean) ** 2 for x in random_list) / len(random_list)\n        std_dev = math.sqrt(variance)\n        result[letter] = std_dev\n    return result"}
{"task_id": "BigCodeBench/6", "solution": "import os\nimport re\nimport glob\ndef task_func(pattern, log_dir='/var/log/'):\n    # Get all files in the specified directory\n    files = glob.glob(os.path.join(log_dir, '*'))\n    \n    # Filter files based on the provided regex pattern\n    matching_files = [f for f in files if re.match(pattern, os.path.basename(f))]\n    \n    # If no files match the pattern, return None\n    if not matching_files:\n        return None\n    \n    # Find the most recent file based on modification time\n    latest_file = max(matching_files, key=os.path.getmtime)\n    \n    # Return the path to the latest log file\n    return latest_file"}
{"task_id": "BigCodeBench/7", "solution": "import csv\nimport collections\nimport operator\ndef task_func(csv_file_path):\n    with open(csv_file_path, mode='r') as file:\n        reader = csv.DictReader(file)\n        sales_data = collections.defaultdict(int)\n        for row in reader:\n            product = row['product']\n            quantity = int(row['quantity'])\n            sales_data[product] += quantity\n    return max(sales_data.items(), key=operator.itemgetter(1))[0]"}
{"task_id": "BigCodeBench/8", "solution": "from collections import Counter\nimport itertools\nfrom random import randint\ndef task_func(T1, RANGE=100):\n    # Convert elements in 'T1' to integers\n    T1_int = [int(x) for x in T1]\n    \n    # Calculate the sum of integers in T1\n    sum_T1 = sum(T1_int)\n    \n    # Generate a list of random integers\n    random_list = [randint(0, RANGE) for _ in range(sum_T1)]\n    \n    # Count the occurrences of each number in the random list\n    counter = Counter(random_list)\n    \n    # Return the Counter object\n    return counter"}
{"task_id": "BigCodeBench/9", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    \n    # Create a bar chart using seaborn\n    plt.figure(figsize=(10,6))\n    sns.barplot(x='Category', y='Value', data=df)\n    plt.title('Category vs Value')\n    plt.show()\n    \n    return df, plt.gca()\nlist_of_pairs = [('A', 10), ('B', 20), ('C', 30)]"}
{"task_id": "BigCodeBench/10", "solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\ndef task_func(T1, RANGE=100):\n    if not T1:\n        raise statistics.StatisticsError(\"T1 is empty\")\n    \n    # Convert elements in 'T1' to integers\n    try:\n        T1 = [int(x) for x in T1]\n    except ValueError:\n        raise ValueError(\"All elements in T1 must be convertible to integers\")\n    \n    # Calculate the sum of the integers in T1\n    total_sum = sum(T1)\n    \n    # Create a list of random integers with size equal to the sum of T1\n    random_list = [random.randint(0, RANGE) for _ in range(total_sum)]\n    \n    # Calculate mean, median, and mode\n    mean = np.mean(random_list)\n    median = np.median(random_list)\n    mode = statistics.mode(random_list)\n    \n    return (mean, median, mode)"}
{"task_id": "BigCodeBench/11", "solution": "import numpy as np\nimport itertools\nimport random\ndef task_func(T1, max_value=100):\n    # Convert elements in 'T1' to integers\n    int_list = [int(item) for item in itertools.chain.from_iterable(T1)]\n    \n    # Create a list of random integers based on the sum of these integers\n    total_sum = sum(int_list)\n    random_list = [random.randint(0, max_value) for _ in range(total_sum)]\n    \n    # Calculate the 25th, 50th, and 75th percentiles\n    p25 = np.percentile(random_list, 25)\n    p50 = np.percentile(random_list, 50)\n    p75 = np.percentile(random_list, 75)\n    \n    # Return the percentiles as a tuple\n    return (p25, p50, p75)"}
{"task_id": "BigCodeBench/12", "solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    try:\n        if not os.path.exists(script_name):\n            raise FileNotFoundError(f\"The script file {script_name} does not exist.\")\n        \n        start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        \n        process = subprocess.run(['bash', script_name], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        \n        end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        exit_status = process.returncode\n        \n        log_data = {\n            'start_time': start_time,\n            'end_time': end_time,\n            'exit_status': exit_status\n        }\n        \n        with open(log_file, 'w') as f:\n            json.dump(log_data, f)\n        \n        if exit_status != 0:\n            raise RuntimeError(f\"The script {script_name} exited with status {exit_status}.\")\n        \n    except FileNotFoundError as e:\n        print(e)\n        raise\n    except subprocess.CalledProcessError as e:\n        print(f\"Error executing the script: {e.stderr.decode()}\")\n        raise RuntimeError(f\"Error executing the script: {e.stderr.decode()}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise"}
{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except ftplib.error_perm as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    except ftplib.error_proto as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except ftplib.error_perm as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # List all files in the directory\n    files = ftp.nlst()\n    \n    # Download each file using wget\n    downloaded_files = []\n    for file in files:\n        try:\n            # Construct the wget command\n            wget_command = f\"wget -O {file} ftp://{ftp_user}:{ftp_password}@{ftp_server}/{ftp_dir}/{file}\"\n            # Execute the wget command using subprocess\n            subprocess.run(wget_command, shell=True, check=True)\n            downloaded_files.append(file)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {file}: {str(e)}\")\n    \n    return downloaded_files"}
{"task_id": "BigCodeBench/14", "solution": "import configparser\nimport os\nimport shutil\ndef task_func(config_file_path, archive_dir='/home/user/archive'):\n    # Check if the config file exists\n    if not os.path.exists(config_file_path):\n        raise FileNotFoundError(f\"Config file not found at {config_file_path}\")\n    \n    # Read the config file\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n    \n    # Check if the [Project] section exists\n    if 'Project' not in config:\n        raise ValueError(\"Config file must contain a [Project] section\")\n    \n    # Get the project directory path from the config file\n    project_dir = config['Project'].get('directory')\n    if not project_dir:\n        raise ValueError(\"Config file must contain a 'directory' key in the [Project] section\")\n    \n    # Check if the project directory exists\n    if not os.path.exists(project_dir):\n        raise FileNotFoundError(f\"Project directory not found at {project_dir}\")\n    \n    # Get the archive directory path\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n    \n    # Get the basename of the project directory\n    zip_filename = os.path.basename(project_dir) + '.zip'\n    zip_path = os.path.join(archive_dir, zip_filename)\n    \n    # Create a ZIP archive of the project directory\n    try:\n        shutil.make_archive(zip_path, 'zip', project_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to create ZIP archive: {e}\")\n    \n    return True"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        commands = [row[0] for row in reader]\n    \n    output_files = []\n    for index, command in enumerate(commands):\n        output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n        with open(output_file_path, 'w') as output_file:\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n                output_file.write(result.stdout.decode())\n            except subprocess.CalledProcessError as e:\n                output_file.write(f\"Command failed with exit code {e.returncode}:\\n{e.output.decode()}\")\n            except Exception as e:\n                output_file.write(f\"An error occurred: {str(e)}\")\n        output_files.append(output_file_path)\n    \n    return output_files"}
{"task_id": "BigCodeBench/16", "solution": "import os\nimport glob\nimport subprocess\ndef task_func(directory, backup_dir='/path/to/backup'):\n    # Check if the specified directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    # Find all .log files in the specified directory\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n    \n    # If no log files are found, return a message\n    if not log_files:\n        return 'No logs found to backup'\n    \n    # Create the backup directory if it doesn't exist\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    \n    # Create a tar.gz file of the log files\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n    subprocess.run(['tar', '-czf', backup_file] + log_files)\n    \n    # Delete the original log files\n    for file in log_files:\n        os.remove(file)\n    \n    # Return the path to the backup file\n    return backup_file"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, terminate and restart it\n            proc.terminate()\n            time.sleep(2)  # Wait for the process to terminate\n            # Start the process again\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # Process is not running, start it\n    subprocess.Popen([process_name])\n    return f\"Process not found. Starting {process_name}.\""}
{"task_id": "BigCodeBench/18", "solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\ndef task_func(file):\n    if not os.path.exists(file):\n        return []\n    if not file.endswith('.csv'):\n        return []\n    try:\n        subprocess.run(['split', '-l', '1000', file, 'split_'], check=True)\n    except subprocess.CalledProcessError:\n        return []\n    split_files = glob.glob('split_*')\n    for f in split_files:\n        with open(f, 'r') as infile:\n            lines = infile.readlines()\n        random.shuffle(lines)\n        with open(f, 'w') as outfile:\n            outfile.writelines(lines)\n    return split_files"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    files = glob.glob(os.path.join(directory, '*'))\n    if not files:\n        return None\n    \n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            if os.path.isfile(file):\n                zipf.write(file, os.path.basename(file))\n    \n    return zip_path"}
{"task_id": "BigCodeBench/20", "solution": "import ast\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n    \n    # Convert string representations of dictionaries to actual dictionaries\n    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n    \n    # Convert the list of dictionaries to a DataFrame\n    dict_df = pd.DataFrame(df['dict_column'].tolist())\n    \n    # Concatenate the original DataFrame and the new DataFrame\n    df = pd.concat([df, dict_df], axis=1)\n    \n    # Drop the original 'dict_column'\n    df = df.drop('dict_column', axis=1)\n    \n    # Plot the data with Seaborn's pairplot\n    ax = sns.pairplot(df)\n    \n    return (df, ax)"}
{"task_id": "BigCodeBench/21", "solution": "import psutil\nimport platform\ndef task_func():\n    # Get OS name\n    os_name = platform.system()\n    \n    # Get system architecture\n    architecture = platform.architecture()[0]\n    \n    # Get memory usage\n    memory = psutil.virtual_memory()\n    memory_usage = memory.percent\n    \n    # Return the results in a dictionary\n    return {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f\"{memory_usage}%\"\n    }"}
{"task_id": "BigCodeBench/22", "solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\ndef task_func(l1, l2, K=10):\n    # Combine two lists by alternating their elements\n    combined = [item for pair in zip_longest(l1, l2) for item in pair if item is not None]\n    \n    # Create a random sample of size K from the combined list\n    sample = choices(combined, k=K)\n    \n    # Calculate the frequency of each element in the sample\n    frequency = collections.Counter(sample)\n    \n    return frequency"}
{"task_id": "BigCodeBench/23", "solution": "import numpy as np\nfrom itertools import zip_longest\ndef task_func(l1, l2, THRESHOLD=0.5):\n    # Combine the two lists using zip_longest to handle lists of different lengths\n    combined = zip_longest(l1, l2)\n    \n    # Initialize a list to store the absolute differences\n    differences = []\n    \n    # Iterate over the combined list\n    for a, b in combined:\n        # Skip None values\n        if a is None or b is None:\n            continue\n        # Calculate the absolute difference from the threshold\n        diff_a = abs(a - THRESHOLD)\n        diff_b = abs(b - THRESHOLD)\n        # Append the differences to the list\n        differences.append((a, diff_a))\n        differences.append((b, diff_b))\n    \n    # Find the element with the smallest difference from the threshold\n    if differences:\n        closest = min(differences, key=lambda x: x[1])\n        return closest[0]\n    else:\n        return None  # or raise an error if no elements are found"}
{"task_id": "BigCodeBench/24", "solution": "import base64\nimport hashlib\nimport os\ndef task_func(password, SALT_LENGTH=32):\n    if password is None or password == \"\":\n        raise ValueError(\"Password cannot be None or empty\")\n    \n    salt = os.urandom(SALT_LENGTH)\n    key = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\n    return base64.b64encode(salt), base64.b64encode(key)"}
{"task_id": "BigCodeBench/25", "solution": "import base64\nimport json\nimport zlib\ndef task_func(data_dict):\n    # Serialize the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n    \n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode('utf-8'))\n    \n    # Encode the compressed data with base64\n    base64_encoded = base64.b64encode(compressed_data)\n    \n    # Convert the base64 encoded bytes to a string\n    base64_str = base64_encoded.decode('utf-8')\n    \n    return base64_str"}
{"task_id": "BigCodeBench/26", "solution": "import base64\nfrom cryptography.fernet import Fernet\ndef task_func(message, encryption_key):\n    # Generate a Fernet key from the provided encryption key\n    fernet_key = Fernet.generate_key()\n    \n    # Create a Fernet cipher suite using the generated key\n    cipher_suite = Fernet(fernet_key)\n    \n    # Encrypt the message using the cipher suite\n    encrypted_message = cipher_suite.encrypt(message.encode())\n    \n    # Encode the encrypted message using base64\n    base64_encoded_message = base64.b64encode(encrypted_message)\n    \n    # Return the base64 encoded encrypted message as a string\n    return base64_encoded_message.decode()"}
{"task_id": "BigCodeBench/27", "solution": "import json\nimport base64\nfrom datetime import datetime\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    # Add a current timestamp to the dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n    \n    # Serialize the modified dictionary to a JSON-formatted string\n    json_str = json.dumps(data)\n    \n    # Encode the JSON string using base64 encoding with ASCII character encoding\n    encoded_str = base64.b64encode(json_str.encode('ascii')).decode('ascii')\n    \n    return encoded_str"}
{"task_id": "BigCodeBench/28", "solution": "import requests\nimport json\nimport base64\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert the Python dictionary into a JSON-formatted string\n    json_data = json.dumps(data)\n    \n    # Encode the JSON string in base64 format\n    base64_data = base64.b64encode(json_data.encode('utf-8'))\n    \n    # Send the base64-encoded string as a 'payload' in a POST request to the API endpoint\n    response = requests.post(url, data={'payload': base64_data.decode('utf-8')})\n    \n    # Return the response object\n    return response"}
{"task_id": "BigCodeBench/29", "solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\ndef task_func(data):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n    \n    # Convert the standardized data to bytes\n    data_bytes = standardized_data.tobytes()\n    \n    # Encode the bytes to base64\n    base64_bytes = base64.b64encode(data_bytes)\n    \n    # Convert the base64 bytes to a string\n    base64_string = base64_bytes.decode('ascii')\n    \n    return base64_string"}
{"task_id": "BigCodeBench/30", "solution": "import json\nimport os\nimport re\ndef task_func(file_path, attribute, INPUT_JSON, EMAIL_REGEX):\n    if not os.path.exists(file_path):\n        raise ValueError(\"The file does not exist.\")\n    \n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    for key in INPUT_JSON['required']:\n        if key not in data:\n            raise ValueError(f\"Missing required attribute: {key}\")\n    \n    for key, value in INPUT_JSON['properties'].items():\n        if key in data:\n            if value['type'] == str and not isinstance(data[key], str):\n                raise ValueError(f\"Type mismatch for {key}: expected str, got {type(data[key]).__name__}\")\n            elif value['type'] == int and not isinstance(data[key], int):\n                raise ValueError(f\"Type mismatch for {key}: expected int, got {type(data[key]).__name__}\")\n            elif value['type'] == str and key == 'email' and not re.match(EMAIL_REGEX, data[key]):\n                raise ValueError(f\"Invalid email format for {key}: {data[key]}\")\n    \n    return data.get(attribute, None)"}
{"task_id": "BigCodeBench/31", "solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nPUNCTUATION = set(punctuation)\ndef task_func(text):\n    # Tokenize the text into words\n    words = nltk.word_tokenize(text)\n    \n    # Filter words that start with '$' and do not consist only of punctuation\n    filtered_words = [word for word in words if word.startswith('$') and not set(word).issubset(PUNCTUATION)]\n    \n    # Count the frequency of each filtered word\n    word_counts = Counter(filtered_words)\n    \n    # If there are no words meeting the criteria, return None\n    if not word_counts:\n        return None\n    \n    # Create a bar chart using seaborn\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=list(word_counts.keys()), y=list(word_counts.values()))\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Words Starting with \"$\"')\n    plt.xticks(rotation=45)\n    plt.show()\n    \n    # Return the axes object\n    return plt.gca()"}
{"task_id": "BigCodeBench/32", "solution": "import requests\nfrom bs4 import BeautifulSoup\ndef task_func(url, tag):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return None\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    element = soup.find(tag)\n    if element:\n        return element.get_text()\n    else:\n        return None"}
{"task_id": "BigCodeBench/33", "solution": "import numpy as np\nfrom functools import reduce\ndef task_func(list_of_pairs):\n    # Extract the second values from each tuple\n    second_values = [pair[1] for pair in list_of_pairs]\n    \n    # Calculate the product of these second values\n    product = reduce(lambda x, y: x * y, second_values)\n    \n    # Return the product as a single-element numpy array\n    return np.array([product])"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Check if there are any words left after removing URLs\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud().generate(' '.join(words))\n    \n    # Display the word cloud\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud"}
{"task_id": "BigCodeBench/35", "solution": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace all elements in DataFrame columns that do not exist in the target_values array with zeros\n    for col in df.columns:\n        df[col] = df[col].apply(lambda x: x if x in target_values else 0)\n    \n    # Output the distribution of each column after replacing\n    for col in df.columns:\n        plt.figure()\n        sns.histplot(df[col], kde=True)\n        plt.title(f'Distribution of {col}')\n        plt.show()\n    \n    return plt.gca()\ndata = {\n    'A': [1, 2, 3, 4, 5],\n    'B': [2, 3, 4, 5, 6],\n    'C': [3, 4, 5, 6, 7]\n}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/36", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nTARGET_VALUES = np.array([1, 3, 4])\ndef task_func(df):\n    # Replace all elements in DataFrame columns that do not exist in the TARGET_VALUES array with zeros\n    for col in df.columns:\n        df[col] = df[col].apply(lambda x: x if x in TARGET_VALUES else 0)\n    \n    # Perform a Box-Cox transformation on each column (if data is not constant, add 1 to account for zeros)\n    transformed_df = pd.DataFrame()\n    for col in df.columns:\n        if df[col].nunique() > 1:\n            transformed_df[col] = stats.boxcox(df[col] + 1)[0]\n        else:\n            transformed_df[col] = df[col]\n    \n    # Display the resulting KDE plots\n    fig, axes = plt.subplots(nrows=1, ncols=len(transformed_df.columns), figsize=(15, 3))\n    for i, col in enumerate(transformed_df.columns):\n        stats.kdeplot(transformed_df[col], ax=axes[i])\n        axes[i].set_title(col)\n    \n    plt.show()\n    \n    return transformed_df, fig\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [2, 3, 4, 5, 6],\n    'C': [3, 4, 5, 6, 7]\n})"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    # Split the dataframe into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the Random Forest Classifier\n    clf = RandomForestClassifier()\n    clf.fit(X, y)\n    \n    # Get feature importances\n    importances = clf.feature_importances_\n    \n    # Create a dataframe to hold feature names and their importances\n    feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n    \n    # Sort the dataframe by importance in descending order\n    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n    \n    # Plot the bar plot\n    plt.figure(figsize=(10,6))\n    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n    \n    # Return the trained classifier and the axes object of the plot\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/38", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\ndef task_func(data_matrix):\n    # Standardize the data matrix\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n    \n    # Calculate the mean value of each row\n    means = standardized_data.mean(axis=1)\n    \n    # Create a DataFrame containing the standardized data and the mean of each row\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = means\n    \n    # Create a histogram plot of the distribution of means\n    plt.hist(means, bins=30, color='skyblue', edgecolor='black')\n    plt.title('Distribution of Means')\n    plt.xlabel('Mean Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    # Return the DataFrame and the histogram plot\n    return df, plt.gca()\ndata_matrix = np.array([\n    [1, 2, 3, 4, 5],\n    [2, 3, 4, 5, 6],\n    [3, 4, 5, 6, 7],\n    [4, 5, 6, 7, 8],\n    [5, 6, 7, 8, 9]\n])"}
{"task_id": "BigCodeBench/39", "solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\nALPHA = 0.05\ndef task_func(data_matrix):\n    # Calculate the mean of each row\n    row_means = np.mean(data_matrix, axis=1)\n    \n    # Calculate the overall mean\n    population_mean = np.mean(data_matrix)\n    \n    # Perform t-test for each row mean against the population mean\n    t_stat, p_value = ttest_1samp(row_means, population_mean)\n    \n    # Find indices of rows with p-value less than ALPHA\n    significant_indices = np.where(p_value < ALPHA)[0]\n    \n    # Create a figure and axis for plotting\n    fig, ax = plt.subplots()\n    \n    # Plot the mean of each row in red\n    ax.plot(row_means, color='red', label='Means')\n    \n    # Plot the significant means in blue\n    ax.plot(row_means[significant_indices], color='blue', label='Significant Means')\n    \n    # Plot the population mean as a horizontal line in green\n    ax.axhline(y=population_mean, color='green', label='Population Mean')\n    \n    # Set labels and legend\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Mean Value')\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return significant_indices, ax"}
{"task_id": "BigCodeBench/40", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(data_matrix):\n    # Calculate Z-scores for each row\n    z_scores = zscore(data_matrix, axis=1)\n    \n    # Calculate the mean of Z-scores per row\n    mean_z = z_scores.mean(axis=1)\n    \n    # Create a DataFrame with Z-scores and their mean\n    df = pd.DataFrame(z_scores, columns=[f'Feature {i+1}' for i in range(data_matrix.shape[1])])\n    df['Mean'] = mean_z\n    \n    # Calculate the correlation matrix of Z-scores\n    corr_matrix = df.corr()\n    \n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    \n    return df, heatmap"}
{"task_id": "BigCodeBench/41", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\ndef task_func(data_matrix):\n    # Calculate the skewness of each row\n    skewness = data_matrix.apply(lambda x: skew(x), axis=1)\n    \n    # Create a DataFrame to store the skewness\n    skew_df = pd.DataFrame({'Skewness': skewness})\n    \n    # Plot the distribution of skewness\n    plt.figure()\n    skew_df['Skewness'].plot(kind='hist', bins=30, title='Distribution of Skewness')\n    plt.xlabel('Skewness')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return skew_df, plt.gca()"}
{"task_id": "BigCodeBench/42", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data_matrix, n_components=2):\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    pca.fit(data_matrix)\n    transformed_data = pca.transform(data_matrix)\n    \n    # Calculate mean of each component\n    mean_values = transformed_data.mean(axis=1)\n    \n    # Create a DataFrame with the transformed data and the mean\n    columns = [f'Component {i+1}' for i in range(n_components)]\n    columns.append('Mean')\n    df = pd.DataFrame(transformed_data, columns=columns)\n    df['Mean'] = mean_values\n    \n    # Calculate cumulative explained variance\n    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n    \n    # Plot cumulative explained variance\n    plt.figure()\n    plt.plot(range(1, n_components+1), cumulative_variance, marker='o')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.title('Cumulative Explained Variance vs. Number of Components')\n    plt.grid(True)\n    plt.show()\n    \n    return df, plt.gca()"}
{"task_id": "BigCodeBench/43", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df):\n    # Replace NaN values with the average of the column\n    for col in df.columns:\n        if df[col].dtype in [np.int64, np.float64]:\n            col_mean = df[col].mean(skipna=True)\n            df[col].fillna(col_mean, inplace=True)\n    \n    # Create a DataFrame with statistics\n    stats_df = df.describe()\n    \n    # Create a list to hold the distribution plots\n    axes = []\n    \n    # Draw a distribution chart for each numeric column\n    for col in df.columns:\n        if df[col].dtype in [np.int64, np.float64]:\n            ax = sns.histplot(df[col], kde=True, bins=10)\n            axes.append(ax)\n    \n    return (stats_df, axes)"}
{"task_id": "BigCodeBench/44", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Normalize numeric columns\n    scaler = MinMaxScaler()\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    \n    # Replace missing values with column's average\n    for col in numeric_cols:\n        col_mean = df[col].mean()\n        df[col].fillna(col_mean, inplace=True)\n    \n    # Draw a box plot for each column\n    fig, axes = plt.subplots(nrows=1, ncols=len(numeric_cols), figsize=(15, 5))\n    for i, col in enumerate(numeric_cols):\n        df[col].plot.box(ax=axes[i])\n        axes[i].set_title(col)\n    \n    plt.show()\n    \n    return df, axes"}
{"task_id": "BigCodeBench/45", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame):\n    # Replace missing values with column's average\n    for col in df.columns:\n        if df[col].dtype in [np.int64, np.float64]:\n            df[col].fillna(df[col].mean(), inplace=True)\n    \n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df.select_dtypes(include=[np.int64, np.float64]))\n    \n    # Create a DataFrame with the first two principal components\n    pca_df = pd.DataFrame(data=principal_components, columns=['principal component 1', 'principal component 2'])\n    \n    # Create a scatter plot\n    plt.figure(figsize=(8,6))\n    sns.scatterplot(x='principal component 1', y='principal component 2', data=pca_df)\n    plt.xlabel('principal component 1')\n    plt.ylabel('principal component 2')\n    \n    # Return the DataFrame and the Axes object\n    return pca_df, plt.gca()"}
{"task_id": "BigCodeBench/46", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Replace missing values with the column's average\n    for col in df.columns:\n        if df[col].dtype in [np.int64, np.float64]:\n            col_mean = df[col].mean()\n            df[col].fillna(col_mean, inplace=True)\n    \n    # Calculate Z-scores for numeric columns\n    z_scores = df.select_dtypes(include=[np.int64, np.float64]).apply(zscore)\n    \n    # Draw a histogram for each column\n    axes = []\n    for col in df.columns:\n        if df[col].dtype in [np.int64, np.float64]:\n            ax = df[col].hist(bins=10)\n            axes.append(ax)\n    \n    return z_scores, axes"}
{"task_id": "BigCodeBench/47", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Replace missing values with the column's average\n    for col in df.columns:\n        if df[col].dtype == 'float64' or df[col].dtype == 'int64':\n            col_mean = df[col].mean()\n            df[col].fillna(col_mean, inplace=True)\n    \n    # Standardize numeric columns\n    scaler = StandardScaler()\n    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    \n    # Compute correlation matrix\n    corr_matrix = df.corr()\n    \n    # Generate heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n    \n    return df, corr_matrix"}
{"task_id": "BigCodeBench/48", "solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(n, output_path=None):\n    # Generate n random Unix timestamps\n    timestamps = [random.randint(0, int(time.time())) for _ in range(n)]\n    # Convert them to strings formatted as UTC DATE_FORMAT\n    formatted_timestamps = [datetime.utcfromtimestamp(ts).strftime(DATE_FORMAT) for ts in timestamps]\n    # Plot a histogram of the distribution of the generated timestamps\n    plt.hist(timestamps, bins=100, color='skyblue', edgecolor='black')\n    plt.title('Distribution of Random Unix Timestamps')\n    plt.xlabel('Unix Timestamp')\n    plt.ylabel('Frequency')\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n    # Return the list of n formatted timestamps\n    return formatted_timestamps"}
{"task_id": "BigCodeBench/49", "solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n    # Convert Unix timestamps to datetime objects\n    datetimes = [datetime.fromtimestamp(ts) for ts in timestamps]\n    # Create a Pandas DataFrame\n    df = pd.DataFrame({'Timestamp': timestamps, 'Datetime': datetimes})\n    # Draw a histogram\n    axes = df['Datetime'].hist(bins=10)\n    return df, axes"}
{"task_id": "BigCodeBench/50", "solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\ndef task_func(timestamp):\n    # Convert the Unix timestamp to a datetime object in UTC\n    utc_time = datetime.utcfromtimestamp(timestamp).replace(tzinfo=pytz.UTC)\n    \n    # Create a list to hold the datetime objects in different time zones\n    datetimes = []\n    \n    # Iterate over the time zones and convert the UTC time to each time zone\n    for tz_name in TIMEZONES:\n        tz = pytz.timezone(tz_name)\n        tz_time = utc_time.astimezone(tz)\n        datetimes.append(tz_time)\n    \n    # Create a pandas DataFrame with 'Timezone' and 'Datetime' columns\n    df = pd.DataFrame({\n        'Timezone': TIMEZONES,\n        'Datetime': datetimes\n    })\n    \n    # Create a bar chart using matplotlib\n    ax = df.plot(kind='bar', x='Timezone', y='Datetime', legend=False)\n    ax.set_xlabel('Timezone')\n    ax.set_ylabel('Datetime')\n    ax.set_title('Datetime = f(Timezone)')\n    \n    # Return the DataFrame and the Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/51", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df, age: int, height: int):\n    # Filter the DataFrame based on the specified 'Age' and 'Height' conditions\n    filtered_df = df[(df['Age'] == age) & (df['Height'] == height)]\n    \n    # Check if the filtered DataFrame has less than 3 columns\n    if filtered_df.shape[1] < 3:\n        # Add a 'Cluster' column with 0 for each row\n        filtered_df['Cluster'] = 0\n    else:\n        # Reshape the data for KMeans clustering\n        X = filtered_df[['Age', 'Height']].values\n        # Perform KMeans clustering with 3 clusters\n        kmeans = KMeans(n_clusters=3, random_state=42)\n        filtered_df['Cluster'] = kmeans.fit_predict(X)\n    \n    # Plot a scatter plot of 'Age' and 'Height' colored by 'Cluster'\n    if 'Cluster' in filtered_df.columns:\n        plt.scatter(filtered_df['Age'], filtered_df['Height'], c=filtered_df['Cluster'], cmap='viridis')\n        plt.xlabel('Age')\n        plt.ylabel('Height')\n        plt.title('KMeans Clustering based on Age and Height')\n        plt.show()\n    else:\n        return filtered_df, None\n    \n    return filtered_df, plt.gca()"}
{"task_id": "BigCodeBench/52", "solution": "import pandas as pd\nimport regex as re\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    \n    # Split the text into words\n    words = re.split(r'\\s+', text)\n    \n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n    \n    # Count the frequency of each word\n    word_counts = pd.Series(words).value_counts()\n    \n    return word_counts"}
{"task_id": "BigCodeBench/53", "solution": "import pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\ndef task_func(text):\n    # Define the regular expression pattern\n    pattern = r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\"\n    \n    # Find all matches in the text\n    matches = re.findall(pattern, text, re.MULTILINE)\n    \n    # Create a DataFrame from the matches\n    df = pd.DataFrame(matches, columns=COLUMN_NAMES)\n    \n    # Convert 'Age' column to integer\n    df['Age'] = df['Age'].astype(int)\n    \n    # Plot the age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df['Age'], kde=False)\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return df"}
{"task_id": "BigCodeBench/54", "solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(text):\n    # Split the text into sentences using regex\n    sentences = re.split(r'(?<=[.!?]) +', text)\n    \n    # Remove empty sentences\n    sentences = [sentence for sentence in sentences if sentence]\n    \n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the sentences to create the document-term matrix\n    dtm = vectorizer.fit_transform(sentences)\n    \n    # Convert the document-term matrix to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return dtm_df"}
{"task_id": "BigCodeBench/55", "solution": "import re\nimport pandas as pd\nSTOPWORDS = [\"Those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"]\ndef task_func(text):\n    # Split the text into sentences\n    sentences = re.split(r'(?<=[.!?]) +', text)\n    \n    # Initialize an empty dictionary\n    sentence_word_counts = {}\n    \n    # Iterate over each sentence\n    for index, sentence in enumerate(sentences, start=1):\n        # Remove punctuation and split into words\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        \n        # Filter out stopwords\n        filtered_words = [word for word in words if word not in STOPWORDS]\n        \n        # Count the number of words in the filtered list\n        word_count = len(filtered_words)\n        \n        # Add to the dictionary\n        sentence_word_counts[f\"Sentence {index}\"] = word_count\n    \n    # Create a pandas Series from the dictionary\n    series = pd.Series(sentence_word_counts)\n    \n    return series"}
{"task_id": "BigCodeBench/56", "solution": "import pandas as pd\nimport regex as re\ndef task_func(text):\n    # Define the regex pattern to match 'Score: [number], Category: [word]'\n    pattern = r'Score: (\\d+), Category: (\\w+)'\n    \n    # Use regex to find all matches in the text\n    matches = re.findall(pattern, text)\n    \n    # Create a list of dictionaries to hold the data\n    data = []\n    for match in matches:\n        score = int(match[0])  # Convert score to integer\n        category = match[1]\n        data.append({'Score': score, 'Category': category})\n    \n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/57", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(csv_file_path: str, title: str):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n    \n    # Compute the correlation matrix\n    corr = df.corr()\n    \n    # Round each correlation to 2 decimals\n    corr = corr.round(2)\n    \n    # Create a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title(title)\n    \n    # Get the current axes\n    ax = plt.gca()\n    \n    # Return the DataFrame and the Axes object\n    return corr, ax"}
{"task_id": "BigCodeBench/58", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, num_samples):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot the normal distribution\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    ax.plot(x, stats.norm.pdf(x, mu, sigma), 'k', linewidth=2, label='Normal Distribution')\n    \n    # Overlay the histogram of the samples\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g', label='Histogram of Samples')\n    \n    # Set the title\n    ax.set_title('Normal Distribution')\n    \n    # Add legend\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return fig"}
{"task_id": "BigCodeBench/59", "solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(page_title):\n    try:\n        page = wikipedia.page(page_title)\n        text = page.content\n        wordcloud = WordCloud().generate(text)\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        return plt.gca()\n    except wikipedia.exceptions.PageError:\n        return None"}
{"task_id": "BigCodeBench/60", "solution": "import json\nimport pandas as pd\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(result)\n    \n    # Save the DataFrame to a CSV file without index\n    df.to_csv(csv_file_path, index=False)\n    \n    # Save the list of dictionaries to a JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(result, f)\n    \n    # Return None as per the requirement\n    return None"}
{"task_id": "BigCodeBench/61", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    # Extract 'from_user' values from the input list of dictionaries\n    x_values = [item['from_user'] for item in result]\n    \n    # Calculate square root of each value\n    y_values = np.sqrt(x_values)\n    \n    # Round each square root value to 2 decimals\n    y_values = np.round(y_values, 2)\n    \n    # Get current date and time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    \n    # Create a plot\n    plt.plot(x_values, y_values)\n    \n    # Add title and labels\n    plt.title(PLOT_TITLE)\n    plt.xlabel(X_LABEL)\n    plt.ylabel(Y_LABEL)\n    \n    # Annotate the graph with current date and time\n    plt.annotate(current_time, (0, 0), (0, -30), va='top', xycoords='axes fraction', textcoords='offset points')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the list of square values and the plot\n    return y_values, plt.gca()\nresult = [{'from_user': 1}, {'from_user': 4}, {'from_user': 9}]"}
{"task_id": "BigCodeBench/62", "solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Draws a histogram of the \"from_user\" values in the provided result.\n    The color of the histogram bars is selected at random from the provided colors list.\n    The function displays the histogram and does not return any value.\n    \"\"\"\n    # Extract \"from_user\" values from the result\n    from_users = [item['from_user'] for item in result]\n    \n    # Select a random color from the provided colors list\n    color = random.choice(colors)\n    \n    # Create a histogram using seaborn\n    sns.histplot(from_users, color=color)\n    \n    # Display the plot\n    plt.show()\nresult = [{'from_user': 'Alice'}, {'from_user': 'Bob'}, {'from_user': 'Alice'}, {'from_user': 'Charlie'}, {'from_user': 'Bob'}]"}
{"task_id": "BigCodeBench/63", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n    \n    # Create a bar chart\n    ax = df.plot(kind='bar', x='Car', y='Color', legend=False)\n    ax.set_title('Distribution of Vehicle Colors')\n    \n    return df, ax\ncar_dict = {'Ford': 'Red', 'Chevrolet': 'Blue', 'Toyota': 'Silver'}"}
{"task_id": "BigCodeBench/64", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Create DataFrame from the given data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Pivot the DataFrame to create a matrix for heatmap\n    pivot_df = df.pivot_table(values='col3', index='col1', columns='col2', aggfunc='sum')\n    \n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(pivot_df, annot=True, fmt=\"d\", cmap='YlGnBu')\n    \n    # Return the DataFrame and the heatmap\n    return df, heatmap\ndata = [\n    ['A', 'X', 10],\n    ['A', 'Y', 15],\n    ['B', 'X', 20],\n    ['B', 'Y', 25],\n    ['A', 'X', 30],\n    ['A', 'Y', 35],\n    ['B', 'X', 40],\n    ['B', 'Y', 45]\n]"}
{"task_id": "BigCodeBench/65", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    grouped = df.groupby(COLUMNS[:-1])[COLUMNS[-1]].value_counts().unstack(fill_value=0)\n    x_label = '-'.join(COLUMNS[:-1])\n    y_label = COLUMNS[-1]\n    ax = grouped.plot(kind='line')\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    return (df, ax)"}
{"task_id": "BigCodeBench/66", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Build a pandas DataFrame by using list of elements\n    df = pd.DataFrame(data, columns=COLUMNS)\n    # Create a new dataframe by grouping the values in the column 'col3' by ['col1', 'col2']\n    grouped_df = df.groupby(['col1', 'col2'])['col3'].value_counts().reset_index(name='count')\n    # Create a distribution plot of the 'col3' column of the previous dataframe using seaborn\n    plot = sns.barplot(x='col3', y='count', hue='col2', data=grouped_df)\n    # Set the xlabel (label for the x-axis) to the 'col3'\n    plot.set_xlabel('col3')\n    # Return the DataFrame and the plot object\n    return grouped_df, plot"}
{"task_id": "BigCodeBench/67", "solution": "import pandas as pd\nimport re\nimport os\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    # Create a list to hold the file names and their sizes\n    files = []\n    \n    # Compile the regular expression pattern\n    regex = re.compile(pattern)\n    \n    # Walk through the directory tree\n    for dirpath, dirnames, filenames in os.walk(dir_path):\n        for filename in filenames:\n            # Check if the file name matches the pattern\n            if regex.match(filename):\n                # Get the full file path\n                file_path = os.path.join(dirpath, filename)\n                # Get the file size\n                file_size = os.path.getsize(file_path)\n                # Append the file name and size to the list\n                files.append((filename, file_size))\n    \n    # Create a pandas DataFrame from the list of tuples\n    df = pd.DataFrame(files, columns=['File', 'Size'])\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/68", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(data)\n    \n    # Filter the lines in which the employee ID begins with the specified prefix\n    filtered_df = df[df['Employee ID'].str.startswith(emp_prefix)]\n    \n    # Draw a histogram of the 'Age' column of the filtered data\n    plt.figure(figsize=(10,6))\n    sns.histplot(filtered_df['Age'], kde=False)\n    plt.title('Age Distribution of Employees with ID starting with {}'.format(emp_prefix))\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    # Return the filtered DataFrame and the axes of the histogram\n    return filtered_df, plt.gca()"}
{"task_id": "BigCodeBench/69", "solution": "import random\nimport matplotlib.pyplot as plt\nSALARY_RANGE = (20000, 100000)\ndef task_func(dict1):\n    # Extract the number of employees in the department with code 'EMPXX'\n    num_employees = dict1.get('EMPXX', 0)\n    \n    # Generate random salaries for each employee\n    salaries = [random.randint(SALARY_RANGE[0], SALARY_RANGE[1]) for _ in range(num_employees)]\n    \n    # Create a histogram of the salaries\n    plt.hist(salaries, bins=20, color='skyblue', edgecolor='black')\n    plt.title('Salary Distribution in EMPXX Department')\n    plt.xlabel('Salary')\n    plt.ylabel('Number of Employees')\n    \n    # Return the Axes object representing the histogram\n    return plt.gca()"}
{"task_id": "BigCodeBench/70", "solution": "import pandas as pd\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nCOLUMNS = ['email', 'list']\ndef task_func(json_file):\n    # Load e-mail data from a JSON file\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n    \n    # Convert it into a Pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Calculate the sum and mean of the list associated with each e-mail\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    \n    # If there is no e-mail data, return an empty dataframe with the right columns and None as the plot\n    if df.empty:\n        return pd.DataFrame(columns=['email', 'list', 'sum', 'mean']), None\n    \n    # Plot the sum and mean values for each email\n    fig, ax = plt.subplots()\n    ax.plot(df['email'], df['sum'], label='Sum')\n    ax.plot(df['email'], df['mean'], label='Mean')\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Values')\n    ax.set_title('Sum and Mean of Email Lists')\n    ax.legend()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/71", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\ndef task_func(csv_file):\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(csv_file)\n    \n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n    \n    # Calculate sum, mean, and standard deviation for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['std'] = df['list'].apply(np.std)\n    \n    # Draw a histogram of the mean values\n    plt.figure()\n    sns.histplot(df['mean'], kde=False)\n    plt.title('Histogram of Mean Values')\n    plt.xlabel('Mean')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    # Return the DataFrame and the histogram plot\n    return df, plt.gcf()"}
{"task_id": "BigCodeBench/72", "solution": "import pandas as pd\nimport os\nimport numpy as np\nimport ast\nimport matplotlib.pyplot as plt\ndef task_func(directory):\n    # List all files in the directory\n    files = os.listdir(directory)\n    csv_files = [f for f in files if f.endswith('.csv')]\n    \n    if not csv_files:\n        return pd.DataFrame(), None\n    \n    # Find the CSV file with the longest filename\n    longest_csv = max(csv_files, key=len)\n    \n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(os.path.join(directory, longest_csv))\n    \n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n    \n    # Calculate sum, mean, and median for each email\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['median'] = df['list'].apply(np.median)\n    \n    # Draw a histogram of the median\n    if not df.empty:\n        plt.hist(df['median'], bins=10, color='skyblue', edgecolor='black')\n        plt.title('Histogram of Median')\n        plt.xlabel('Median')\n        plt.ylabel('Frequency')\n        plt.show()\n    else:\n        return df, None\n    \n    return df, plt.gca()\ndirectory = 'path_to_directory'"}
{"task_id": "BigCodeBench/73", "solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    # Load data from the \"EmailData\" table\n    query = \"SELECT email, list FROM EmailData\"\n    df = pd.read_sql_query(query, conn)\n    \n    # Convert the 'list' column from string to actual list\n    df['list'] = df['list'].apply(ast.literal_eval)\n    \n    # Calculate sum, mean, and variance for each list\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n    \n    # Close the database connection\n    conn.close()\n    \n    # Create a bar chart for sum, mean, and variance\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='email', y=['sum', 'mean', 'var'], ax=ax)\n    \n    # Return the DataFrame and the Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/74", "solution": "import socket\nimport requests\nfrom requests.exceptions import ConnectionError, Timeout\nfrom socket import gaierror\ndef task_func(host):\n    if not host:\n        raise ValueError(\"Host cannot be None or empty string\")\n    try:\n        ip = socket.gethostbyname(host)\n    except gaierror:\n        raise ValueError(\"Invalid hostname\")\n    try:\n        response = requests.get(f\"https://ipinfo.io/{ip}?token=YOUR_TOKEN\")\n        response.raise_for_status()\n    except ConnectionError:\n        raise ConnectionError(\"Problem connecting to geolocation service\")\n    except Timeout:\n        raise Timeout(\"Request timed out\")\n    data = response.json()\n    return {\"ip\": ip, \"geolocation\": data}"}
{"task_id": "BigCodeBench/75", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"df must be a pandas DataFrame\")\n    if not df.empty:\n        raise ValueError(\"df must be empty\")\n    if sales_lower_bound >= sales_upper_bound:\n        raise ValueError(\"sales_lower_bound must be less than sales_upper_bound\")\n    \n    if fruits is None:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    if days is None:\n        days = 30\n    if seed is not None:\n        np.random.seed(seed)\n    \n    dates = [datetime.today() - timedelta(days=i) for i in range(days)]\n    data = []\n    for date in dates:\n        for fruit in fruits:\n            sales = np.random.randint(sales_lower_bound, sales_upper_bound)\n            data.append({'Date': date, 'Fruit': fruit, 'Sales': sales})\n    df = pd.DataFrame(data)\n    \n    plot = sns.boxplot(x='Fruit', y='Sales', data=df)\n    return df, plot"}
{"task_id": "BigCodeBench/76", "solution": "import random\nimport string\nfrom django.http import HttpResponse\ndef task_func(request, session_expire_time):\n    # Generate a random session key of length 20 containing both letters and digits\n    while True:\n        session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n        if any(c.isalpha() for c in session_key) and any(c.isdigit() for c in session_key):\n            break\n\n    # Set the session key in a cookie on the HttpResponse object\n    response = HttpResponse(\"Session key generated successfully.\")\n    response.set_cookie('session_key', session_key, max_age=session_expire_time)\n\n    return response"}
{"task_id": "BigCodeBench/77", "solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\ndef task_func(data):\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n    \n    try:\n        password_bytes = base64.b64decode(password)\n    except binascii.Error:\n        return HttpResponseBadRequest('Bad Request.')\n    \n    try:\n        password_hash = hashlib.sha256(password_bytes).hexdigest()\n    except UnicodeDecodeError:\n        return HttpResponseBadRequest('Bad Request.')\n    \n    if username == 'admin' and password_hash == hashlib.sha256('password'.encode()).hexdigest():\n        return HttpResponse('Login successful.', status=400)\n    else:\n        return HttpResponse('Login failed.', status=401)"}
{"task_id": "BigCodeBench/78", "solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\ndef task_func(request, header, csv_data):\n    # Create a file-like object to hold the CSV data\n    csv_file = io.StringIO()\n    \n    # Create a CSV writer object\n    writer = csv.writer(csv_file)\n    \n    # Write the header row\n    writer.writerow(header)\n    \n    # Write the data rows\n    for row in csv_data:\n        writer.writerow(row)\n    \n    # Reset the file pointer to the beginning\n    csv_file.seek(0)\n    \n    # Create a FileResponse object with the CSV data\n    response = FileResponse(csv_file, content_type='text/csv')\n    response['Content-Disposition'] = 'attachment; filename=\"data.csv\"'\n    \n    return response"}
{"task_id": "BigCodeBench/79", "solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\ndef task_func(request, file_paths):\n    # Create a in-memory ZIP file\n    zip_buffer = io.BytesIO()\n    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file_path in file_paths:\n            # Assuming file_paths is a list of paths to files that exist\n            # In a real-world scenario, you would need to handle file existence and permissions\n            with open(file_path, 'rb') as f:\n                zipf.writestr(file_path, f.read())\n    \n    # Reset the buffer position to the beginning\n    zip_buffer.seek(0)\n    \n    # Create a FileResponse with the ZIP file\n    response = FileResponse(zip_buffer, content_type='application/zip')\n    response['Content-Disposition'] = 'attachment; filename=\"files.zip\"'\n    \n    return response"}
{"task_id": "BigCodeBench/80", "solution": "from flask import Flask, render_template, request\nimport json\nimport logging\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    \n    @app.route('/', methods=['POST'])\n    def index():\n        data = request.get_json()\n        logging.info(json.dumps(data))\n        return render_template('index.html', data=data)\n    \n    return app"}
{"task_id": "BigCodeBench/81", "solution": "from flask import Flask\nfrom flask_restful import Resource, Api\nimport requests\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class ExternalData(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return response.json()\n\n    api.add_resource(ExternalData, '/data')\n\n    return app"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = password\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    login_manager = LoginManager()\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    users = [User(1, 'user1', generate_password_hash('password1')),\n             User(2, 'user2', generate_password_hash('password2'))]\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        for user in users:\n            if user.id == int(user_id):\n                return user\n        return None\n\n    @app.route('/')\n    def home():\n        return render_template('home.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = None\n            for u in users:\n                if u.username == form.username.data:\n                    user = u\n                    break\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('home'))\n            else:\n                return 'Invalid username or password', 401\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('home'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return 'This is a protected page!'\n\n    return app"}
{"task_id": "BigCodeBench/83", "solution": "from flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USE_SSL'] = True\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_DEFAULT_SENDER'] = smtp_user\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message('Hello', sender=smtp_user, recipients=[smtp_user])\n        msg.body = 'Hello Flask message sent from Flask-Mail'\n        mail.send(msg)\n        return 'Sent'\n\n    return app"}
{"task_id": "BigCodeBench/84", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    \"\"\"\n    Generate a sales report with randomly simulated sales and profit data for a given list of products.\n    The data is aggregated by product and sorted by total profit in descending order.\n\n    Parameters:\n    - products (list of str): List of product names.\n    - n_samples (int): Number of sales samples to generate for each product.\n    - sales_lower (float): Lower bound for sales quantity.\n    - sales_upper (float): Upper bound for sales quantity.\n    - profit_margin_min (float): Minimum profit margin.\n    - profit_margin_max (float): Maximum profit margin.\n    - random_seed (int): Seed for random number generation.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing aggregated sales and profit data for each product, sorted by profit.\n\n    Raises:\n    - ValueError: If n_samples is not a positive integer, or if sales_lower is greater than sales_upper.\n    - TypeError: If products is not a list of strings, or if sales_lower, sales_upper, profit_margin_min, or profit_margin_max are not numeric.\n    \"\"\"\n\n    # Validate inputs\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer\")\n    if not all(isinstance(param, (int, float)) for param in [sales_lower, sales_upper, profit_margin_min, profit_margin_max]):\n        raise TypeError(\"sales_lower, sales_upper, profit_margin_min, and profit_margin_max must be numeric\")\n    if sales_lower > sales_upper:\n        raise ValueError(\"sales_lower must not be greater than sales_upper\")\n\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Simulate sales data\n    sales_data = []\n    for product in products:\n        for _ in range(n_samples):\n            sales_quantity = np.random.uniform(sales_lower, sales_upper)\n            profit_margin = np.random.uniform(profit_margin_min, profit_margin_max)\n            sales_data.append({\n                'product': product,\n                'sales_quantity': sales_quantity,\n                'profit_margin': profit_margin\n            })\n\n    # Create DataFrame\n    df = pd.DataFrame(sales_data)\n\n    # Calculate profit\n    df['profit'] = df['sales_quantity'] * df['profit_margin']\n\n    # Aggregate by product\n    aggregated_df = df.groupby('product').agg({'sales_quantity': 'sum', 'profit': 'sum'}).reset_index()\n\n    # Sort by profit in descending order\n    aggregated_df = aggregated_df.sort_values(by='profit', ascending=False)\n\n    return aggregated_df"}
{"task_id": "BigCodeBench/85", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, random_seed=42):\n    # Convert start_date and end_date to datetime objects\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    \n    # Check if end_date is before start_date\n    if end_date < start_date:\n        raise ValueError(\"end_date is before start_date, invalid date range\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate dates\n    dates = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n    \n    # Generate random weather data\n    temperatures = np.random.uniform(-10, 40, size=len(dates))\n    humidities = np.random.uniform(20, 100, size=len(dates))\n    wind_speeds = np.random.uniform(0, 20, size=len(dates))\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'Date': dates,\n        'Temperature': temperatures,\n        'Humidity': humidities,\n        'Wind Speed': wind_speeds\n    })\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    df.plot(x='Date', y=['Temperature', 'Humidity', 'Wind Speed'], ax=ax)\n    \n    return df, ax\nstart_date = '2023-01-01'\nend_date = '2023-01-10'"}
{"task_id": "BigCodeBench/86", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, size=len(students))\n    df = pd.DataFrame({'Student': students, 'Score': scores})\n    df = df.sort_values(by='Score')\n    fig, ax = plt.subplots()\n    ax.bar(df['Student'], df['Score'])\n    ax.set_title('Scores of Students')\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Score')\n    return df, ax"}
{"task_id": "BigCodeBench/87", "solution": "import pandas as pd\nfrom random import choices, seed\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    ratings = choices(ratings, weights)\n    df = pd.DataFrame({'Product': products, 'Rating': ratings})\n    df = df.sort_values(by='Rating', ascending=False)\n    return df"}
{"task_id": "BigCodeBench/88", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Convert start and end dates to datetime objects\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    \n    # Calculate the number of days between start and end dates\n    delta = end_date - start_date\n    num_days = delta.days + 1  # inclusive\n    \n    # Generate random sales data for each day\n    sales = np.random.randint(0, 501, num_days)\n    \n    # Create a list of dates\n    dates = [start_date + timedelta(days=i) for i in range(num_days)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    \n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Set 'Date' as the index\n    df.set_index('Date', inplace=True)\n    \n    # Plot the sales over time\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['Sales'], marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Over Time')\n    \n    return df, ax\nstart_date = '2023-01-01'\nend_date = '2023-01-10'"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Standardize the column\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data_scaled))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)\n    \n    # Remove outliers\n    data_no_outliers = data.drop(data.index[outliers])\n    \n    # Plot original data with outliers\n    plt.figure(figsize=(10,5))\n    plt.scatter(data.index, data_scaled, color='blue', label='Original Data')\n    plt.scatter(data.index[outliers], data_scaled[outliers], color='red', label='Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Scaled Value')\n    plt.legend()\n    plt.show()\n    \n    # Plot data without outliers\n    data_scaled_no_outliers = scaler.fit_transform(data_no_outliers[[column]])\n    plt.figure(figsize=(10,5))\n    plt.scatter(data_no_outliers.index, data_scaled_no_outliers, color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Scaled Value')\n    plt.legend()\n    plt.show()\n    \n    # Return original data, data without outliers, and outliers indices\n    return (data, data_no_outliers, outliers)"}
{"task_id": "BigCodeBench/90", "solution": "import numpy as np\nimport math\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n    \n    # Convert target to radians\n    lat2 = math.radians(target[0])\n    lon2 = math.radians(target[1])\n    \n    # Initialize a list to store distances and indices\n    distances = []\n    \n    # Iterate through each data point\n    for idx, point in enumerate(data):\n        lat1 = math.radians(point[0])\n        lon1 = math.radians(point[1])\n        \n        # Calculate the differences\n        dlon = lon2 - lon1\n        dlat = lat2 - lat1\n        \n        # Apply the Haversine formula\n        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n        \n        # Calculate the distance\n        distance = 6371 * c  # Radius of the Earth in kilometers\n        \n        # Append the distance and index to the list\n        distances.append((distance, idx))\n    \n    # Sort the list by distance\n    distances.sort()\n    \n    # Extract the top 'k' indices\n    nearest_indices = [idx for distance, idx in distances[:k]]\n    \n    # Return the 'k' nearest neighbors\n    return [data[idx] for idx in nearest_indices]"}
{"task_id": "BigCodeBench/91", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import linregress\nimport matplotlib.pyplot as plt\ndef task_func(data, column1, column2):\n    \"\"\"\n    Perform a linear regression on two columns of a dataset and record the result.\n    Additionally, generates a plot representing the original data and the fitted line.\n\n    Parameters:\n    - data: pandas DataFrame containing the dataset.\n    - column1: string, name of the first column for regression.\n    - column2: string, name of the second column for regression.\n\n    Returns:\n    - tuple: The slope, intercept, r-value, p-value, and standard error of the regression.\n    - Axes: The matplotlib Axes object containing the plot.\n\n    Raises:\n    - ValueError: If the specified columns do not exist in the DataFrame.\n    \"\"\"\n    # Check if the columns exist in the DataFrame\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"One or both specified columns do not exist in the DataFrame.\")\n    \n    # Extract the data for regression\n    x = data[column1]\n    y = data[column2]\n    \n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y, label='Original Data')\n    ax.plot(x, intercept + slope*x, color='red', label='Fitted Line')\n    ax.legend()\n    \n    return (slope, intercept, r_value, p_value, std_err), ax"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters < 2:\n        raise ValueError(\"n_clusters must be an integer greater than 1\")\n    \n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.predict(data)\n    centroids = kmeans.cluster_centers_\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n    ax.set_title('K-means Clustering')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    \n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n    \n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df[\"PC1\"], transformed_df[\"PC2\"])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n    \n    return transformed_df, ax"}
{"task_id": "BigCodeBench/94", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, num_samples):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n    \n    # Fit a normal distribution to the samples\n    params = norm.fit(samples)\n    \n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the samples\n    n, bins, patches = ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the PDF of the fitted normal distribution\n    pdf = norm.pdf(bins, loc=params[0], scale=params[1])\n    ax.plot(bins, pdf, 'k-', linewidth=2)\n    \n    # Set the title with the fit results\n    ax.set_title(f\"Fit results: mean = {params[0]:.2f}, std = {params[1]:.2f}\")\n    \n    # Return the figure and the samples\n    return fig, samples"}
{"task_id": "BigCodeBench/95", "solution": "import pandas as pd\nfrom random import randint, uniform, seed\ndef task_func(categories=None, months=None, random_seed=42):\n    if categories is None or months is None:\n        raise ValueError(\"Both 'categories' and 'months' must be provided as lists and cannot be empty.\")\n    if not isinstance(categories, list) or not isinstance(months, list):\n        raise ValueError(\"Both 'categories' and 'months' must be provided as lists.\")\n    if not categories or not months:\n        raise ValueError(\"Both 'categories' and 'months' must be non-empty lists.\")\n    \n    seed(random_seed)\n    data = []\n    for month in months:\n        for category in categories:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append({'Month': month, 'Category': category, 'Sales': sales})\n    \n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/96", "solution": "import csv\nfrom collections import Counter\nimport operator\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file, delimiter=csv_delimiter)\n        words = []\n        for row in reader:\n            words.extend(row)\n    word_counts = Counter(words)\n    sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_word_counts"}
{"task_id": "BigCodeBench/97", "solution": "import math\nimport itertools\nfrom functools import reduce\ndef task_func(numbers):\n    total_sum = 0.0\n    n = len(numbers)\n    for r in range(1, n+1):\n        combinations = itertools.combinations(numbers, r)\n        for combo in combinations:\n            product = reduce(lambda x, y: x * y, combo)\n            log_product = math.log(product)\n            total_sum += log_product\n    return total_sum"}
{"task_id": "BigCodeBench/98", "solution": "import random\nimport string\nfrom collections import Counter\ndef task_func(num_strings, string_length):\n    # Generate a list of random strings\n    random_strings = [''.join(random.choice(string.ascii_lowercase) for _ in range(string_length)) for _ in range(num_strings)]\n    \n    # Concatenate all strings into one\n    combined_string = ''.join(random_strings)\n    \n    # Count the frequency of each character\n    frequency = Counter(combined_string)\n    \n    # Sort the characters by frequency in descending order\n    sorted_frequency = sorted(frequency.items(), key=lambda x: x[1], reverse=True)\n    \n    return sorted_frequency"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target_names[iris.target]\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    pairplot = sns.pairplot(df, hue='species')\n\n    # Set the title\n    pairplot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n\n    # Label the axes\n    for ax in pairplot.axes.flatten():\n        ax.set_xlabel(ax.get_xlabel(), fontsize=10)\n        ax.set_ylabel(ax.get_ylabel(), fontsize=10)\n\n    # Show the plot\n    plt.show()\n\n    # Return the figure object\n    return pairplot.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        today = datetime.today()\n        dates = [today - pd.DateOffset(days=i) for i in range(30)]\n        \n        # Generate random values\n        values = [random.randint(0, 100) for _ in range(30)]\n        \n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        \n        # Plot the data\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], color='blue')\n        \n        # Set the title and labels\n        ax.set_title('Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        \n        # Set the font to Arial\n        plt.rcParams['font.family'] = 'Arial'\n        \n        # Return the Axes object\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    \"\"\"\n    Draw the correlation heatmap of the Boston Housing dataset using Seaborn, with an option to save it to a specified file.\n    \n    Parameters:\n    - data_url (str): URL to the Boston Housing dataset.\n    - seed (int): Seed for reproducibility.\n    - save_path (str, optional): Path to save the plot. If None, the plot is not saved.\n    \n    Returns:\n    - matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n    \n    Raises:\n    - ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    # Load the dataset\n    try:\n        df = pd.read_csv(data_url, header=None)\n    except Exception as e:\n        raise ValueError(f\"Error loading the dataset: {e}\")\n    \n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate a correlation matrix\n    corr = df.corr()\n    \n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr, annot=True, cmap='coolwarm')\n    \n    # Save the plot if save_path is provided\n    if save_path:\n        try:\n            plt.savefig(save_path)\n        except Exception as e:\n            raise ValueError(f\"Error saving the plot: {e}\")\n    \n    # Return the Axes object\n    return heatmap.axes"}
{"task_id": "BigCodeBench/102", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\ndef task_func():\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n    # Convert the dataset to a DataFrame\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n    # Create a pairplot\n    pairplot = sns.pairplot(df)\n    # Return the figure and the DataFrame\n    return pairplot.fig, df"}
{"task_id": "BigCodeBench/103", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(temperatures):\n    if not isinstance(temperatures, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if temperatures.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n    if 'date' not in temperatures.columns or 'temperature' not in temperatures.columns:\n        raise ValueError(\"Input DataFrame must contain 'date' and 'temperature' columns.\")\n    \n    plt.figure()\n    plt.plot(temperatures['date'], temperatures['temperature'], color='blue')\n    plt.xlabel('Date', fontname='Arial')\n    plt.ylabel('Temperature (\u00b0C)', fontname='Arial')\n    plt.title('Daily Temperatures in New York', fontname='Arial')\n    plt.xticks(fontname='Arial')\n    plt.yticks(fontname='Arial')\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/104", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    \"\"\"\n    Analyzes the groups in a DataFrame by plotting a scatter plot of the ordinals against the values for each group.\n\n    Parameters:\n    - df (pd.DataFrame): The DataFrame containing the data.\n    - groups (list): List of group names to plot.\n\n    Returns:\n    - matplotlib.axes.Axes: The Axes object with the scatter plot.\n\n    Raises:\n    - ValueError: If 'df' is not a DataFrame or lacks required columns.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The first argument must be a pandas DataFrame.\")\n    \n    # Check if required columns are present\n    required_columns = ['Date', 'Value', 'Group']\n    missing_cols = [col for col in required_columns if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"The DataFrame is missing the following columns: {', '.join(missing_cols)}\")\n    \n    # Convert 'Date' to datetime if not already\n    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n        df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Create a color cycle for different groups\n    colors = cycle('bgrcmyk')\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot each group\n    for group in groups:\n        if group in df['Group'].unique():\n            group_data = df[df['Group'] == group]\n            ax.scatter(group_data['Date'], group_data['Value'], color=next(colors), label=group)\n    \n    # Set the title and labels\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    \n    # Add a legend\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/105", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The dataframe is empty.\")\n    if 'date' not in df.columns:\n        raise ValueError(\"The 'date' column is missing.\")\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column is not in datetime format.\")\n    \n    # Convert 'date' column to ordinal format\n    df['date'] = df['date'].dt.toordinal()\n    \n    # Create correlation matrix\n    corr = df.corr()\n    \n    # Generate correlation matrix heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr, annot=True, cmap='coolwarm')\n    heatmap.set_title('Correlation Matrix', fontdict={'fontsize':18}, pad=16)\n    \n    # Generate pair plot\n    pairplot = sns.pairplot(df)\n    \n    return plt.gcf(), pairplot"}
{"task_id": "BigCodeBench/106", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    # Check if 'date' and 'value' columns exist\n    required_columns = ['date', 'value']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"DataFrame is missing required columns: {', '.join(missing_columns)}\")\n    \n    # Check if 'date' column is in datetime format\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"The 'date' column must be in datetime format.\")\n    \n    # Convert 'date' to ordinal\n    df['date'] = df['date'].dt.toordinal()\n    \n    # Prepare data for linear regression\n    X = df[['date']]\n    y = df['value']\n    \n    # Fit the model\n    model = LinearRegression()\n    model.fit(X, y)\n    predictions = model.predict(X)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df['date'], y, label='Original')\n    ax.plot(df['date'], predictions, label='Predicted', color='red')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return (model, predictions, ax)"}
{"task_id": "BigCodeBench/107", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or lacks required columns.\")\n    \n    # Convert 'date' column to ordinal\n    df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y%m%d').astype(int)\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    df['cluster'] = kmeans.fit_predict(df[['date', 'value']])\n    \n    # Plot the clusters\n    plt.figure(figsize=(10, 6))\n    for cluster in df['cluster'].unique():\n        cluster_data = df[df['cluster'] == cluster]\n        plt.scatter(cluster_data['date'], cluster_data['value'], label=f'Cluster {cluster}')\n    \n    plt.xlabel('Date (ordinal)')\n    plt.ylabel('Value')\n    plt.title('KMeans Clustering of Value vs Date')\n    plt.legend()\n    plt.show()\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Parameters:\n    - df (pandas.DataFrame): DataFrame containing the time series data.\n    - freq (str): Frequency string for the time series.\n    - decomposition_model (str): Model for decomposition, either 'additive' or 'multiplicative'.\n\n    Returns:\n    - tuple: A tuple containing the decomposition result (DecomposeResult object) and the matplotlib Axes object.\n\n    Raises:\n    - ValueError: If 'df' is not a DataFrame, lacks required columns, or contains invalid data types.\n    - ValueError: If 'freq' is not a valid frequency string.\n    - ValueError: If 'decomposition_model' is not 'additive' or 'multiplicative'.\n    \"\"\"\n\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df must be a pandas DataFrame\")\n\n    # Check if 'value' column exists in df\n    if 'value' not in df.columns:\n        raise ValueError(\"df must contain a 'value' column\")\n\n    # Check if 'value' column contains numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data\")\n\n    # Check if 'freq' is a valid frequency string\n    valid_freqs = ['D', 'W', 'M', 'Q', 'A']\n    if freq not in valid_freqs:\n        raise ValueError(f\"Invalid frequency: {freq}. Valid frequencies are: {', '.join(valid_freqs)}\")\n\n    # Check if 'decomposition_model' is either 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(f\"Invalid decomposition model: {decomposition_model}. Must be 'additive' or 'multiplicative'\")\n\n    # Set the index of the DataFrame to datetime with the specified frequency\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    df.index.freq = freq\n\n    # Perform decomposition\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model)\n\n    # Plot the decomposition\n    fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(10, 12))\n    decomposition.plot(ax=axes)\n    plt.tight_layout()\n\n    return decomposition, axes"}
{"task_id": "BigCodeBench/109", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The first argument must be a pandas DataFrame.\")\n    \n    if 'Item' not in df.columns or 'Location' not in df.columns:\n        raise ValueError(\"The DataFrame must contain 'Item' and 'Location' columns.\")\n    \n    if items is None:\n        items = ['Item1', 'Item2', 'Item3']  # Predefined list of items\n    if locations is None:\n        locations = ['Location1', 'Location2', 'Location3']  # Predefined list of locations\n    \n    # Filter the DataFrame based on the provided items and locations\n    if items is not None:\n        df = df[df['Item'].isin(items)]\n    if locations is not None:\n        df = df[df['Location'].isin(locations)]\n    \n    # Pivot the DataFrame to get the count of each item per location\n    pivot_df = df.pivot_table(index='Location', columns='Item', aggfunc='size', fill_value=0)\n    \n    # Plot the bar chart\n    ax = pivot_df.plot(kind='bar')\n    plt.title('Item Distribution by Location')\n    plt.xlabel('Location')\n    plt.ylabel('Count')\n    plt.legend(title='Item')\n    \n    return ax"}
{"task_id": "BigCodeBench/110", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if 'Date' not in df.columns or 'Sales' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Sales' columns.\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Date'], df['Sales'], marker='o', linestyle='-', color='b')\n    plt.title('Daily Turnover')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.grid(True)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    return plt.gca()"}
{"task_id": "BigCodeBench/111", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    required_columns = ['Date', 'Time', 'Temperature']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"DataFrame is missing required columns: {', '.join(missing_columns)}\")\n    \n    # Assuming 'Date' and 'Time' are in the format that can be converted to datetime\n    df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n    df.set_index('Datetime', inplace=True)\n    \n    # Resample data to daily frequency and calculate mean temperature\n    daily_temp = df['Temperature'].resample('D').mean()\n    \n    # Create a DataFrame for the heatmap\n    heatmap_df = daily_temp.unstack()\n    \n    # Create the heatmap\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(heatmap_df, cmap='viridis')\n    plt.title('Daily Temperature Heatmap')\n    plt.xlabel('Date')\n    plt.ylabel('Time')\n    \n    # Return the heatmap object\n    return plt.gcf()"}
{"task_id": "BigCodeBench/112", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if 'Status' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Status' column.\")\n    \n    status_counts = df['Status'].value_counts()\n    labels = status_counts.index\n    sizes = status_counts.values\n    \n    fig, ax = plt.subplots()\n    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    \n    return ax"}
{"task_id": "BigCodeBench/113", "solution": "import json\nfrom collections import Counter\nimport random\ndef task_func(my_dict, keys):\n    if len(keys) != 10:\n        raise ValueError(\"The 'keys' parameter must contain exactly 10 unique elements.\")\n    \n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n    \n    json_filename = 'updated_dictionary.json'\n    with open(json_filename, 'w') as f:\n        json.dump(my_dict, f)\n    \n    key_frequencies = Counter(my_dict.keys())\n    txt_filename = 'key_frequencies.txt'\n    with open(txt_filename, 'w') as f:\n        for key, count in key_frequencies.items():\n            f.write(f\"{key}: {count}\\n\")\n    \n    return (my_dict, json_filename, txt_filename)\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nkeys = ['d', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm']"}
{"task_id": "BigCodeBench/114", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(my_dict):\n    if 'array' not in my_dict:\n        raise KeyError(\"The dictionary must contain a key 'array' with a numpy array as its value.\")\n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value associated with the 'array' key must be a numpy array.\")\n    \n    scaler = MinMaxScaler()\n    normalized_array = scaler.fit_transform(my_dict['array'].reshape(-1,1))\n    my_dict['normalized_array'] = normalized_array.flatten()\n    \n    return my_dict"}
{"task_id": "BigCodeBench/115", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list is empty\")\n    arr = np.array(numbers)\n    arr_mode = mode(arr)\n    arr_entropy = entropy(arr, base=2)\n    return {'mode': arr_mode, 'entropy': arr_entropy}"}
{"task_id": "BigCodeBench/116", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, sample_size):\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(mu, sigma, sample_size)\n    \n    # Plot the histogram of the samples\n    plt.hist(samples, bins=30, alpha=0.75, color='blue')\n    plt.xlabel('Sample values')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Generated Samples')\n    plt.show()\n    \n    return samples\nmu = 0\nsigma = 1\nsample_size = 1000"}
{"task_id": "BigCodeBench/117", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be positive\")\n    \n    if name_list is None:\n        name_list = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n    if gender_list is None:\n        gender_list = ['Male', 'Female']\n    \n    set_seed(seed)\n    names = [choice(name_list) for _ in range(num_of_students)]\n    ages = [np.random.randint(age_range[0], age_range[1] + 1) for _ in range(num_of_students)]\n    genders = [choice(gender_list) for _ in range(num_of_students)]\n    scores = [np.random.randint(score_range[0], score_range[1] + 1) for _ in range(num_of_students)]\n    \n    data = {\n        'Name': names,\n        'Age': ages,\n        'Gender': genders,\n        'Score': scores\n    }\n    \n    return pd.DataFrame(data)"}
{"task_id": "BigCodeBench/118", "solution": "import os\nimport shutil\ndef task_func(directory, backup_directory):\n    # Check if the backup directory exists, if not, create it\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n    \n    # List to hold the paths of copied files\n    copied_files = []\n    \n    # Iterate over all files in the specified directory\n    for filename in os.listdir(directory):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Construct the full path to the file\n            file_path = os.path.join(directory, filename)\n            # Construct the full path to the backup file\n            backup_file_path = os.path.join(backup_directory, filename)\n            # Copy the file to the backup directory\n            shutil.copy2(file_path, backup_file_path)\n            # Append the path of the copied file to the list\n            copied_files.append(backup_file_path)\n    \n    # Return the list of paths to the copied files\n    return copied_files"}
{"task_id": "BigCodeBench/119", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n    plt.plot(x, y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date\")\n    \n    random_seed(seed)\n    delta = end_date - start_date\n    days = delta.days + 1  # inclusive of both start and end dates\n    random_dates = [start_date + timedelta(days=randint(0, days-1)) for _ in range(days)]\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/121", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    np.random.seed(seed)\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = np.random.randint(100, 1000, size=(len(categories), 12))\n    sales_df = pd.DataFrame(sales_data, columns=[f'Month {i+1}' for i in range(12)])\n    sales_df['Category'] = categories\n    sales_df = sales_df.melt(id_vars='Category', var_name='Month', value_name='Sales')\n    \n    my_list.append(12)\n    \n    fig, ax = plt.subplots()\n    sns.barplot(x='Month', y='Sales', hue='Category', data=sales_df, ax=ax)\n    plt.title('Sales Data by Category')\n    plt.show()\n    \n    return sales_df, ax"}
{"task_id": "BigCodeBench/122", "solution": "import numpy as np\nimport random\ndef task_func(my_list):\n    # Append a random integer between 0 and 100 to the list\n    my_list.append(random.randint(0, 100))\n    \n    # Calculate the sum of the list\n    total = sum(my_list)\n    \n    # Generate a numpy array of random floating-point numbers with the size equal to the sum of the list\n    result = np.random.rand(total)\n    \n    return result"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(\"Directory not found\")\n    \n    # Add '12' to the list\n    my_list.append('12')\n    \n    # Determine the number of files to concatenate\n    num_files = sum(map(int, my_list))\n    \n    # Get all CSV files in the directory\n    files = glob.glob(os.path.join(file_dir, '*' + file_ext))\n    \n    if not files:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n    \n    # Read and concatenate the CSV files\n    dfs = [pd.read_csv(file) for file in files[:num_files]]\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric values (int or float)\")\n    \n    # Enhance my_list by appending the number 12\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    sum_elements = sum(my_list)\n    \n    # Generate a list of random integers based on the sum of elements in my_list, limited by 'size'\n    random_numbers = [randint(1, 100) for _ in range(min(sum_elements, size))]\n    \n    # Measure the time taken for this process\n    start_time = time.time()\n    # Generate the list of random numbers\n    random_numbers = [randint(1, 100) for _ in range(min(sum_elements, size))]\n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    # Plot a histogram of the generated random numbers\n    plt.hist(random_numbers, bins=range(1, 101), align='left', rwidth=0.8)\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    plt.show()\n    \n    # Return the time taken and the matplotlib Axes object for the histogram\n    return (time_taken, plt.gca())"}
{"task_id": "BigCodeBench/125", "solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\ndef task_func(letters, n):\n    # Generate all possible combinations of the given set of letters of length 'n'\n    combinations = list(itertools.combinations(letters, n))\n    \n    # Count the occurrences of each letter in these combinations\n    letter_counts = defaultdict(int)\n    for combo in combinations:\n        for letter in combo:\n            letter_counts[letter] += 1\n    \n    # Generate a random number between 0 and 100 for the file name\n    random_number = random.randint(0, 100)\n    file_name = f\"prefix_{random_number}.json\"\n    \n    # Save the results in a JSON file\n    with open(file_name, 'w') as f:\n        json.dump(dict(letter_counts), f)\n    \n    # Return the name of the generated JSON file\n    return file_name"}
{"task_id": "BigCodeBench/126", "solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport numpy as np\ndef task_func(animals=None, seed=42):\n    if animals is None:\n        animals = ['Lion', 'Elephant', 'Giraffe', 'Zebra', 'Monkey']\n    random_seed(seed)\n    data = []\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        std_dev = statistics.stdev(counts)\n        data.append([animal, mean, median, std_dev])\n    df = pd.DataFrame(data, columns=['Animal', 'Mean', 'Median', 'Standard Deviation'])\n    df.plot(kind='bar', x='Animal', y='Mean', title='Mean Animal Counts')\n    return df"}
{"task_id": "BigCodeBench/127", "solution": "import os\nimport shutil\nimport glob\nimport hashlib\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    # Ensure the destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    # Initialize a counter for moved files\n    moved_files_count = 0\n    \n    # Use glob to get all files in the root directory\n    files = glob.glob(os.path.join(ROOT_DIR, '*'))\n    \n    for file in files:\n        # Check if the item is a file\n        if os.path.isfile(file):\n            # Calculate the MD5 hash of the file\n            with open(file, 'rb') as f:\n                file_hash = hashlib.md5(f.read()).hexdigest()\n            \n            # If the hash matches the specific hash, move the file\n            if file_hash == SPECIFIC_HASH:\n                shutil.move(file, DEST_DIR)\n                moved_files_count += 1\n    \n    return moved_files_count"}
{"task_id": "BigCodeBench/128", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\ndef task_func(POINTS=100):\n    x = np.zeros(POINTS)\n    y = np.zeros(POINTS)\n    for i in range(1, POINTS):\n        direction = randint(0, 3)\n        if direction == 0:\n            x[i] = x[i - 1] + 1\n            y[i] = y[i - 1]\n        elif direction == 1:\n            x[i] = x[i - 1] - 1\n            y[i] = y[i - 1]\n        elif direction == 2:\n            x[i] = x[i - 1]\n            y[i] = y[i - 1] + 1\n        else:\n            x[i] = x[i - 1]\n            y[i] = y[i - 1] - 1\n    plt.plot(x, y)\n    plt.show()\n    return plt.gcf()"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.ConnectionError as e:\n        raise ConnectionError(f\"Connection error: {e}\")\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP error: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    tables = soup.find_all('table')\n\n    if not tables:\n        raise ValueError(\"No table found on the page\")\n\n    table = tables[0]\n    headers = []\n    rows = []\n\n    header_row = table.find('tr')\n    if header_row:\n        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n    else:\n        # If no headers, use default column names\n        headers = [f\"Column {i+1}\" for i in range(len(table.find('tr').find_all('td')))]\n\n    for row in table.find_all('tr'):\n        cells = row.find_all(['td', 'th'])\n        if cells:\n            rows.append([cell.get_text(strip=True) for cell in cells])\n\n    df = pd.DataFrame(rows, columns=headers)\n    return df"}
{"task_id": "BigCodeBench/130", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    try:\n        data = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n\n    # Generate a random salt of the specified size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the data\n    salted_data = data + salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_object = hashlib.sha256(salted_data)\n    hash_bytes = hash_object.digest()\n\n    # Base64-encode the salt\n    salt_base64 = base64.b64encode(salt).decode('utf-8')\n\n    # Return the salt and hash as a tuple\n    return (salt_base64, hash_bytes)"}
{"task_id": "BigCodeBench/131", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    try:\n        data = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n\n    # Generate a random salt of the specified size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the data\n    salted_data = data + salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_object = hashlib.sha256(salted_data)\n    hash_bytes = hash_object.digest()\n\n    # Base64-encode the salt\n    salt_base64 = base64.b64encode(salt).decode('utf-8')\n\n    # Return the salt and hash as a tuple\n    return (salt_base64, hash_bytes)"}
{"task_id": "BigCodeBench/132", "solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(hex_str):\n    try:\n        # Remove the '\\x' prefix if present\n        if hex_str.startswith('\\\\x'):\n            hex_str = hex_str[2:]\n        # Convert hex string to bytes\n        byte_data = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"Invalid hex string\")\n    \n    # Calculate frequency of each byte value\n    frequency = np.bincount(byte_data)\n    \n    # Create a DataFrame of byte frequencies\n    byte_values = np.arange(256)\n    freq_df = pd.DataFrame({'Byte Value': byte_values, 'Frequency': frequency})\n    \n    # Create a plot of byte frequencies\n    fig, ax = plt.subplots()\n    ax.bar(byte_values, frequency)\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Byte Frequency Plot')\n    \n    return freq_df, ax"}
{"task_id": "BigCodeBench/133", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    scaler = MinMaxScaler()\n    df.iloc[:, -1] = scaler.fit_transform(df.iloc[:, -1].values.reshape(-1,1))\n    \n    plt.figure()\n    plt.plot(df.index, df.iloc[:, -1], label='Normalized Data')\n    plt.title(f'Normalized Data of {df.columns[-1]}')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Value')\n    plt.legend()\n    plt.show()\n    \n    return df, plt.gca()"}
{"task_id": "BigCodeBench/134", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    last_col = df.columns[-1]\n    ax = df[last_col].plot.hist(bins=bins, title='Histogram of ' + last_col, x='Value', y='Frequency')\n    return ax"}
{"task_id": "BigCodeBench/135", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input is not a DataFrame or has no columns.\")\n    \n    # Impute missing values in the last column using mean imputation\n    imputer = SimpleImputer(strategy='mean')\n    df.iloc[:, -1] = imputer.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n    \n    # Create a box plot to visualize the distribution of data in the last column\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x=df.iloc[:, -1])\n    plt.title('Box Plot of the Last Column')\n    plt.xlabel('Values')\n    plt.show()\n    \n    return df, plt.gca()\ndata = {\n    'A': [1, 2, np.nan, 4],\n    'B': [5, np.nan, np.nan, 8],\n    'C': [9, 10, 11, 12]\n}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/136", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    pca = PCA(n_components=2)\n    pca.fit(df)\n    pca_df = pd.DataFrame(pca.transform(df), columns=['Principal Component 1', 'Principal Component 2'])\n    plt.figure(figsize=(8,6))\n    plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], c='b')\n    plt.title('2 Component PCA')\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.show()\n    return pca_df, plt.gca()"}
{"task_id": "BigCodeBench/137", "solution": "import pandas as pd\nfrom scipy.stats import skew\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input is not a DataFrame or has no columns.\")\n    last_col = df.iloc[:, -1]\n    return skew(last_col)"}
{"task_id": "BigCodeBench/138", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    \"\"\"\n    Create and return a bar chart of the frequency of letters in a DataFrame where the column 'Letters' contains English uppercase letters.\n\n    Parameters:\n    - df: pandas DataFrame containing a column 'Letters' with English uppercase letters.\n    - letters: list of English uppercase letters (default is all uppercase letters).\n\n    Returns:\n    - A Matplotlib Axes object representing the bar graph of letter frequency.\n\n    Raises:\n    - ValueError: If 'df' is not a DataFrame or lacks the 'Letters' column.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if 'Letters' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Letters' column.\")\n    \n    # Filter the DataFrame to only include the specified letters\n    df = df[df['Letters'].isin(letters)]\n    \n    # Count the frequency of each letter\n    frequency = df['Letters'].value_counts().sort_index()\n    \n    # Create a bar chart\n    ax = frequency.plot(kind='bar')\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n    \n    return ax"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if numeric_cols.empty:\n        raise ValueError(\"No numeric columns found in the DataFrame.\")\n    \n    axes = []\n    for col in numeric_cols:\n        ax = plt.subplot(len(numeric_cols), 1, numeric_cols.get_loc(col) + 1)\n        df[col].hist(ax=ax, bins=30)\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes"}
{"task_id": "BigCodeBench/140", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, cols):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df must be a pandas DataFrame\")\n    if not isinstance(cols, list):\n        raise ValueError(\"cols must be a list\")\n    for col in cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' not found in dataframe\")\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n    return df"}
{"task_id": "BigCodeBench/141", "solution": "import numpy as np\nimport pandas as pd\nimport statistics\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"rows must be a positive integer greater than 0\")\n    \n    np.random.seed(seed)\n    data = {col: np.random.randint(1, 101, rows) for col in columns}\n    df = pd.DataFrame(data)\n    \n    results = {}\n    for col in columns:\n        col_data = df[col]\n        mean_val = col_data.mean()\n        median_val = col_data.median()\n        results[col] = {'mean': mean_val, 'median': median_val}\n    \n    return df, results"}
{"task_id": "BigCodeBench/142", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(0, 2 * np.pi, 100)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    \n    axes[0].plot(x, np.sin(x))\n    axes[0].set_title('Sine function')\n    axes[0].set_xlabel('x')\n    axes[0].set_ylabel('sin(x)')\n    \n    axes[1].plot(x, np.cos(x))\n    axes[1].set_title('Cosine function')\n    axes[1].set_xlabel('x')\n    axes[1].set_ylabel('cos(x)')\n    \n    return fig, axes"}
{"task_id": "BigCodeBench/143", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate x values from -10 to 10\n    x = np.linspace(-10, 10, 400)\n    # Calculate y values based on the equation y = 2x + 1\n    y = 2 * x + 1\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    # Plot the line y = 2x + 1\n    ax.plot(x, y, color='red', label='y=2x+1')\n    # Calculate the y value for x=2\n    y_solution = 2 * 2 + 1\n    # Mark the solution point at x=2, y=5 with a green circle\n    ax.plot(2, y_solution, color='green', marker='o', label='x=2, y=5')\n    # Set the title of the plot\n    ax.set_title('Solution of the equation y=2x+1 at x=2')\n    # Label the x-axis\n    ax.set_xlabel('x')\n    # Label the y-axis\n    ax.set_ylabel('y')\n    # Add a legend\n    ax.legend()\n    # Show the plot\n    plt.show()\n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/144", "solution": "import ipaddress\nimport requests\ndef task_func(ip_range, timeout):\n    active_servers = []\n    try:\n        network = ipaddress.ip_network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided.\")\n    \n    for ip in network.hosts():\n        try:\n            response = requests.get(f\"http://{ip}\", timeout=timeout)\n            if response.status_code == 200:\n                active_servers.append(str(ip))\n        except requests.exceptions.RequestException:\n            continue\n    return active_servers"}
{"task_id": "BigCodeBench/145", "solution": "import csv\nfrom ipaddress import IPv4Network\ndef task_func(ip_range, csv_path):\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for ip in IPv4Network(ip_range):\n            writer.writerow([str(ip)])\n    return csv_path"}
{"task_id": "BigCodeBench/146", "solution": "import subprocess\nfrom ipaddress import IPv4Network\ndef task_func(ip_range):\n    active_ips = {}\n    network = IPv4Network(ip_range)\n    for ip in network.hosts():\n        ip = str(ip)\n        try:\n            response = subprocess.run(['ping', '-c', '1', ip], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n            if response.returncode == 0:\n                active_ips[ip] = True\n            else:\n                active_ips[ip] = False\n        except subprocess.CalledProcessError as e:\n            raise e\n    return active_ips"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans a specified IP address range and checks if a specified port is open on each IP.\n    The function returns a dictionary with IP addresses as keys and a boolean indicating the port's status (True if open, False otherwise).\n\n    Args:\n    ip_range (str): The IP address range to scan, in CIDR notation (e.g., '192.168.1.0/24').\n    port (int): The port to check on each IP address.\n\n    Returns:\n    dict: A dictionary mapping IP addresses to their port status (True if open, False otherwise).\n    \"\"\"\n    ip_network = IPv4Network(ip_range)\n    ip_list = list(ip_network.hosts())\n    results = {}\n\n    def scan_ip(ip):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        results[str(ip)] = result == 0\n        sock.close()\n\n    threads = []\n    for ip in ip_list:\n        thread = Thread(target=scan_ip, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/148", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Encrypt the categorical data in a specific column of a DataFrame using LabelEncoder.\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame.\n    - column_name (str): The name of the column to be encoded.\n\n    Returns:\n    - pd.DataFrame: The DataFrame with the encoded column.\n    \"\"\"\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df"}
{"task_id": "BigCodeBench/149", "solution": "import pandas as pd\nimport numpy as np\nDEFAULT_COLUMNS = ['Element', 'Count']\ndef task_func(elements, include_index=False):\n    # Create a list to hold dictionaries, each containing 'Element' and 'Count' keys\n    data = []\n    for element in elements:\n        count = len(element)\n        data.append({'Element': element, 'Count': count})\n    \n    # Create DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n    \n    # If include_index is True, add an 'Index' column\n    if include_index:\n        df.insert(0, 'Index', range(1, len(df) + 1))\n    \n    # Reorder columns if include_index is True\n    if include_index:\n        df = df[DEFAULT_COLUMNS]\n    \n    return df"}
{"task_id": "BigCodeBench/150", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(product_dict, product_keys):\n    # Create a DataFrame from the product dictionary\n    df = pd.DataFrame(product_dict)\n    \n    # Calculate the average price and profit\n    avg_price = df['Price'].mean()\n    avg_profit = df['Profit'].mean()\n    \n    # Add the average price and profit to the DataFrame\n    df['Average Price'] = avg_price\n    df['Average Profit'] = avg_profit\n    \n    # Plot a bar chart of the profit for each product\n    if not df.empty:\n        ax = df.plot(kind='bar', x='Product', y='Profit', title='Profit Report')\n    else:\n        ax = None\n    \n    # Return the DataFrame and the Axes object\n    return (df, ax)\nproduct_dict = {\n    'Product': ['A', 'B', 'C'],\n    'Quantity': [10, 15, 20],\n    'Price': [100, 150, 200],\n    'Profit': [50, 75, 100]\n}\nproduct_keys = ['Product', 'Quantity', 'Price', 'Profit']"}
{"task_id": "BigCodeBench/151", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict, data_keys):\n    # Check if any of the keys in data_keys are present in data_dict\n    if not any(key in data_dict for key in data_keys):\n        raise ValueError(\"No keys in data_keys found in data_dict\")\n    \n    # Extract the data to be normalized\n    data = [data_dict[key] for key in data_keys]\n    \n    # Reshape the data to fit the scaler\n    data = np.array(data).reshape(-1,1)\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the data\n    normalized_data = scaler.fit_transform(data)\n    \n    # Create a DataFrame for the normalized data\n    normalized_df = pd.DataFrame(normalized_data, columns=data_keys)\n    \n    # Plot the normalized data\n    fig, ax = plt.subplots()\n    normalized_df.plot(ax=ax)\n    \n    return normalized_df, ax\ndata_dict = {'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50], 'C': [100, 200, 300, 400, 500]}\ndata_keys = ['A', 'B', 'C']"}
{"task_id": "BigCodeBench/152", "solution": "import pandas as pd\nimport numpy as np\nfrom random import randint\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\ndef task_func():\n    # Create a dictionary to hold the data\n    data = {student: {course: randint(0, 100) for course in COURSES} for student in STUDENTS}\n    \n    # Calculate the average grade for each student\n    for student in STUDENTS:\n        grades = list(data[student].values())\n        average = np.mean(grades)\n        data[student]['Average'] = average\n    \n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame.from_dict(data, orient='index')\n    \n    # Reorder columns to have 'Average' at the end\n    cols = list(df.columns)\n    cols.append(cols.pop(cols.index('Average')))\n    df = df[cols]\n    \n    return df"}
{"task_id": "BigCodeBench/153", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(data):\n    le = LabelEncoder()\n    le.fit(data)\n    encoded_data = le.transform(data)\n    result_df = pd.DataFrame({'Category': data, 'Encoded': encoded_data})\n    return result_df\ndata = ['red', 'blue', 'green', 'red', 'blue']"}
{"task_id": "BigCodeBench/154", "solution": "import re\nimport os\nimport glob\nimport mimetypes\ndef task_func(directory, file_pattern, suffix):\n    # Initialize an empty dictionary to store file names and their MIME types\n    file_mime_types = {}\n    \n    # Use glob to find all files in the directory that match the file_pattern\n    files = glob.glob(os.path.join(directory, file_pattern))\n    \n    # Iterate over each file found\n    for file in files:\n        # Check if the file ends with the specified suffix\n        if file.endswith(suffix):\n            # Use mimetypes to guess the MIME type of the file\n            mime_type, _ = mimetypes.guess_type(file)\n            # If a MIME type is found, add it to the dictionary\n            if mime_type:\n                file_mime_types[os.path.basename(file)] = mime_type\n            else:\n                # If no MIME type is found, add 'application/octet-stream' as a default\n                file_mime_types[os.path.basename(file)] = 'application/octet-stream'\n    \n    # Return the dictionary containing file names and their MIME types\n    return file_mime_types"}
{"task_id": "BigCodeBench/155", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    # Calculate the average of each row\n    df['Average'] = df.mean(axis=1)\n    \n    # Create a plot of the averages against their respective row indices\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['Average'], marker='o')\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    ax.set_title('Row Averages')\n    \n    # Return the DataFrame with the 'Average' column and the plot axes\n    return (df, ax)\ndata = [\n    [1, 2, 3, 4, 5, 6, 7, 8],\n    [9, 8, 7, 6, 5, 4, 3, 2],\n    [2, 3, 4, 5, 6, 7, 8, 9],\n    [10, 11, 12, 13, 14, 15, 16, 17]\n]"}
{"task_id": "BigCodeBench/156", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Normalize the data using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n    \n    # Create a DataFrame from the normalized data\n    df = pd.DataFrame(normalized_data, columns=data.columns)\n    \n    # Calculate the average of each row and add it as a new column 'Average'\n    df['Average'] = df.mean(axis=1)\n    \n    # Create a bar plot of the average values\n    ax = df['Average'].plot(kind='bar')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/157", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    if not all(isinstance(x, (int, float)) for x in data.values.flatten()):\n        raise ValueError(\"All elements in the DataFrame must be numeric.\")\n    \n    if len(data.shape) != 2:\n        raise ValueError(\"Input data must be a 2D array.\")\n    \n    # Calculate the average of values across each row\n    data['Average'] = data.mean(axis=1)\n    \n    # Calculate the correlation matrix\n    corr_matrix = data.corr()\n    \n    # Create a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    \n    return data, heatmap"}
{"task_id": "BigCodeBench/158", "solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\nimport os\ndef task_func(url_str, file_path):\n    # Fetch the JSON data from the given URL\n    with urllib.request.urlopen(url_str) as url:\n        data = json.loads(url.read().decode())\n\n    # Compress the JSON data into a gzip file\n    with gzip.open(file_path, 'wb') as f:\n        f.write(json.dumps(data).encode())\n\n    # Return the path to the compressed gzip file\n    return file_path"}
{"task_id": "BigCodeBench/159", "solution": "import struct\nimport io\nimport gzip\nimport numpy as np\ndef task_func(newArray):\n    # Convert the NumPy array to bytes\n    buffer = io.BytesIO()\n    np.save(buffer, newArray)\n    data = buffer.getvalue()\n    \n    # Compress the bytes using gzip\n    compressed_data = gzip.compress(data)\n    \n    return compressed_data"}
{"task_id": "BigCodeBench/160", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Check if the input data has exactly eight columns\n    if data.shape[1] != 8:\n        raise ValueError(\"Input data must have exactly eight columns.\")\n    \n    # Compute the average of each row\n    data['Average'] = data.mean(axis=1)\n    \n    # Plot the distribution of these averages\n    plt.figure()\n    ax = sns.distplot(data['Average'])\n    \n    # Evaluate the normality of the averages\n    if data.shape[0] >= 20:\n        _, p_value = stats.normaltest(data['Average'])\n    else:\n        p_value = None\n    \n    return data, ax, p_value"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n\n    # Initialize lists to hold extracted data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read its contents\n    with open(log_file, 'r') as file:\n        lines = file.readlines()\n\n    # Iterate through each line in the log file\n    for line in lines:\n        match = re.match(pattern, line)\n        if match:\n            # Extract the type, timestamp, and message from the log entry\n            log_type = match.group(1)\n            timestamp_str = match.group(2)\n            message = match.group(3)\n\n            # Validate the timestamp\n            try:\n                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n            except ValueError:\n                raise ValueError(f\"Invalid timestamp: {timestamp_str}\")\n\n            # Append the extracted data to the lists\n            types.append(log_type)\n            timestamps.append(timestamp)\n            messages.append(message)\n\n    # Check if any valid log entries were found\n    if not types:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Define the output CSV file path\n    csv_file = 'structured_logs.csv'\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file, index=False)\n\n    # Return the file path to the newly created CSV file\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Remove punctuation and split text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    if word_lengths:\n        plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), rwidth=rwidth)\n        plt.xlabel('Word Length')\n        plt.ylabel('Frequency')\n        plt.title('Distribution of Word Lengths')\n        plt.show()\n    else:\n        print(\"No words found in the text.\")\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/163", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(rows=5, cols=5):\n    if cols > 10:\n        raise ValueError(\"Number of columns exceeds the number of available categories.\")\n    categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n    data = np.random.rand(rows, cols)\n    df = pd.DataFrame(data, columns=categories[:cols])\n    ax = df.plot(kind='bar', stacked=True)\n    return ax"}
{"task_id": "BigCodeBench/164", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random data for each category\n    data = np.random.randint(data_range[0], data_range[1], size=(num_labels, num_labels))\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Create a stacked bar chart\n    df.plot(kind='bar', stacked=True, ax=ax)\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the figure object\n    return fig"}
{"task_id": "BigCodeBench/165", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    # Create a DataFrame with categories 'A' through 'E' and random integer values\n    categories = ['A', 'B', 'C', 'D', 'E']\n    data = {cat: [randint(rand_range[0], rand_range[1]) for _ in range(num_rows)] for cat in categories}\n    df = pd.DataFrame(data)\n    \n    # Plotting the DataFrame with a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    \n    # Display the plot\n    plt.show()\n    \n    return fig"}
{"task_id": "BigCodeBench/166", "solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    # Check if start_date and end_date are datetime objects\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime objects\")\n    \n    # Check if start_date is before end_date\n    if start_date > end_date:\n        raise ValueError(\"start_date must be before end_date\")\n    \n    # Create a list of all dates between start_date and end_date\n    all_dates = pd.date_range(start=start_date, end=end_date)\n    \n    # Create a list of holidays for the specified country\n    country_holidays = holidays.CountryHoliday(country)\n    \n    # Filter out weekends and holidays\n    business_days = [date for date in all_dates if date.weekday() < 5 and date not in country_holidays]\n    \n    return business_days"}
{"task_id": "BigCodeBench/167", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Generate a DataFrame containing random integer values across a specified number of categories\n    categories = [f'Category {i+1}' for i in range(num_types)]\n    data = {category: [randint(integer_range[0], integer_range[1]) for _ in range(num_types)] for category in categories}\n    df = pd.DataFrame(data, index=categories)\n    \n    # Visualize these data as a horizontal stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    \n    # Set the title and labels\n    ax.set_title('Horizontal Stacked Bar Chart')\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Categories')\n    \n    # Return the figure and axes\n    return fig, ax"}
{"task_id": "BigCodeBench/168", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_groups=5, data_size=5, labels=None):\n    # Generate random data\n    data = np.random.rand(data_size, num_groups)\n    df = pd.DataFrame(data, columns=[f'Group {i+1}' for i in range(num_groups)])\n    \n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    \n    # Set labels and title\n    if labels:\n        ax.set_xticklabels(labels)\n    ax.set_title('Stacked Bar Chart')\n    ax.set_ylabel('Values')\n    ax.set_xlabel('Categories')\n    \n    # Save the plot to a file\n    filename = 'test_plot.png'\n    plt.savefig(filename)\n    \n    # Return the figure, dataframe, and filename\n    return fig, df, filename"}
{"task_id": "BigCodeBench/169", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma must be positive.\")\n    \n    filtered_image = gaussian_filter(image, sigma=sigma)\n    \n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n    \n    plt.show()\n    \n    return ax, filtered_image"}
{"task_id": "BigCodeBench/170", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url, sort_by_column=\"title\"):\n    try:\n        response = requests.get(csv_url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Failed to retrieve data from {csv_url}: {e}\")\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to retrieve data from {csv_url}: Status code {response.status_code}\")\n\n    data = StringIO(response.text)\n    df = pd.read_csv(data)\n    df_sorted = df.sort_values(by=sort_by_column)\n    return df_sorted"}
{"task_id": "BigCodeBench/171", "solution": "import random\nimport pandas as pd\nimport collections\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n    # Reverse the dictionary to map vegetables to people\n    vegetable_to_people = collections.defaultdict(list)\n    for person, vegetables in vegetable_dict.items():\n        for vegetable in vegetables:\n            vegetable_to_people[vegetable].append(person)\n    \n    # Assign random counts to these vegetables\n    vegetable_counts = {vegetable: random.randint(1, 10) for vegetable in vegetable_to_people}\n    \n    # Calculate the occurrences of each vegetable as a percentage of the total counts\n    total_counts = sum(vegetable_counts.values())\n    vegetable_percentages = {vegetable: (count / total_counts) * 100 for vegetable, count in vegetable_counts.items()}\n    \n    # Create a dictionary to map each vegetable to a person from the input where vegetables are values\n    vegetable_to_person = {}\n    for vegetable, people in vegetable_to_people.items():\n        vegetable_to_person[vegetable] = random.choice(people)\n    \n    # Create a DataFrame with vegetable names, their random counts, and their percentage occurrence\n    data = {\n        'Vegetable': list(vegetable_counts.keys()),\n        'Count': list(vegetable_counts.values()),\n        'Percentage': list(vegetable_percentages.values())\n    }\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/172", "solution": "import json\nfrom datetime import datetime\ndef task_func(json_data):\n    # Load the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the UTC datetime string\n    utc_datetime_str = data.get('utc_datetime')\n    \n    if not utc_datetime_str:\n        return False  # or raise an error, depending on requirements\n    \n    # Convert the string to a datetime object\n    utc_datetime = datetime.fromisoformat(utc_datetime_str)\n    \n    # Determine if it's a weekend (Saturday or Sunday)\n    if utc_datetime.weekday() >= 5:  # 5 and 6 correspond to Saturday and Sunday\n        return True\n    else:\n        return False"}
{"task_id": "BigCodeBench/173", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(country_dict):\n    # Create a list of country names from the dictionary keys\n    countries = list(country_dict.keys())\n    \n    # Generate random GDP values for each country\n    gdp_values = np.random.randint(1000000000, 100000000000, size=len(countries))\n    \n    # Create a DataFrame with country names as index and GDP values as a column\n    df = pd.DataFrame(gdp_values, index=countries, columns=['GDP'])\n    \n    return df"}
{"task_id": "BigCodeBench/174", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"The input data must be a pandas DataFrame.\")\n    data[key] = np.random.randint(min_value, max_value, size=len(data))\n    return data"}
{"task_id": "BigCodeBench/175", "solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    # Check if the DataFrame is empty\n    if df.empty:\n        return plt.gca()\n    \n    # Check if the required columns are present\n    required_columns = ['Title', 'Views', 'Likes']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        print(f\"Missing columns: {', '.join(missing_columns)}\")\n        return plt.gca()\n    \n    # Filter videos with titles containing \"how\" or \"what\"\n    pattern = re.compile(r'\\b(how|what)\\b', re.IGNORECASE)\n    matching_titles = df[df['Title'].apply(lambda x: bool(pattern.search(x)))]\n    \n    # Calculate like ratios\n    matching_titles['Like Ratio'] = matching_titles['Likes'] / matching_titles['Views']\n    \n    # Check if there are any matching videos\n    if matching_titles.empty:\n        return plt.gca()\n    \n    # Create a bar plot of like ratios\n    ax = matching_titles.plot(kind='bar', x='Title', y='Like Ratio', legend=False)\n    ax.set_ylabel('Like Ratio')\n    ax.set_title('Like Ratios of \"How\" or \"What\" Videos')\n    \n    return ax"}
{"task_id": "BigCodeBench/176", "solution": "import re\nimport socket\ndef task_func(ip_addresses: list) -> dict:\n    ip_hostname_dict = {}\n    for ip in ip_addresses:\n        try:\n            hostname = socket.gethostbyaddr(ip)[0]\n            ip_hostname_dict[ip] = hostname\n        except socket.herror:\n            ip_hostname_dict[ip] = None\n    return ip_hostname_dict"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\ndef task_func(df):\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or missing necessary columns 'Title' and 'Content'\")\n\n    # Extract articles with titles containing 'like' or 'what' (case-insensitive)\n    keywords = ['like', 'what']\n    mask = df['Title'].str.contains('|'.join(keywords), case=False)\n    articles = df[mask]\n\n    # Tokenize and count word frequencies in the content, excluding punctuation\n    all_words = []\n    for content in articles['Content']:\n        # Remove punctuation\n        content = content.translate(str.maketrans('', '', punctuation))\n        # Tokenize\n        words = nltk.word_tokenize(content)\n        all_words.extend(words)\n\n    # Count frequency of each word\n    word_counts = Counter(all_words)\n\n    return dict(word_counts)"}
{"task_id": "BigCodeBench/178", "solution": "import re\nfrom urllib import request\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(ip_address):\n    # Check if the provided IP address is valid\n    if not re.match(IP_REGEX, ip_address):\n        return 'Invalid IP address received'\n    \n    # Get the public IP address from a JSON response\n    try:\n        with request.urlopen('https://ipapi.co/json/') as url:\n            data = json.loads(url.read().decode())\n            public_ip = data.get('ip')\n            if public_ip:\n                return public_ip\n            else:\n                return 'Public IP address not found in the response'\n    except Exception as e:\n        return f'Error: {str(e)}'"}
{"task_id": "BigCodeBench/179", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ndef task_func(df):\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        return plt.gca()\n    \n    # Filter articles with titles containing \"how\" or \"what\"\n    filtered_df = df[df['Title'].str.contains('how|what', case=False)]\n    \n    if filtered_df.empty:\n        return plt.gca()\n    \n    # Combine content into a single string\n    content = ' '.join(filtered_df['Content'])\n    \n    # Remove punctuation and convert to lowercase\n    content = re.sub(r'[^\\w\\s]', '', content).lower()\n    \n    # Split content into words\n    words = content.split()\n    \n    # Calculate TF-IDF scores\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([content])\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_scores = tfidf_matrix.toarray()[0]\n    \n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    plt.bar(feature_names, tfidf_scores)\n    plt.xlabel('Words')\n    plt.ylabel('TF-IDF Score')\n    plt.xticks(rotation=90)\n    plt.title('TF-IDF Scores of Words in Articles with Specific Titles')\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/180", "solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    try:\n        # Open the image file\n        img = Image.open(img_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The image file cannot be found.\")\n    \n    # Convert the image to a numpy array\n    img_array = np.array(img)\n    \n    # List to store the scaled images and their axes\n    scaled_images = []\n    \n    # Iterate through each scale factor\n    for factor in scale_factors:\n        # Resize the image using skimage.transform.resize\n        scaled_img = resize(img_array, (int(img_array.shape[0] * factor), int(img_array.shape[1] * factor)), mode='constant')\n        \n        # Create a new figure and axes for each scaled image\n        fig, ax = plt.subplots()\n        \n        # Display the scaled image on the axes\n        ax.imshow(scaled_img)\n        \n        # Append the axes and the scaled image to the list\n        scaled_images.append((ax, scaled_img))\n    \n    # Return the list of tuples containing axes and scaled images\n    return scaled_images"}
{"task_id": "BigCodeBench/181", "solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\ndef task_func(data, min_delay, max_delay):\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    return HttpResponse(data, content_type=\"application/json\")"}
{"task_id": "BigCodeBench/182", "solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(df):\n    # Filter articles that start with \"how\" or \"what\"\n    filtered_articles = df[df['title'].str.lower().str.startswith(('how', 'what'))]\n    \n    # Vectorize the content using CountVectorizer\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(filtered_articles['content'])\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=2)  # Assuming 2 clusters for \"how\" and \"what\" questions\n    cluster_labels = kmeans.fit_predict(X)\n    \n    return cluster_labels.tolist()"}
{"task_id": "BigCodeBench/183", "solution": "from django.http import HttpResponse\nimport uuid\ndef task_func(data):\n    # Generate a UUID\n    unique_id = str(uuid.uuid4())\n    \n    # Create a JSON response with the data and the UUID\n    response_data = {\n        'data': data,\n        'uuid': unique_id\n    }\n    \n    # Convert the response data to JSON format\n    json_response = HttpResponse(response_data, content_type='application/json')\n    \n    # Add the UUID to the HTTP headers\n    json_response['X-Unique-Id'] = unique_id\n    \n    return json_response"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Preprocess the text data\n    def preprocess_text(text):\n        # Remove numbers and punctuation\n        text = re.sub(r'[^a-zA-Z]', ' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n    \n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    \n    # Vectorize the text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert the vectorized data to a DataFrame\n    vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return vectorized_df"}
{"task_id": "BigCodeBench/185", "solution": "import pandas as pd\nimport numpy as np\nimport folium\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Check if 'Lon' or 'Lat' keys are missing in the dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Missing 'Lon' or 'Lat' keys in the dictionary.\")\n    \n    # Check if the values for 'Lon' and 'Lat' are tuples\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values for 'Lon' and 'Lat' must be tuples.\")\n    \n    # Generate random coordinates within the given ranges\n    lon_range = dic['Lon']\n    lat_range = dic['Lat']\n    lons = np.random.uniform(lon_range[0], lon_range[1], size=len(cities))\n    lats = np.random.uniform(lat_range[0], lat_range[1], size=len(cities))\n    \n    # Create a DataFrame with city names, longitudes, and latitudes\n    data = {'City': cities, 'Longitude': lons, 'Latitude': lats}\n    df = pd.DataFrame(data)\n    \n    # Create a map centered at the mean coordinates\n    mean_lon = df['Longitude'].mean()\n    mean_lat = df['Latitude'].mean()\n    m = folium.Map(location=[mean_lat, mean_lon], zoom_start=4)\n    \n    # Add markers for each city\n    for index, row in df.iterrows():\n        folium.Marker([row['Latitude'], row['Longitude']], popup=row['City']).add_to(m)\n    \n    return (m, df)"}
{"task_id": "BigCodeBench/186", "solution": "from geopy.distance import geodesic\nimport folium\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty\")\n    \n    # Create a Folium map\n    m = folium.Map(location=[0, 0], zoom_start=2)\n    \n    # Dictionary to store distances\n    distances = {}\n    \n    # List to store locations\n    locations = list(dic.values())\n    \n    # Add markers for each location\n    for location in locations:\n        folium.Marker(location=location, popup=location[0]).add_to(m)\n    \n    # Calculate distances between each pair of locations\n    for i in range(len(locations)):\n        for j in range(i+1, len(locations)):\n            dist = geodesic(locations[i], locations[j]).kilometers\n            distances[f\"{locations[i][0]} to {locations[j][0]}\"] = dist\n    \n    return m, distances"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Missing 'Lon' or 'Lat' keys in the dictionary.\")\n    lon_range = dic['Lon']\n    lat_range = dic['Lat']\n    if not isinstance(lon_range, tuple) or not isinstance(lat_range, tuple):\n        raise ValueError(\"'Lon' and 'Lat' values must be tuples.\")\n    if len(lon_range) != 2 or len(lat_range) != 2:\n        raise ValueError(\"Tuples must contain exactly two elements.\")\n    if lon_range[0] >= lon_range[1] or lat_range[0] >= lat_range[1]:\n        raise ValueError(\"Range tuples must have the first element smaller than the second.\")\n    \n    city_coords = []\n    for city in cities:\n        lon = np.random.uniform(lon_range[0], lon_range[1])\n        lat = np.random.uniform(lat_range[0], lat_range[1])\n        city_coords.append((city, Point(lon, lat)))\n    \n    gdf = gpd.GeoDataFrame(city_coords, columns=['City', 'Coordinates'])\n    return gdf"}
{"task_id": "BigCodeBench/188", "solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\ndef task_func(dic):\n    # Create a map centered around the first location\n    map = folium.Map(location=[dic[0]['lat'], dic[0]['lon']], zoom_start=10)\n    \n    # Add markers for each location\n    for location in dic:\n        folium.Marker([location['lat'], location['lon']], popup=location['name']).add_to(map)\n    \n    return map\ndic = [\n    {'name': 'Location 1', 'address': '1600 Amphitheatre Parkway, Mountain View, CA'},\n    {'name': 'Location 2', 'address': '1 Infinite Loop, Cupertino, CA'},\n    {'name': 'Location 3', 'address': '1 Microsoft Way, Redmond, WA'}\n]"}
{"task_id": "BigCodeBench/189", "solution": "import re\nimport json\nimport requests\ndef task_func(data_url: str) -> list:\n    try:\n        response = requests.get(data_url)\n        response.raise_for_status()\n        data = response.json()\n        names = []\n        for item in data:\n            if 'name' in item:\n                name = item['name']\n                if not re.search(r'\\[.*?\\]', name):\n                    names.append(name)\n        return names\n    except requests.exceptions.RequestException as e:\n        return \"Invalid url input\""}
{"task_id": "BigCodeBench/190", "solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\ndef task_func(csv_input):\n    # Connect to SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n    c = conn.cursor()\n    \n    # Create table or replace if exists\n    c.execute(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n    c.execute(f\"CREATE TABLE {TABLE_NAME} (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)\")\n    \n    # Read CSV input\n    if isinstance(csv_input, str):\n        with open(csv_input, 'r') as f:\n            reader = csv.reader(f)\n            data = [row for row in reader]\n    else:\n        reader = csv.reader(csv_input)\n        data = [row for row in reader]\n    \n    # Insert data into table\n    for row in data:\n        c.execute(f\"INSERT INTO {TABLE_NAME} (name, age) VALUES (?, ?)\", (row[0], row[1]))\n    \n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n    \n    # Query the table and return as DataFrame\n    conn = sqlite3.connect(DATABASE_NAME)\n    df = pd.read_sql_query(f\"SELECT * FROM {TABLE_NAME}\", conn)\n    conn.close()\n    \n    return df"}
{"task_id": "BigCodeBench/191", "solution": "import random\nfrom scipy import stats\ndef task_func(animals, mean):\n    sales = {animal: 0 for animal in animals}\n    num_customers = stats.poisson.rvs(mu=mean)\n    for _ in range(num_customers):\n        animal = random.choice(animals)\n        sales[animal] += 1\n    print(sales)\n    return sales"}
{"task_id": "BigCodeBench/192", "solution": "import re\nimport smtplib\nfrom email.message import EmailMessage\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    # Extract names that are not enclosed by square brackets\n    pattern = r'(?<!\\[)\\b\\w+\\b(?!\\])'\n    names = re.findall(pattern, text)\n    \n    # Send the names in an email\n    message = EmailMessage()\n    message['Subject'] = 'Extracted Names'\n    message.set_content('\\n'.join(names))\n    \n    # Connect to the SMTP server\n    if smtp is None:\n        server = smtplib.SMTP(smtp_server, smtp_port)\n        server.starttls()\n        server.login(email_address, email_password)\n    else:\n        server = smtp\n    \n    # Send the email\n    server.sendmail(email_address, recepient_address, message.as_string())\n    \n    # Close the connection\n    server.quit()\n    \n    return names"}
{"task_id": "BigCodeBench/193", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, randint, random\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\ndef task_func(rows, columns):\n    data = []\n    for _ in range(rows):\n        row = []\n        for _ in range(columns):\n            data_type = choice(DATA_TYPES)\n            if data_type == str:\n                row.append(''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5)))\n            elif data_type == int:\n                row.append(randint(0, 9))\n            elif data_type == float:\n                row.append(randint(0, 9) + random())\n            elif data_type == list:\n                row.append([randint(0, 9) for _ in range(randint(1, 5))])\n            elif data_type == tuple:\n                row.append(tuple(randint(0, 9) for _ in range(randint(1, 5))))\n            elif data_type == dict:\n                row.append({randint(0, 9): randint(0, 9) for _ in range(randint(1, 5))})\n            elif data_type == set:\n                row.append(set(randint(0, 9) for _ in range(randint(1, 5))))\n        data.append(row)\n    df = pd.DataFrame(data, columns=[f'col{col}' for col in range(columns)])\n    return df"}
{"task_id": "BigCodeBench/194", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\ndef task_func(data_size):\n    # Generate random numeric data\n    data = np.random.rand(data_size)\n    \n    # Select a random color for the histogram bars\n    color = np.random.choice(BAR_COLOR)\n    \n    # Create a histogram of the data with the selected color\n    plt.hist(data, color=color)\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the data and the color used\n    return (data, color)"}
{"task_id": "BigCodeBench/195", "solution": "import subprocess\nimport platform\nimport time\ndef task_func(url):\n    if platform.system() == \"Windows\":\n        subprocess.Popen([\"start\", url], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    elif platform.system() == \"Darwin\":\n        subprocess.Popen([\"open\", url], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    elif platform.system() == \"Linux\":\n        subprocess.Popen([\"xdg-open\", url], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    else:\n        print(\"Unsupported OS\")\n    time.sleep(1)\n    return 0"}
{"task_id": "BigCodeBench/196", "solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef task_func(length, range_limit=100, seed=0):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n    random.seed(seed)\n    random_numbers = [random.randint(0, range_limit) for _ in range(length)]\n    sns.set()\n    plt.figure()\n    sns.histplot(random_numbers, kde=False)\n    plt.show()\n    return plt.gca(), random_numbers"}
{"task_id": "BigCodeBench/197", "solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(l1, l2, N=10):\n    # Calculate the differences between the respective elements of l1 and l2\n    differences = [abs(a - b) for a, b in zip(l1, l2)]\n    \n    # Square the differences\n    squared_diffs = [d ** 2 for d in differences]\n    \n    # Take the square root of the squared differences\n    sqrt_diffs = [math.sqrt(d) for d in squared_diffs]\n    \n    # Find the N biggest differences\n    biggest_diffs = heapq.nlargest(N, sqrt_diffs)\n    \n    # Plot the differences\n    plt.plot(sqrt_diffs)\n    plt.title('Differences between l1 and l2')\n    plt.xlabel('Index')\n    plt.ylabel('Difference')\n    \n    # Return the plotted values as a matplotlib Axes object\n    return plt.gca()\nl1 = [1, 2, 3, 4, 5]\nl2 = [5, 4, 3, 2, 1]\nN = 3"}
{"task_id": "BigCodeBench/198", "solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n    avg = statistics.mean(data)\n    greater_than_avg = np.array([x for x in data if x > avg])\n    count = len([x for x in data if x > value])\n    sorted_data = sorted(data)\n    plt.hist(sorted_data, bins=10, color='skyblue', edgecolor='black')\n    plt.title('Histogram of Sorted Numbers')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    return greater_than_avg, count"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions must be strings\")\n    if not all(isinstance(tz, str) for tz in timezones.values()):\n        raise ValueError(\"All timezones must be strings\")\n    if not isinstance(seed, int):\n        raise ValueError(\"Seed must be an integer\")\n    \n    set_seed(seed)\n    report = []\n    for city in cities:\n        local_tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(local_tz)\n        weather = weather_conditions[randint(0, len(weather_conditions)-1)]\n        report.append({\n            'City': city,\n            'Local Time': local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather\n        })\n    return pd.DataFrame(report)"}
{"task_id": "BigCodeBench/200", "solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n    \n    # Find the average of all generated numbers\n    avg = statistics.mean(numbers)\n    \n    # Find numbers greater than the average\n    greater_than_avg = [num for num in numbers if num > avg]\n    \n    # Count how many numbers are greater than or equal to a specified value\n    count = sum(1 for num in numbers if num >= value)\n    \n    # Sort the numbers\n    sorted_numbers = sorted(numbers)\n    \n    # Plot the sorted numbers\n    plt.plot(sorted_numbers)\n    plt.show()\n    \n    return greater_than_avg, count"}
{"task_id": "BigCodeBench/201", "solution": "import bisect\nimport statistics\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df, column, value):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number.\")\n    if column not in df.columns:\n        raise ValueError(\"Column does not exist in the DataFrame.\")\n    \n    # Calculate the average of the column\n    avg = statistics.mean(df[column])\n    \n    # Find values larger than the average\n    values_above_avg = df[column][df[column] > avg].to_numpy()\n    \n    # Count the number of values larger than the given value\n    count_above_value = len(df[df[column] > value])\n    \n    # Generate a histogram plot\n    fig, ax = plt.subplots()\n    df[column].hist(bins=30, ax=ax)\n    ax.axvline(x=avg, color='r', linestyle='dashed', linewidth=2, label='Average')\n    ax.axvline(x=value, color='g', linestyle='dashed', linewidth=2, label='Given Value')\n    ax.legend()\n    \n    return (values_above_avg, count_above_value, ax)"}
{"task_id": "BigCodeBench/202", "solution": "import re\nimport json\nfrom collections import Counter\ndef task_func(json_str, top_n=10):\n    # Load the JSON string into a Python dictionary\n    data = json.loads(json_str)\n    \n    # Initialize a Counter to keep track of URL frequencies\n    url_counter = Counter()\n    \n    # Define a regular expression pattern for matching URLs\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    \n    # Function to extract URLs from a string\n    def extract_urls(text):\n        return url_pattern.findall(text)\n    \n    # Iterate through the dictionary and extract URLs\n    for key, value in data.items():\n        if isinstance(value, str):\n            urls = extract_urls(value)\n            for url in urls:\n                url_counter[url] += 1\n    \n    # Get the top N URLs\n    top_urls = url_counter.most_common(top_n)\n    \n    # Convert the top URLs to a dictionary\n    result = dict(top_urls)\n    \n    return result"}
{"task_id": "BigCodeBench/203", "solution": "import json\nimport smtplib\nfrom email.message import EmailMessage\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    if input_data is None:\n        input_data = {}\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n    \n    # Extract recipient email address and names from JSON-formatted string\n    try:\n        data = json.loads(input_data)\n        recipient_email = data.get('recipient_email')\n        names = data.get('names', [])\n    except json.JSONDecodeError:\n        print(\"Invalid JSON input\")\n        return [], None\n    \n    # Send the names in an email\n    if recipient_email and names:\n        message = EmailMessage()\n        message['Subject'] = 'Extracted Names'\n        message['From'] = email_address\n        message['To'] = recipient_email\n        message.set_content('\\n'.join(names))\n        \n        try:\n            smtp.send_message(message)\n            print(\"Email sent successfully\")\n        except smtplib.SMTPException as e:\n            print(f\"Failed to send email: {e}\")\n            return [], None\n    else:\n        print(\"Recipient email or names not found in input data\")\n        return [], None\n    \n    return names, message"}
{"task_id": "BigCodeBench/204", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(L):\n    # Calculate mean\n    mean = np.mean(L)\n    \n    # Calculate median\n    median = np.median(L)\n    \n    # Calculate mode\n    mode = stats.mode(L)[0][0]\n    \n    # Calculate standard deviation\n    std_dev = np.std(L)\n    \n    # Create histogram plot\n    fig, ax = plt.subplots()\n    ax.hist(L, bins=10, color='skyblue', edgecolor='black')\n    ax.set_title('Histogram of the Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Return a dictionary with the results and the plot\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'std_dev': std_dev,\n        'plot': ax\n    }\nL = [1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10]"}
{"task_id": "BigCodeBench/205", "solution": "import subprocess\nfrom multiprocessing import Pool\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\ndef task_func(commands):\n    if not commands:\n        return []\n    with Pool() as p:\n        outputs = p.map(execute_command, commands)\n    return outputs"}
{"task_id": "BigCodeBench/206", "solution": "import csv\nimport json\nimport os\ndef task_func(file_name):\n    try:\n        with open(file_name, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            data = [row for row in csv_reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n    json_file_name = os.path.splitext(file_name)[0] + '.json'\n    with open(json_file_name, 'w') as json_file:\n        json.dump(data, json_file)\n    return json_file_name"}
{"task_id": "BigCodeBench/207", "solution": "import re\nimport requests\ndef task_func(input):\n    # Extract the API endpoint from the input string\n    endpoint = re.search(r'https?://[^\\s]+', input)\n    if endpoint:\n        endpoint = endpoint.group(0)\n    else:\n        return {\"error\": \"No API endpoint found in the input string.\"}\n    \n    # Send a GET request to the endpoint\n    try:\n        response = requests.get(endpoint)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException as e:\n        return {\"error\": f\"Failed to retrieve data from {endpoint}: {e}\"}\n    \n    # Return the response data in JSON format\n    try:\n        data = response.json()\n    except ValueError as e:\n        return {\"error\": f\"Failed to parse JSON from {endpoint}: {e}\"}\n    \n    return data"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    stats = {\n        'count': len(walk),\n        'mean': np.mean(walk),\n        'std': np.std(walk),\n        'min': np.min(walk),\n        '5th percentile': np.percentile(walk, 5),\n        '25th percentile': np.percentile(walk, 25),\n        'median': np.median(walk),\n        '75th percentile': np.percentile(walk, 75),\n        '95th percentile': np.percentile(walk, 95),\n        'max': np.max(walk)\n    }\n    \n    fig, ax = plt.subplots()\n    ax.plot(walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    \n    return stats, ax"}
{"task_id": "BigCodeBench/209", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Extract x and y values from the data\n    x = [t[0] for t in data]\n    y = [t[1] for t in data]\n    \n    # Find the tuple with the maximum value at index 1\n    max_tuple = max(data, key=itemgetter(1))\n    \n    # Create a scatter plot\n    scatter = plt.scatter(x, y, label='Data Points')\n    \n    # Highlight the tuple with the maximum value at index 1\n    plt.scatter(max_tuple[0], max_tuple[1], color='red', label='Max Tuple')\n    \n    # Set the title and labels\n    plt.title('Max Tuple Highlighted')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    \n    # Add a legend\n    plt.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return plt.gca()\ndata = [(1, 2), (3, 4), (5, 6), (7, 8)]"}
{"task_id": "BigCodeBench/210", "solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Count the frequency of each letter in the dataset\n    letter_counts = collections.Counter(data)\n    \n    # Find the letter with the maximum count\n    max_letter = max(letter_counts, key=letter_counts.get)\n    \n    # Create a bar plot\n    letters = list(letter_counts.keys())\n    counts = list(letter_counts.values())\n    fig, ax = plt.subplots()\n    bars = ax.bar(letters, counts, color='skyblue')\n    \n    # Highlight the bar with the maximum count\n    bars[letters.index(max_letter)].set_color('red')\n    \n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    \n    # Add legend\n    ax.legend(['Letter Counts', 'Max Value Letter'])\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Download the zip file from the URL\n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download file from {url}\")\n\n    # Save the zip file to a temporary location\n    with open(\"temp.zip\", \"wb\") as f:\n        f.write(response.content)\n\n    # Extract the contents to the specified directory\n    with zipfile.ZipFile(\"temp.zip\", \"r\") as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Get the list of extracted files\n    extracted_files = os.listdir(destination_directory)\n\n    # Remove the temporary zip file\n    os.remove(\"temp.zip\")\n\n    return extracted_files"}
{"task_id": "BigCodeBench/212", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Unzip the data into x and y arrays\n    x, y = zip(*data)\n    \n    # Find the point with the maximum y-value\n    max_y_point = max(data, key=itemgetter(1))\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    \n    # Mark the point with the maximum y-value\n    ax.scatter(*max_y_point, color='red', s=100)\n    \n    # Label the axes and set the title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Points with Max Y Point Highlighted')\n    \n    # Return the axes object and the maximum y-value point\n    return ax, max_y_point\ndata = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]"}
{"task_id": "BigCodeBench/213", "solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    x = []\n    y = []\n    for i in range(intervals):\n        x.append(i)\n        y.append(random.random())\n        time.sleep(1)\n    plt.plot(x, y)\n    plt.show()\n    return plt.gca(), kurtosis(y)"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Parameters:\n    - seed (int): Seed for random number generation.\n    - image_size (tuple): Size of the image (height, width, channels).\n    - range_low (int): Lower bound for random values.\n    - range_high (int): Upper bound for random values.\n\n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object of the plot.\n    - image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n    - ValueError: If range_low is not less than range_high.\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size, dtype=np.uint8)\n    ax = plt.imshow(image)\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/215", "solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nHEADERS = {\n    'accept': 'application/json'\n}\ndef task_func(url, parameters):\n    try:\n        response = requests.get(url, params=parameters, headers=HEADERS)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Error accessing the URL: {e}\")\n    \n    try:\n        data = response.json()\n    except json.JSONDecodeError:\n        raise Exception(\"Invalid data format\")\n    \n    if not data:\n        raise Exception(\"Empty data\")\n    \n    df = pd.DataFrame(data)\n    \n    if df.empty:\n        raise Exception(\"Empty DataFrame\")\n    \n    correlation_matrix = df.corr()\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n    \n    return df, heatmap"}
{"task_id": "BigCodeBench/216", "solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\ndef task_func(json_dir_path, word_count):\n    # Initialize a list to hold all words from all JSON files\n    all_words = []\n    \n    # List all files in the directory\n    files = os.listdir(json_dir_path)\n    \n    # Iterate over each file in the directory\n    for file in files:\n        # Check if the file is a JSON file\n        if file.endswith('.json'):\n            # Construct the full file path\n            file_path = os.path.join(json_dir_path, file)\n            \n            # Open and read the JSON file\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            \n            # Extract the text content from the JSON data\n            # Assuming the text is stored in a key called 'text'\n            text = data.get('text', '')\n            \n            # Split the text into words and add them to the list\n            words = text.split()\n            all_words.extend(words)\n    \n    # Count the frequency of each word\n    word_counts = Counter(all_words)\n    \n    # Get the most common words\n    most_common = word_counts.most_common(word_count)\n    \n    return most_common"}
{"task_id": "BigCodeBench/217", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    ax = plt.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    xmin, xmax = ax[1].min(), ax[1].max()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(f'Normal Distribution with $\\\\mu = {mu:.2f}, \\\\sigma = {sigma:.2f}$')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    empirical_mean = np.mean(sample)\n    empirical_std = np.std(sample)\n    return ax, empirical_mean, empirical_std"}
{"task_id": "BigCodeBench/218", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\ndef task_func(df, dict_mapping, plot_histogram=False):\n    \"\"\"\n    Pre-processes a DataFrame by replacing values according to a dictionary mapping, \n    standardizing specified features, and optionally drawing a histogram of the target variable.\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame to be preprocessed.\n    - dict_mapping (dict): A dictionary mapping old values to new values for replacement.\n    - plot_histogram (bool): Whether to plot a histogram of the target variable.\n\n    Returns:\n    - pd.DataFrame: The preprocessed DataFrame with standardized features and values replaced as per dict_mapping.\n    - matplotlib.axes._subplots.AxesSubplot: The histogram of the target variable if plot_histogram is True, otherwise None.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    # Check if all required columns are in the DataFrame\n    required_cols = FEATURES + [TARGET]\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"The following columns are missing from the DataFrame: {', '.join(missing_cols)}\")\n    \n    # Replace values in the DataFrame according to dict_mapping\n    df.replace(dict_mapping, inplace=True)\n    \n    # Standardize the specified features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    # Optionally plot a histogram of the target variable\n    if plot_histogram:\n        import matplotlib.pyplot as plt\n        ax = df[TARGET].hist(bins=30)\n        plt.title('Histogram of Target Variable')\n        plt.xlabel(TARGET)\n        plt.ylabel('Frequency')\n        return df, ax\n    else:\n        return df, None"}
{"task_id": "BigCodeBench/219", "solution": "import math\nimport statistics\nimport numpy as np\ndef task_func(input_list):\n    # Sort the input list in ascending order based on the degree value of its elements\n    sorted_list = sorted(input_list, key=lambda x: x.degree)\n    \n    # Calculate the mean, median, and mode of the sorted list\n    mean = round(statistics.mean(sorted_list), 2)\n    median = round(statistics.median(sorted_list), 2)\n    mode = round(statistics.mode(sorted_list), 2)\n    \n    # Calculate the magnitude of the fast fourier transform of the degree values\n    degree_values = [x.degree for x in input_list]\n    fft = np.fft.fft(degree_values)\n    magnitude = np.abs(fft)\n    \n    # Calculate the mean, median, and mode of the magnitude\n    mean_fft = round(statistics.mean(magnitude), 2)\n    median_fft = round(statistics.median(magnitude), 2)\n    mode_fft = round(statistics.mode(magnitude), 2)\n    \n    # Return the results as a tuple\n    return (mean, median, mode, mean_fft, median_fft, mode_fft)"}
{"task_id": "BigCodeBench/220", "solution": "from random import choice\nimport turtle\nimport time\ndef task_func(colors):\n    # Set up the turtle screen\n    screen = turtle.Screen()\n    screen.title(\"Random Colored Squares\")\n    screen.bgcolor(\"white\")\n    \n    # Create a turtle object\n    t = turtle.Turtle()\n    t.speed(1)  # Set the drawing speed\n    \n    # Draw five squares with random colors\n    for _ in range(5):\n        # Choose a random color from the list\n        color = choice(colors)\n        t.color(color)\n        \n        # Draw a square\n        for _ in range(4):\n            t.forward(100)\n            t.right(90)\n        \n        # Move the turtle to a new position for the next square\n        t.penup()\n        t.forward(150)\n        t.pendown()\n        \n        # Pause for 1 second before drawing the next square\n        time.sleep(1)\n    \n    # Keep the window open\n    turtle.done()\ncolors = [\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\", \"pink\"]"}
{"task_id": "BigCodeBench/221", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\ndef task_func(df, dct):\n    try:\n        # Check if 'feature1' is in the DataFrame\n        if 'feature1' not in df.columns:\n            return \"Invalid input\"\n        \n        # Replace values in the DataFrame based on the provided dictionary mapping\n        for col in df.columns:\n            if col in dct:\n                df[col] = df[col].replace(dct[col])\n        \n        # Initialize a dictionary to store the statistics\n        stats_dict = {}\n        \n        for feature in FEATURES:\n            if feature in df.columns:\n                # Calculate mean\n                mean = np.mean(df[feature])\n                # Calculate median\n                median = np.median(df[feature])\n                # Calculate mode\n                mode = stats.mode(df[feature])[0][0] if not df[feature].isnull().all() else None\n                # Calculate variance\n                variance = np.var(df[feature])\n                \n                # Store the statistics in the dictionary\n                stats_dict[feature] = {\n                    'mean': mean,\n                    'median': median,\n                    'mode': mode,\n                    'variance': variance\n                }\n            else:\n                stats_dict[feature] = \"Feature not found\"\n        \n        return stats_dict\n    except Exception as e:\n        return \"Invalid input\""}
{"task_id": "BigCodeBench/222", "solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(list_input):\n    # Sort the list based on the degree value of its elements\n    sorted_list = sorted(list_input, key=lambda x: math.degrees(x))\n    \n    # Calculate the cumulative sum of the sorted list\n    cumulative_sum = np.cumsum(sorted_list)\n    \n    # Draw a line chart of the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumulative_sum)\n    ax.set_title('Cumulative Sum of Sorted List')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    \n    # Return the cumulative sum as a numpy array and the Axes object\n    return cumulative_sum, ax"}
{"task_id": "BigCodeBench/223", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    \n    if columns is None:\n        columns = df.columns\n    \n    for col in columns:\n        if col in dct:\n            if df[col].dtype == 'object':\n                le = LabelEncoder()\n                df[col] = le.fit_transform(df[col])\n            else:\n                mean = df[col].mean()\n                std = df[col].std()\n                if std != 0:\n                    df[col] = (df[col] - mean) / std\n                else:\n                    df[col] = 0\n        else:\n            if df[col].dtype == 'object':\n                le = LabelEncoder()\n                df[col] = le.fit_transform(df[col])\n            else:\n                mean = df[col].mean()\n                std = df[col].std()\n                if std != 0:\n                    df[col] = (df[col] - mean) / std\n                else:\n                    df[col] = 0\n    return df"}
{"task_id": "BigCodeBench/224", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x = np.arange(range_start, range_end, step)\n    sin_x = np.sin(x)\n    cos_x = np.cos(x)\n    abs_diff = np.abs(sin_x - cos_x)\n    fft_abs_diff = fft(abs_diff)\n    mean = np.mean(fft_abs_diff)\n    median = np.median(fft_abs_diff)\n    \n    generator = zip(x, sin_x, cos_x, abs_diff)\n    ax = plt.plot(x, sin_x, label='sin(x)')\n    ax += plt.plot(x, cos_x, label='cos(x)')\n    ax += plt.plot(x, abs_diff, label='abs(sin(x) - cos(x))')\n    plt.legend()\n    plt.show()\n    \n    return (generator, ax, abs(mean), abs(median))"}
{"task_id": "BigCodeBench/225", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    \"\"\"\n    Replace values in a DataFrame with a dictionary mapping and optionally record histograms for specified columns.\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame.\n    - dct (dict): A dictionary mapping old values to new values for replacement.\n    - columns (list, optional): List of column names to perform replacements on. If None, all columns are considered.\n    - plot_histograms (bool, optional): Whether to plot histograms for the specified columns after replacement. Defaults to False.\n\n    Returns:\n    - pd.DataFrame: The DataFrame with replaced values.\n\n    Raises:\n    - ValueError: If the input df is not a DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    if columns is None:\n        columns = df.columns\n    \n    for col in columns:\n        if col in df.columns:\n            df[col] = df[col].replace(dct)\n    \n    if plot_histograms:\n        for col in columns:\n            if col in df.columns:\n                plt.figure()\n                df[col].hist(bins=10)\n                plt.title(f'Histogram of {col}')\n                plt.xlabel(col)\n                plt.ylabel('Frequency')\n                plt.show()\n    \n    return df"}
{"task_id": "BigCodeBench/226", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(range_start=0, range_end=10, step=0.1):\n    x_values = np.arange(range_start, range_end, step)\n    y_values = np.exp(x_values)\n    data = zip(x_values, y_values)\n    return data"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file {audio_file} does not exist.\")\n    \n    # Read the audio file\n    audio_data, sample_rate = sf.read(audio_file)\n    \n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(audio_data**2)))\n    \n    # Normalize the list L to MxN matrix\n    L = np.array(L)\n    L = L.reshape(M, N)\n    L_normalized = L / np.max(np.abs(L))\n    \n    # Generate the spectrogram\n    f, t, Sxx = signal.stft(L_normalized, fs=sample_rate)\n    Sxx_db = 20 * np.log10(np.abs(Sxx) + 1e-10)\n    \n    # Adjust the amplitude based on SPL\n    Sxx_db_adjusted = Sxx_db + spl\n    \n    # Create a figure and axis for the spectrogram\n    fig, ax = plt.subplots()\n    img = ax.imshow(Sxx_db_adjusted, aspect='auto', origin='lower', extent=[t.min(), t.max(), f.min(), f.max()], cmap='viridis')\n    \n    # Set the scale for frequency and time\n    ax.set_yscale('log')\n    ax.set_xscale('linear')\n    \n    # Add a color bar\n    cbar = fig.colorbar(img, ax=ax)\n    cbar.set_label('Amplitude (dB)')\n    \n    # Set labels and title\n    ax.set_xlabel('Time (s)')\n    ax.set_ylabel('Frequency (Hz)')\n    ax.set_title('Spectrogram with Adjusted SPL')\n    \n    # Return the normalized matrix and the figure object\n    return L_normalized, fig"}
{"task_id": "BigCodeBench/228", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, dct):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    \n    # Replace certain values in the DataFrame with a dictionary mapping\n    for col in df.columns:\n        if col in dct:\n            df[col].replace(dct[col], inplace=True)\n    \n    # Calculate the Pearson correlation coefficient between each pair of columns\n    corr_matrix = df.corr(method='pearson')\n    \n    return corr_matrix"}
{"task_id": "BigCodeBench/229", "solution": "import json\nimport random\nfrom datetime import datetime, timedelta\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\ndef task_func(file_path, num_entries, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    # Generate random activity data\n    activity_data = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'read', 'write'])\n        timestamp = datetime.now() - timedelta(days=random.randint(1, 365))\n        entry = {\n            'user': user,\n            'action': action,\n            'timestamp': timestamp.isoformat()\n        }\n        activity_data.append(entry)\n    \n    # Write to JSON file\n    with open(file_path, 'w') as f:\n        json.dump(activity_data, f, indent=4)\n    \n    return file_path"}
{"task_id": "BigCodeBench/230", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        return \"Invalid input\"\n    \n    if not all(col in df.columns for col in COLUMNS):\n        return \"Invalid input\"\n    \n    # Ensure 'Name' is unique\n    if df['Name'].duplicated().any():\n        df = df.drop_duplicates(subset='Name')\n    \n    # Create histogram of scores\n    plt.figure(figsize=(10, 5))\n    sns.histplot(df, x='Score', kde=True)\n    plt.title('Histogram of Scores')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    \n    # Create boxplot of scores by country\n    plt.figure(figsize=(10, 5))\n    sns.boxplot(x='Country', y='Score', data=df)\n    plt.title('Boxplot of Scores by Country')\n    plt.xlabel('Country')\n    plt.ylabel('Score')\n    \n    # Combine both plots into a single figure\n    fig = plt.gcf()\n    return fig"}
{"task_id": "BigCodeBench/231", "solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\ndef task_func(obj_list) -> Axes:\n    if not obj_list:\n        mean = 0\n        std = 0\n    else:\n        values = [obj.value for obj in obj_list]\n        mean = np.mean(values)\n        std = np.std(values)\n    x = np.linspace(mean - 4*std, mean + 4*std, 100)\n    y = stats.norm.pdf(x, mean, std)\n    plt.plot(x, y)\n    plt.title('Custom Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Probability Density')\n    plt.grid(True)\n    return plt.gca()"}
{"task_id": "BigCodeBench/232", "solution": "import pandas as pd\nimport collections\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a DataFrame\")\n    \n    # Remove duplicates based on customer names\n    df_unique = df.drop_duplicates(subset='customer_name')\n    \n    # Calculate total sales\n    total_sales = df_unique['sales'].sum()\n    \n    # Find the most popular category\n    category_counts = df_unique['category'].value_counts()\n    most_popular_category = category_counts.idxmax()\n    \n    # In case of a tie, choose the first category in alphabetical order\n    if category_counts[most_popular_category] == category_counts[category_counts.idxmin()]:\n        most_popular_category = sorted(category_counts[category_counts == category_counts[most_popular_category]].index)[0]\n    \n    return {'Total Sales': total_sales, 'Most Popular Category': most_popular_category}"}
{"task_id": "BigCodeBench/233", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(obj_list, attr, num_bins=30, seed=0):\n    random.seed(seed)\n    values = [getattr(obj, attr) for obj in obj_list]\n    plt.hist(values, bins=num_bins)\n    plt.title('Histogram of attribute values')\n    plt.xlabel('Attribute Value')\n    plt.ylabel('Count')\n    return plt.gca()"}
{"task_id": "BigCodeBench/234", "solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    # Remove duplicates based on 'name' column\n    df_unique = df.drop_duplicates(subset='name')\n    \n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df_unique['age'], df_unique['score'])\n    \n    # Create scatter plot\n    plt.scatter(df_unique['age'], df_unique['score'], label='Data points')\n    \n    # Plot the regression line\n    plt.plot(df_unique['age'], intercept + slope*df_unique['age'], color='red', label='Regression line')\n    \n    # Set labels and title\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.title('Linear Regression: Age vs Score')\n    \n    # Show legend\n    plt.legend()\n    \n    # Return the plot objects\n    return plt.gcf(), plt.gca()"}
{"task_id": "BigCodeBench/235", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, num_samples)\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=num_bins, density=True, alpha=0.6, color='g')\n    x = np.linspace(min(data), max(data), 100)\n    y = (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(- (x - mu)**2 / (2 * sigma**2))\n    ax.plot(x, y, 'r', linewidth=2)\n    ax.set_title('Histogram of Normal Distribution with PDF and OLS')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    return ax"}
{"task_id": "BigCodeBench/236", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\ndef task_func(df, test_size=0.2, random_state=42):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    \n    # Drop duplicates based on 'Name' column\n    df = df.drop_duplicates(subset='Name')\n    \n    # Assuming 'Category' is the target variable and 'Age' and 'Score' are features\n    X = df[['Age', 'Score']]\n    y = df['Category']\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the Random Forest Classifier\n    clf = RandomForestClassifier(random_state=random_state)\n    clf.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy"}
{"task_id": "BigCodeBench/237", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, save_plot=False, plot_path=None):\n    \"\"\"\n    Unzip a list of objects and their 3D coordinates, run PCA to reduce the dimensionality to 2D,\n    and depending on the value of save_plot parameter, either save the plot to the provided path\n    and return the 2D coordinates or return the 2D coordinates and the plot's Axes.\n\n    Parameters:\n    - data (list of tuples): Each tuple contains an object name (string) and its 3D coordinates (list of 3 floats).\n    - save_plot (bool): If True, save the plot to the specified path and return the 2D coordinates.\n    - plot_path (str, optional): The path where the plot should be saved. Required if save_plot is True.\n\n    Returns:\n    - coordinates_2d (numpy.ndarray): The 2D coordinates after applying PCA.\n    - ax (matplotlib.axes._axes.Axes, optional): The plot's Axes if save_plot is True.\n\n    Raises:\n    - ValueError: If save_plot is True but plot_path is not provided.\n    \"\"\"\n    # Unzip the data\n    objects, coordinates = zip(*data)\n    coordinates = np.array(coordinates)\n\n    # Run PCA to reduce dimensionality to 2D\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(coordinates)\n\n    if save_plot:\n        if plot_path is None:\n            raise ValueError(\"plot_path must be provided when save_plot is True\")\n        \n        # Plot the 2D coordinates\n        ax = plt.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n        \n        # Save the plot\n        plt.savefig(plot_path)\n        \n        return coordinates_2d, ax\n    else:\n        return coordinates_2d"}
{"task_id": "BigCodeBench/238", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Standardize 'Age' and 'Score' columns\n    scaler = StandardScaler()\n    df[['Age', 'Score']] = scaler.fit_transform(df[['Age', 'Score']])\n    \n    # Remove duplicate entries based on 'Name'\n    df.drop_duplicates(subset='Name', inplace=True)\n    \n    # Plot a scatter plot of standardized 'Age' and 'Score'\n    plt.scatter(df['Age'], df['Score'])\n    plt.title('Scatter Plot of Standardized Age and Score')\n    plt.xlabel('Age (standardized)')\n    plt.ylabel('Score (standardized)')\n    \n    # Return the DataFrame with standardized 'Age' and 'Score' and duplicates removed\n    # and the Axes object of the scatter plot\n    return df, plt.gca()\ndata = {\n    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],\n    'Age': [25, 30, 25, 35, 30],\n    'Score': [85, 90, 85, 95, 90]\n}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = []\n    for item in original:\n        if isinstance(item, tuple):\n            for sub_item in item:\n                if isinstance(sub_item, (int, float)):\n                    numeric_values.append(sub_item)\n        elif isinstance(item, (int, float)):\n            numeric_values.append(item)\n    \n    # Convert the list of numeric values to a numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Compute basic statistics\n    mean = np.mean(numeric_array)\n    std_dev = np.std(numeric_array)\n    minimum = np.min(numeric_array)\n    maximum = np.max(numeric_array)\n    statistics = {\n        'mean': mean,\n        'standard_deviation': std_dev,\n        'minimum': minimum,\n        'maximum': maximum\n    }\n    \n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, density=True, alpha=0.6, bins='auto')\n    # Fit a normal distribution to the data\n    mu, std = stats.norm.fit(numeric_array)\n    # Generate the PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return numeric_array, statistics, ax\noriginal = [(1, 2), 3, (4, 5), 6]"}
{"task_id": "BigCodeBench/240", "solution": "import pandas as pd\nfrom random import uniform\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame({column_name: data})\n    return df"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array])\n    \n    # Plot the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array[0], label='Normalized')\n    ax.legend()\n    \n    return original_array, normalized_array[0], ax"}
{"task_id": "BigCodeBench/242", "solution": "import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(image_path, kernel_size):\n    # Check if the kernel_size is a positive integer\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"kernel_size must be a positive integer\")\n    \n    # Read the image\n    try:\n        image = cv2.imread(image_path)\n        if image is None:\n            raise FileNotFoundError(\"The specified image file does not exist.\")\n    except Exception as e:\n        raise FileNotFoundError(\"The specified image file does not exist.\") from e\n    \n    # Apply blur effect\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n    \n    # Convert images to RGB for matplotlib\n    original_image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    blurred_image_rgb = cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB)\n    \n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    \n    # Display original and blurred images\n    axes[0].imshow(original_image_rgb)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n    \n    axes[1].imshow(blurred_image_rgb)\n    axes[1].set_title('Blurred Image')\n    axes[1].axis('off')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the blurred image and the axes objects\n    return blurred_image, axes"}
{"task_id": "BigCodeBench/243", "solution": "import pandas as pd\nimport random\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\ndef task_func(n_data_points=N_DATA_POINTS):\n    if n_data_points == 0:\n        return pd.DataFrame(columns=['Value'])\n    else:\n        data = [round(random.uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(n_data_points)]\n        df = pd.DataFrame(data, columns=['Value'])\n        return df"}
{"task_id": "BigCodeBench/244", "solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\ndef task_func(original):\n    if not original:\n        return np.array([]), np.array([]), None\n    original = np.array(original)\n    fft_data = fft(original)\n    magnitude = np.abs(fft_data)\n    axes = plt.hist(magnitude, bins=30, color='skyblue', edgecolor='black')\n    return original, fft_data, axes"}
{"task_id": "BigCodeBench/245", "solution": "import pandas as pd\nimport random\nfrom scipy import stats\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate a random dataset of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Calculate statistical measures\n    mean = round(pd.Series(data).mean(), 3)\n    median = round(pd.Series(data).median(), 3)\n    mode = round(stats.mode(data)[0][0], 3)\n    \n    # Return the results in a dictionary\n    return {'mean': mean, 'median': median, 'mode': mode}"}
{"task_id": "BigCodeBench/246", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\nANGLES = np.arange(0, 2*np.pi, 0.01)\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], np.array([]), None\n    np.random.seed(seed)\n    frequencies = np.random.rand(n_waves) * 10 + 1\n    sine_waves = [np.sin(f * ANGLES) for f in frequencies]\n    mixed_signal = np.sum(sine_waves, axis=0)\n    fft_data = fft(mixed_signal)\n    magnitude = np.abs(fft_data)\n    fig, ax = plt.subplots()\n    ax.hist(magnitude, bins=50)\n    ax.set_title('Histogram of FFT Magnitude')\n    return sine_waves, fft_data, ax"}
{"task_id": "BigCodeBench/247", "solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    if max_value < min_value:\n        raise ValueError(\"max_value must be greater than or equal to min_value\")\n    \n    # Generate random data\n    data = [random.uniform(min_value, max_value) for _ in range(n_data_points)]\n    \n    # Truncate to 3 decimal places\n    data = [round(x, 3) for x in data]\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(data, columns=['Original Value'])\n    \n    # Normalize the data\n    scaler = StandardScaler()\n    normalized_data = scaler.fit_transform(df)\n    \n    # Create a new DataFrame with normalized values\n    normalized_df = pd.DataFrame(normalized_data, columns=['Normalized Value'])\n    \n    return normalized_df"}
{"task_id": "BigCodeBench/248", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data_list is empty\")\n    unzipped = list(itertools.zip_longest(*data_list))\n    for i, data in enumerate(unzipped):\n        plt.plot(data)\n    plt.show()"}
{"task_id": "BigCodeBench/249", "solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate a random set of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    \n    # Create a DataFrame with the generated data\n    df = pd.DataFrame(data, columns=['Value'])\n    \n    # Divide the data into train and test sets based on the given test size\n    train_set, test_set = train_test_split(df, test_size=test_size, random_state=42)\n    \n    # Return the train and test sets as a tuple\n    return (train_set, test_set)"}
{"task_id": "BigCodeBench/250", "solution": "import numpy as np\nimport itertools\nimport json\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    # Initialize a dictionary to store the mean values for each position\n    mean_values = {}\n    \n    # Iterate over each position in the data list\n    for i, position_data in enumerate(data_list):\n        # Extract numeric values from the position data\n        numeric_values = [value for value in position_data if isinstance(value, (int, float))]\n        \n        # Calculate the mean of the numeric values\n        if numeric_values:\n            mean = np.mean(numeric_values)\n        else:\n            mean = None  # or you can set it to 0 or any other value as per requirement\n        \n        # Store the mean in the dictionary with the key 'Position {i}'\n        mean_values[f'Position {i}'] = mean\n    \n    # Optionally, export the results to a specified JSON file\n    if json_file_name:\n        with open(json_file_name, 'w') as json_file:\n            json.dump(mean_values, json_file, indent=4)\n    \n    # Return the dictionary with mean values\n    return mean_values\ndata_list = [\n    [1, 2, 3, 4, 5],\n    [10, 20, 30, 40, 50],\n    [100, 200, 300, 400, 500]\n]"}
{"task_id": "BigCodeBench/251", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    # Assuming the DataFrame has a column named 'job' that contains job titles\n    job_counts = data['job'].value_counts()\n    \n    fig, ax = plt.subplots()\n    ax.pie(job_counts, labels=job_counts.index, autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    \n    return fig"}
{"task_id": "BigCodeBench/252", "solution": "import matplotlib.pyplot as plt\nfrom itertools import zip_longest\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\ndef task_func(data, labels):\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    # Iterate over the data and labels\n    for i, (x, y, label) in enumerate(zip_longest(data, labels, fillvalue=None)):\n        # If there are more data series than colors, cycle through the colors\n        color = COLORS[i % len(COLORS)] if i < len(COLORS) else 'black'\n        # Plot the data\n        ax.plot(x, y, color=color, label=label)\n    # Add a legend\n    ax.legend()\n    # Show the plot\n    plt.show()\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/253", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\ndef task_func(ax):\n    # Generate random phase shift\n    phase_shift = random.uniform(0, 2 * np.pi)\n    # Generate random amplitude\n    amplitude = random.uniform(0.5, 2)\n    # Generate random frequency\n    frequency = random.uniform(0.1, 1)\n    # Generate random color\n    color = random.choice(COLORS)\n    # Generate random radial label position\n    radial_label_position = random.uniform(0, 2 * np.pi)\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, 100)\n    # Generate y values\n    y = amplitude * np.sin(frequency * x + phase_shift)\n    # Plot the sine wave\n    ax.plot(x, y, color=color)\n    # Set radial label\n    ax.text(radial_label_position, 1.1, f'Color: {color}', va='bottom', ha='center')\n    # Return the color code\n    return color"}
{"task_id": "BigCodeBench/254", "solution": "import json\nimport math\ndef task_func(decimal_value, precision=2):\n    sqrt = round(math.sqrt(decimal_value), precision)\n    return json.dumps(sqrt)"}
{"task_id": "BigCodeBench/255", "solution": "import matplotlib\nimport numpy as np\nFUNCTIONS = [np.sin, np.cos, np.tan]\ndef task_func(ax, func_index):\n    if not isinstance(ax, matplotlib.axes._axes.Axes):\n        raise ValueError(\"Input ax is not an Axes object.\")\n    if func_index < 0 or func_index >= len(FUNCTIONS):\n        raise ValueError(\"Invalid function index.\")\n    theta = np.linspace(0, 2 * np.pi, 100)\n    r = FUNCTIONS[func_index](theta)\n    ax.plot(theta, r, label=FUNCTIONS[func_index].__name__)\n    ax.set_rticks([0.5, 1, 1.5])\n    ax.set_rlabel_position(-22.5)\n    ax.grid(True)\n    ax.legend()\n    return ax"}
{"task_id": "BigCodeBench/256", "solution": "import json\nimport random\nimport hashlib\nfrom datetime import datetime\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string\")\n    \n    random.seed(seed)\n    password = ''.join(random.choice('abcdefghijklmnopqrstuvwxyz0123456789') for _ in range(password_length))\n    \n    # Combine the salt and the UTC datetime string\n    combined = salt + utc_datetime.isoformat()\n    \n    # Hash the combined string using SHA-256\n    hashed_password = hashlib.sha256(combined.encode()).hexdigest()\n    \n    # Encode the hashed password as a JSON string\n    result = json.dumps(hashed_password)\n    \n    return result"}
{"task_id": "BigCodeBench/257", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(ax, num_turns):\n    \"\"\"\n    Draws a spiral on the polar diagram 'ax' with the specified number of turns 'num_turns'.\n    The spiral starts at the center and expands outward with each turn.\n    The radial ticks on the plot are positioned at intervals corresponding to the number of turns multiplied by 45 degrees.\n\n    Parameters:\n    ax (matplotlib.axes._axes.Axes): The axes object to draw the spiral on.\n    num_turns (int): The number of turns in the spiral.\n\n    Returns:\n    matplotlib.axes._axes.Axes: The modified Axes object with the spiral plot.\n    \"\"\"\n    # Generate angles for the spiral\n    theta = np.linspace(0, num_turns * 2 * np.pi, 1000)\n    # Calculate the radius for each angle\n    r = theta\n    # Convert to Cartesian coordinates\n    x = r * np.cos(theta)\n    y = r * np.sin(theta)\n    # Plot the spiral\n    ax.plot(x, y)\n    # Set radial ticks at intervals corresponding to num_turns * 45 degrees\n    ax.set_rticks(np.arange(0, num_turns * 45, 45) * np.pi / 180)\n    # Set the limits for the plot\n    ax.set_xlim(-10, 10)\n    ax.set_ylim(-10, 10)\n    # Set the aspect ratio to 'equal' for a circular plot\n    ax.set_aspect('equal')\n    # Return the modified axes object\n    return ax"}
{"task_id": "BigCodeBench/258", "solution": "import json\nimport random\nfrom datetime import datetime\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime\n    return json.dumps(person)"}
{"task_id": "BigCodeBench/259", "solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(ax, num_points):\n    if not isinstance(ax, matplotlib.axes.Axes):\n        raise ValueError(\"Input ax must be a matplotlib.axes.Axes object.\")\n    if num_points < 0:\n        raise ValueError(\"num_points must be a non-negative integer.\")\n    \n    # Generate random angles and radii\n    angles = np.random.uniform(0, 2 * np.pi, num_points)\n    radii = np.random.uniform(0, 1, num_points)\n    \n    # Plot the points\n    ax.scatter(angles, radii)\n    \n    # Set radial ticks\n    num_ticks = num_points // 10\n    if num_ticks < 1:\n        num_ticks = 1\n    ax.set_rticks(np.linspace(0, 1, num_ticks + 1)[1:])\n    \n    return ax"}
{"task_id": "BigCodeBench/260", "solution": "import json\nimport os\nimport glob\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\ndef task_func(directory):\n    # Initialize counter for updated files\n    updated_files = 0\n\n    # Use glob to find all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    for file in json_files:\n        # Open the JSON file\n        with open(file, 'r') as f:\n            data = json.load(f)\n\n        # Check if the key already exists\n        if KEY not in data:\n            # Add the new key-value pair\n            data[KEY] = VALUE\n\n            # Write the updated data back to the file\n            with open(file, 'w') as f:\n                json.dump(data, f, indent=4)\n            updated_files += 1\n\n    return updated_files"}
{"task_id": "BigCodeBench/261", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, radius):\n    if not ax.name == 'polar':\n        raise TypeError(\"The provided axes is not a polar plot.\")\n    if radius < 0:\n        raise ValueError(\"Radius cannot be negative.\")\n    theta = np.linspace(0, 2 * np.pi, 100)\n    x = radius * np.cos(theta)\n    y = radius * np.sin(theta)\n    ax.plot(x, y)\n    return ax"}
{"task_id": "BigCodeBench/262", "solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(dictionary, new_key, new_value):\n    # Add a new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n    \n    # Extract the values from the dictionary\n    values = list(dictionary.values())\n    \n    # Plot the distribution of the values using seaborn\n    plt.figure(figsize=(10,6))\n    sns.histplot(values, kde=True)\n    plt.title('Distribution of Values')\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    # Return the updated dictionary and the axes object of the plotted bar graph\n    return dictionary, plt.gca()"}
{"task_id": "BigCodeBench/263", "solution": "import os\nimport glob\nimport shutil\nimport time\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\ndef task_func(my_path: str, days_old: int) -> str:\n    # Create archive directory if it doesn't exist\n    archive_dir = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n    \n    # Get the current time\n    current_time = time.time()\n    \n    # Calculate the cutoff time\n    cutoff_time = current_time - (days_old * 86400)\n    \n    # Find files with specified extensions in the given directory\n    files = glob.glob(os.path.join(my_path, '*'))\n    for file in files:\n        if os.path.isfile(file):\n            # Get the file extension\n            _, ext = os.path.splitext(file)\n            if ext in FILE_EXTENSIONS:\n                # Get the file's modification time\n                file_stat = os.stat(file)\n                modification_time = file_stat.st_mtime\n                # Check if the file is older than 'days_old'\n                if modification_time < cutoff_time:\n                    # Move the file to the archive directory\n                    shutil.move(file, archive_dir)\n    \n    return archive_dir"}
{"task_id": "BigCodeBench/264", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"The provided value is not a number.\")\n    \n    dictionary[key] = value\n    mean = value\n    std = value / 2  # Assuming standard deviation is half of the mean for this example\n    np.random.seed(seed)\n    data = np.random.normal(loc=mean, scale=std, size=n)\n    series = pd.Series(data)\n    plt.hist(series, bins=bins)\n    plt.show()\n    return (dictionary, series)"}
{"task_id": "BigCodeBench/265", "solution": "import collections\nimport json\nimport os\ndef task_func(data, json_file_name='data.json'):\n    # Add a new key \"a\" with the value 1 to the input dictionary\n    data['a'] = 1\n\n    # Calculate the frequency of its values\n    freq = collections.Counter(data.values())\n\n    # Save the updated dictionary along with its frequency distribution to a JSON file\n    with open(json_file_name, 'w') as f:\n        json.dump({'data': data, 'freq': dict(freq)}, f)\n\n    # Output the path of the JSON file\n    return json_file_name"}
{"task_id": "BigCodeBench/266", "solution": "import os\nimport os.path\nimport csv\nimport collections\nFILE_NAME = 'file_sizes.csv'\ndef task_func(my_path):\n    # Create a list to hold the file sizes\n    file_sizes = []\n    \n    # Walk through the directory tree\n    for dirpath, dirnames, filenames in os.walk(my_path):\n        for filename in filenames:\n            # Get the full path of the file\n            filepath = os.path.join(dirpath, filename)\n            # Get the size of the file\n            size = os.path.getsize(filepath)\n            # Append the file path and size to the list\n            file_sizes.append((filepath, size))\n    \n    # Write the file sizes to a CSV file\n    with open(FILE_NAME, mode='w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        # Write the header\n        writer.writerow(['File Path', 'Size'])\n        # Write the file sizes\n        for filepath, size in file_sizes:\n            writer.writerow([filepath, size])\n    \n    # Return the path of the CSV file\n    return FILE_NAME"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Adds a new key \"a\" with the value 1 to the dictionary.\n    data['a'] = 1\n\n    # 2. Generates a signal based on the values in \"data\".\n    # Assuming the signal is a simple sine wave for demonstration\n    time = np.arange(0, len(data), 1)\n    signal = np.sin(2 * np.pi * 5 * time / sample_rate)\n\n    # 3. Runs a Fast Fourier Transform (FFT) on the signal.\n    fft_result = fftpack.fft(signal)\n\n    # 4. Plots and returns the FFT of the signal.\n    freqs = fftpack.fftfreq(len(signal)) * sample_rate\n    plt.plot(freqs, np.abs(fft_result))\n    plt.xlabel('Frequency [Hz]')\n    plt.ylabel('Amplitude')\n    plt.title('FFT of the Signal')\n    plt.show()\n\n    return fft_result, plt.gca()\ndata = {'key1': 10, 'key2': 20}"}
{"task_id": "BigCodeBench/268", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\ndef task_func(n_keys, n_values):\n    # Create a list of random keys from the predefined list\n    keys = random.sample(LETTERS, n_keys)\n    \n    # Create a dictionary with keys and values\n    # Values are lists of integers starting from 1 with length n_values\n    result = {key: list(range(1, n_values + 1)) for key in keys}\n    \n    return result"}
{"task_id": "BigCodeBench/269", "solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # 1. Adds a key \"a\" with a value of 1.\n    data_dict[\"a\"] = 1\n\n    # 2. Conducts statistical analysis on its values (mean, median, mode), by rounding the mean to 2 decimal places.\n    values = list(data_dict.values())\n    mean_val = round(np.mean(values), 2)\n    median_val = np.median(values)\n    mode_val = stats.mode(values)[0][0] if len(stats.mode(values)[0]) > 0 else None\n    stats_dict = {\"mean\": mean_val, \"median\": median_val, \"mode\": mode_val}\n\n    # 3. Normalizes the values using MinMaxScaler to a range of (0, 1).\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    normalized_values = scaler.fit_transform(np.array(values).reshape(-1, 1)).flatten()\n\n    # 4. Plots a histogram of the normalized values, with the title \"Histogram of Normalized Values\", and x labels \"Value\" and y labels \"Frequency\".\n    plt.hist(normalized_values, bins=10, color='skyblue', edgecolor='black')\n    plt.title(\"Histogram of Normalized Values\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    # Return the processed dictionary, statistical properties, and the histogram plot\n    return (data_dict, stats_dict, plt)"}
{"task_id": "BigCodeBench/270", "solution": "import re\nfrom collections import Counter\ndef task_func(sentence):\n    # Use regular expression to find all words in the sentence\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    # Use Counter to count the occurrences of each word\n    word_counts = Counter(words)\n    # Return the dictionary of word counts\n    return dict(word_counts)"}
{"task_id": "BigCodeBench/271", "solution": "import random\nimport string\nimport hashlib\nimport time\ndef task_func(data_dict: dict, seed=0) -> dict:\n    # Add a key \"a\" with a value of 1\n    data_dict[\"a\"] = 1\n    \n    # Generate a random salt of length 5 using lowercase ASCII letters\n    random.seed(seed)\n    salt = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))\n    \n    # For each key-value pair in the dictionary, concatenate the value with the generated salt,\n    # hash the concatenated string using SHA-256, and update the value with the hashed string\n    for key in data_dict:\n        if key != \"a\":\n            value = str(data_dict[key])\n            concatenated = value + salt\n            hashed = hashlib.sha256(concatenated.encode()).hexdigest()\n            data_dict[key] = hashed\n    \n    # Add a 'timestamp' key with the current UNIX timestamp as its value\n    data_dict['timestamp'] = int(time.time())\n    \n    return data_dict"}
{"task_id": "BigCodeBench/272", "solution": "import cgi\nimport http.server\nimport json\ndef task_func():\n    class JSONHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Content-Type header is not application/json\"}).encode('utf-8'))\n                return\n\n            content_length = int(self.headers.get('Content-Length', 0))\n            post_data = self.rfile.read(content_length)\n            try:\n                data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"Invalid JSON\"}).encode('utf-8'))\n                return\n\n            if 'data' not in data:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(json.dumps({\"status\": \"error\", \"message\": \"No data received\"}).encode('utf-8'))\n                return\n\n            self.send_response(200)\n            self.end_headers()\n            self.wfile.write(json.dumps({\"status\": \"success\", \"message\": \"Data received successfully.\"}).encode('utf-8'))\n\n    return JSONHandler"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_type = self.headers.get('Content-Type')\n        if content_type != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Content-Type header is not application/json')\n            return\n        length = int(self.headers.get('Content-Length', 0))\n        data = self.rfile.read(length)\n        try:\n            json_data = json.loads(data)\n            if 'data' in json_data:\n                self.send_response(200)\n                self.end_headers()\n                self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\n            else:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(b'No data key in request')\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\ndef task_func():\n    http.server.test(HandlerClass=RequestHandler, port=8000)"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            try:\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(b'Invalid JSON')\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            for key in required_keys:\n                if key not in email_data:\n                    self.send_response(400)\n                    self.end_headers()\n                    self.wfile.write(f'Missing {key} in email data'.encode())\n                    return\n\n            try:\n                server = smtplib.SMTP(smtp_server, smtp_port)\n                server.starttls()\n                server.login(smtp_username, smtp_password)\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n                server.sendmail(smtp_username, [email_data['to']], msg.as_string())\n                server.quit()\n                self.send_response(200)\n                self.end_headers()\n                self.wfile.write(b'Email sent successfully')\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.end_headers()\n                self.wfile.write(b'Authentication Failed')\n            except Exception as e:\n                self.send_response(500)\n                self.end_headers()\n                self.wfile.write(f'Internal Server Error: {str(e)}'.encode())\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/275", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input n must be greater than or equal to 1.\")\n    numbers = list(range(1, n+1))\n    pairs = list(combinations(numbers, 2))\n    return pairs"}
{"task_id": "BigCodeBench/276", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    # Calculate the maximum values of each row\n    max_values = np.max(matrix, axis=1)\n    \n    # Calculate the histogram of the maximum values\n    hist, bins = np.histogram(max_values, bins=30, density=True)\n    \n    # Estimate the core density of the distribution\n    core_density = stats.gaussian_kde(max_values)\n    \n    # Calculate skew and kurtosis\n    skew = stats.skew(max_values)\n    kurt = stats.kurtosis(max_values)\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.bar(bins[:-1], hist, width=np.diff(bins), align='edge')\n    ax.set_title('Histogram of Maximum Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return skew, kurt, ax\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": "BigCodeBench/277", "solution": "import random\nfrom itertools import combinations\nimport math\ndef task_func(n):\n    if n < 2:\n        return None\n    dots = [(random.random(), random.random()) for _ in range(n)]\n    closest_pair = None\n    min_distance = float('inf')\n    for pair in combinations(dots, 2):\n        distance = math.sqrt((pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2)\n        if distance < min_distance:\n            min_distance = distance\n            closest_pair = pair\n    return closest_pair"}
{"task_id": "BigCodeBench/278", "solution": "import numpy as np\nfrom sympy import symbols, solve\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a = np.random.randint(-10, 10)\n    b = np.random.randint(-10, 10)\n    c = np.random.randint(-10, 10)\n    x = symbols('x')\n    solutions = solve(a*x**2 + b*x + c, x)\n    solutions = [round(complex(sol.evalf()), precision) for sol in solutions]\n    return tuple(solutions)"}
{"task_id": "BigCodeBench/279", "solution": "import random\nfrom collections import Counter\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\ndef task_func(x=1):\n    hands = []\n    drawn_cards = []\n    for _ in range(x):\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n        drawn_cards.extend(hand)\n    return (hands, Counter(drawn_cards))"}
{"task_id": "BigCodeBench/280", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(signal, precision=2, seed=777):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate a random signal if none is provided\n    if signal is None:\n        signal = np.random.rand(10)\n    \n    # Calculate the DFT using FFT\n    transformed_signal = fft(signal)\n    \n    # Round the transformed signal values to the specified precision\n    transformed_signal = np.round(transformed_signal, precision)\n    \n    # Plot the original signal\n    fig, ax = plt.subplots()\n    ax.plot(signal)\n    ax.set_title('Original Signal')\n    plt.show()\n    \n    # Plot the transformed signal\n    fig, ax = plt.subplots()\n    ax.plot(transformed_signal)\n    ax.set_title('Transformed Signal')\n    plt.show()\n    \n    # Return the transformed signal and the axes objects for the plots\n    return transformed_signal, (ax, ax)"}
{"task_id": "BigCodeBench/281", "solution": "import re\nimport os\nfrom collections import Counter\ndef task_func(folder_path: str) -> dict:\n    ip_pattern = re.compile(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b')\n    ip_counts = Counter()\n    \n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith('.log'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    content = f.read()\n                    ips = ip_pattern.findall(content)\n                    ip_counts.update(ips)\n    \n    return dict(ip_counts)"}
{"task_id": "BigCodeBench/282", "solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    image = cv2.imread(file_path)\n    if image is None:\n        raise ValueError(f\"Could not read the image from {file_path}.\")\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    r, g, b = cv2.split(image)\n    \n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    \n    hist, xedges, yedges = np.histogram2d(r.ravel(), g.ravel(), bins=256, range=[[0,256], [0,256]])\n    hist, yedges, zedges = np.histogram2d(g.ravel(), b.ravel(), bins=256, range=[[0,256], [0,256]])\n    hist, xedges, zedges = np.histogram2d(r.ravel(), b.ravel(), bins=256, range=[[0,256], [0,256]])\n    \n    X, Y = np.meshgrid(xedges[:-1], yedges[:-1])\n    Z = np.zeros_like(X)\n    \n    ax.bar3d(X, Y, Z, 1, 1, hist, shade=True)\n    \n    fig.canvas.mpl_connect('button_press_event', onpick)\n    \n    return ax"}
{"task_id": "BigCodeBench/283", "solution": "import os\nimport json\nfrom collections import Counter\ndef task_func(json_files_path='./json_files/', key='name'):\n    \"\"\"\n    Count the occurrence of a particular key in all JSON files in a specified directory\n    and return a dictionary with the values of the specified key and their counts.\n\n    Args:\n    json_files_path (str): The path to the directory containing JSON files.\n    key (str): The key to count occurrences of in the JSON files.\n\n    Returns:\n    dict: A dictionary with values of the key as keys and their counts as values.\n    \"\"\"\n    result = Counter()\n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_files_path, filename), 'r') as file:\n                data = json.load(file)\n                if key in data:\n                    result[data[key]] += 1\n    return dict(result)"}
{"task_id": "BigCodeBench/284", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\ndef task_func(url):\n    # Initialize a browser object\n    br = mechanize.Browser()\n    # Open the URL\n    response = br.open(url)\n    # Read the content of the page\n    content = response.read()\n    # Parse the content with BeautifulSoup\n    soup = BeautifulSoup(content, 'html.parser')\n    # Find all 'a' tags\n    a_tags = soup.find_all('a')\n    # Extract href attributes\n    hyperlinks = [tag['href'] for tag in a_tags if 'href' in tag.attrs]\n    # Combine with base URL to get absolute URLs\n    absolute_hyperlinks = [urljoin(url, link) for link in hyperlinks]\n    return absolute_hyperlinks"}
{"task_id": "BigCodeBench/285", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\ndef task_func(url, form_id, data):\n    br = mechanize.Browser()\n    br.set_handle_robots(False)\n    br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n    br.open(url)\n    for form in br.forms():\n        if form.attrs.get('id') == form_id:\n            br.form = form\n            break\n    else:\n        return 'No Form Found'\n    br.form.set_all_readonly(False)\n    for name, value in data.items():\n        br.form[name] = value\n    response = br.submit()\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    title = soup.title.string if soup.title else 'No Title'\n    return title"}
{"task_id": "BigCodeBench/286", "solution": "from collections import Counter\nimport os\nimport csv\ndef task_func(output_file, test_directory):\n    try:\n        # Initialize a counter to keep track of total words\n        total_words = Counter()\n        \n        # List all files in the directory\n        files = os.listdir(test_directory)\n        \n        # Iterate over each file\n        for file in files:\n            # Check if the file is a .txt file\n            if file.endswith('.txt'):\n                # Open the file and read its content\n                with open(os.path.join(test_directory, file), 'r') as f:\n                    content = f.read()\n                    # Split the content into words and update the counter\n                    words = content.split()\n                    total_words.update(words)\n        \n        # Write the counts to a CSV file\n        with open(output_file, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow([\"Word\", \"Count\"])\n            for word, count in total_words.items():\n                writer.writerow([word, count])\n        \n        # Return the total number of words\n        return sum(total_words.values())\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    total_words = 0\n    file_counts = {}\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".txt\"):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    words = f.read().split()\n                    total_words += len(words)\n                    file_counts[file] = len(words)\n    with open(filename, 'w') as f:\n        json.dump(file_counts, f)\n    return total_words"}
{"task_id": "BigCodeBench/288", "solution": "import collections\nimport json\nimport os\ndef task_func(directory_path: str) -> dict:\n    # Initialize a counter to keep track of key occurrences\n    key_counter = collections.Counter()\n\n    # List all files in the directory\n    files = os.listdir(directory_path)\n\n    # Iterate over each file in the directory\n    for file in files:\n        # Check if the file is a JSON file\n        if file.endswith('.json'):\n            # Construct the full file path\n            file_path = os.path.join(directory_path, file)\n            # Open and read the JSON file\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            # Update the counter with the keys from the JSON data\n            key_counter.update(data.keys())\n\n    # Convert the counter to a dictionary\n    result = dict(key_counter)\n    return result"}
{"task_id": "BigCodeBench/289", "solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Normalize the data\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Initialize a list to store the history of each fold\n    history_list = []\n    \n    # Initialize KFold\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    # Iterate over each fold\n    for fold, (train_index, test_index) in enumerate(kf.split(X_scaled)):\n        print(f'Fold {fold + 1}/{n_splits}')\n        \n        # Split the data into train and test sets for this fold\n        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # Define the model\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(20, activation='relu', input_shape=(X_train.shape[1],)),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n        \n        # Compile the model\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        \n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), verbose=0)\n        \n        # Append the history to the list\n        history_list.append(history.history)\n    \n    return history_list"}
{"task_id": "BigCodeBench/290", "solution": "import nltk\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(directory_path):\n    unique_words = set()\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            if file.endswith('.txt'):\n                with open(os.path.join(root, file), 'r') as f:\n                    words = f.read().split()\n                    words = [word.lower() for word in words if word.isalpha()]\n                    unique_words.update(words)\n    return len(unique_words - STOPWORDS)"}
{"task_id": "BigCodeBench/291", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, 1000)\n    sns.histplot(samples, kde=True, stat='density', color='skyblue', edgecolor='black')\n    plt.title('Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/292", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Create a copy of the DataFrame to avoid modifying the original\n    df_scaled = df.copy()\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Scale the 'Age' and 'Income' columns for each group by 'id'\n    for group_id, group_df in df_scaled.groupby('id'):\n        # Scale 'Age' and 'Income' within the group\n        group_df[['Age', 'Income']] = scaler.fit_transform(group_df[['Age', 'Income']])\n        # Update the original DataFrame with scaled values\n        df_scaled.loc[group_df.index] = group_df\n    \n    # Create a histogram of the 'Income' column after scaling\n    plt.hist(df_scaled['Income'], bins=30, color='skyblue', edgecolor='black')\n    plt.title('Histogram of Scaled Income')\n    plt.xlabel('Scaled Income')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    # Return the scaled DataFrame and the histogram data\n    return df_scaled, plt.gcf()\ndata = {\n    'id': [1, 1, 2, 2],\n    'Age': [25, 35, 22, 32],\n    'Income': [50000, 60000, 45000, 55000]\n}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/293", "solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(elements, subset_size):\n    # Generate all subsets of the given size\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sums of the subsets\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Draw a histogram of the sums\n    plt.hist(subset_sums, bins=10, edgecolor='black')\n    plt.title('Histogram of Subset Sums')\n    plt.xlabel('Sum of Subsets')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    # Return the Axes object of the plotted histogram\n    # and the combinations of subsets and their sums\n    return plt.gca(), subsets, subset_sums\nelements = (1, 2, 3)\nsubset_size = 2"}
{"task_id": "BigCodeBench/294", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    if 'id' not in df.columns or 'age' not in df.columns or 'income' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'id', 'age', and 'income' columns.\")\n    \n    scaler = StandardScaler()\n    \n    # Standardize 'age' and 'income' for each group by 'id'\n    df['age'] = df.groupby('id')['age'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1,1)).flatten())\n    df['income'] = df.groupby('id')['income'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1,1)).flatten())\n    \n    return df"}
{"task_id": "BigCodeBench/295", "solution": "import itertools\nimport statistics\ndef task_func(elements, subset_size):\n    # Generate all possible subsets of the given size\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sum of each subset\n    subset_sums = [sum(subset) for subset in subsets]\n    \n    # Calculate mean, median, and mode of the sums\n    mean = statistics.mean(subset_sums)\n    median = statistics.median(subset_sums)\n    mode = statistics.mode(subset_sums)\n    \n    # Return the results in a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode\n    }"}
{"task_id": "BigCodeBench/296", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    if df.empty:\n        return plt.gca()\n    \n    value_counts = df['value'].value_counts()\n    axes = value_counts.plot(kind='bar')\n    axes.set_title('Value Distribution')\n    axes.set_xlabel('Value')\n    axes.set_ylabel('Count')\n    \n    return axes"}
{"task_id": "BigCodeBench/297", "solution": "import itertools\nimport collections\ndef task_func(elements, subset_size):\n    # Generate all possible subsets of the given size\n    subsets = itertools.combinations(elements, subset_size)\n    \n    # Calculate the sum of each subset and count their occurrences\n    sum_counts = collections.defaultdict(int)\n    for subset in subsets:\n        subset_sum = sum(subset)\n        sum_counts[subset_sum] += 1\n    \n    # Convert the defaultdict to a regular dict for the output\n    result = dict(sum_counts)\n    \n    return result"}
{"task_id": "BigCodeBench/298", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    \"\"\"\n    Splits a list in the 'Value' column of a DataFrame into several columns,\n    scales these columns using StandardScaler, and optionally returns the scaled data\n    using a bar chart. The 'Date' column is converted to datetime and used as the index in the plot.\n\n    Parameters:\n    - df: pandas DataFrame with at least 'Date' and 'Value' columns.\n    - plot: boolean, if True, returns a matplotlib Axes object with the bar chart.\n\n    Returns:\n    - DataFrame: A pandas DataFrame with the 'Date' column and additional columns for each element in the original 'Value' list,\n                 where these columns contain the scaled values.\n    - Axes (optional): A matplotlib Axes object containing the bar chart, returned if 'plot' is True.\n\n    Raises:\n    - KeyError: If the DataFrame does not have the 'Date' and 'Value' columns.\n    \"\"\"\n    # Check if 'Date' and 'Value' columns are present\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns.\")\n\n    # Convert 'Date' column to datetime and set as index\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # Split 'Value' into multiple columns\n    df_split = df['Value'].apply(pd.Series)\n\n    # Scale the split columns using StandardScaler\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df_split)\n    scaled_df = pd.DataFrame(scaled_data, columns=df_split.columns, index=df_split.index)\n\n    # Optionally plot the scaled data\n    if plot:\n        ax = scaled_df.plot(kind='bar')\n        ax.set_title('Scaled Values Over Time')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        return scaled_df, ax\n    else:\n        return scaled_df"}
{"task_id": "BigCodeBench/299", "solution": "import itertools\nimport math\nfrom pandas import Series\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements):\n        return 1, Series()\n    if subset_size == 0:\n        return 1, Series()\n    \n    subsets = list(itertools.combinations(elements, subset_size))\n    subset_sums = [sum(subset) for subset in subsets]\n    product_of_sums = math.prod(subset_sums)\n    \n    top_n_sums = Series(subset_sums).nlargest(top_n)\n    \n    return product_of_sums, top_n_sums"}
{"task_id": "BigCodeBench/300", "solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if 'Date' and 'Value' columns exist\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"DataFrame must contain 'Date' and 'Value' columns.\")\n    \n    # Convert 'Date' to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Split 'Value' lists into separate columns\n    df = df.explode('Value')\n    \n    # Calculate Z-scores\n    df['Z-Score'] = zscore(df['Value'])\n    \n    # Reshape the DataFrame to have 'Date' and 'Z-Score' columns\n    df = df[['Date', 'Z-Score']]\n    \n    # Create a box plot of Z-scores over time\n    plt.figure(figsize=(10, 6))\n    plt.boxplot(df['Z-Score'], vert=False)\n    plt.title('Z-Scores Over Time')\n    plt.xlabel('Z-Score')\n    plt.ylabel('Date')\n    plt.show()\n    \n    # Return the DataFrame with Z-scores\n    return df"}
{"task_id": "BigCodeBench/301", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the input date string\n    date = parse(date_str)\n    \n    # Convert the date to the target timezone\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    date = date.replace(tzinfo=from_tz)\n    date = date.astimezone(to_tz)\n    \n    # Calculate the year since the closest solar cycle year\n    closest_year = min(SOLAR_CYCLE_YEARS, key=lambda x: abs(x - date.year))\n    years_since_cycle = date.year - closest_year\n    \n    # Calculate the solar activity using a cosine function\n    solar_activity = 0.5 * (1 + math.cos(2 * math.pi * years_since_cycle / 11))\n    \n    return solar_activity"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, plot=False):\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n    \n    if 'Value' not in df.columns:\n        raise ValueError(\"Input DataFrame must contain a 'Value' column.\")\n    \n    if not all(isinstance(x, list) for x in df['Value']):\n        raise ValueError(\"The 'Value' column must contain lists.\")\n    \n    # Flatten the lists in the 'Value' column\n    flattened = [item for sublist in df['Value'] for item in sublist]\n    \n    # Create a new DataFrame with the flattened values\n    df_flattened = pd.DataFrame({'Value': flattened})\n    \n    # Calculate the Pearson correlation matrix\n    corr_matrix = df_flattened.corr(method='pearson')\n    \n    if plot:\n        # Create a heatmap of the correlation matrix\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title('Correlation Heatmap')\n        plt.show()\n    \n    return corr_matrix"}
{"task_id": "BigCodeBench/303", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the input date string\n    date = parse(date_str)\n    \n    # Convert the date to the target timezone\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    date = date.replace(tzinfo=from_tz)\n    date = date.astimezone(to_tz)\n    \n    # Calculate the number of days since the reference year\n    reference_year = MOON_PHASES_YEARS[0]\n    days_since_reference = (date.year - reference_year) * 365 + date.timetuple().tm_yday - 1\n    \n    # Calculate the moon phase\n    # The formula for the moon phase is based on the number of days since the new moon\n    # and the synodic month (approximately 29.53 days)\n    synodic_month = 29.53058867\n    days_since_new_moon = days_since_reference % synodic_month\n    moon_phase = days_since_new_moon / synodic_month\n    \n    # Normalize the moon phase to be between 0 and 1\n    moon_phase = (moon_phase + 0.5) % 1\n    \n    return moon_phase"}
{"task_id": "BigCodeBench/304", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if df.empty:\n        return (0, 0)\n    \n    # Convert lists in the DataFrame to separate columns\n    for col in df.columns:\n        if df[col].dtype == 'object' and df[col].str.contains(',').any():\n            df[col] = df[col].str.split(',').apply(pd.Series, 1).stack()\n            df[col] = df[col].str.strip()\n            df = df.reset_index(level=1, drop=True)\n    \n    # Perform PCA\n    pca = PCA()\n    pca.fit(df)\n    \n    # Get explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n    \n    # Create bar chart\n    ax = plt.subplots()[1]\n    ax.bar(range(len(explained_variance_ratio)), explained_variance_ratio, alpha=0.5, align='center')\n    ax.set_title('Explained Variance Ratio of Principal Components')\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n    \n    return (explained_variance_ratio, ax)"}
{"task_id": "BigCodeBench/305", "solution": "from collections import Counter\nimport itertools\nimport random\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    for i in range(len(list_of_lists)):\n        if not list_of_lists[i]:\n            list_of_lists[i] = random.sample(ALPHABET, k=10)\n    all_letters = list(itertools.chain.from_iterable(list_of_lists))\n    return Counter(all_letters)"}
{"task_id": "BigCodeBench/306", "solution": "import os\nimport logging\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    removed_files = []\n    for filename in os.listdir(directory):\n        if 'jquery' in filename.lower():\n            file_path = os.path.join(directory, filename)\n            try:\n                os.remove(file_path)\n                removed_files.append(filename)\n            except Exception as e:\n                logging.error(f\"Error removing {filename}: {e}\")\n    \n    return len(removed_files), removed_files"}
{"task_id": "BigCodeBench/307", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    for i in range(len(list_of_lists)):\n        if len(list_of_lists[i]) == 0:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(5)]\n    combined_data = [item for sublist in list_of_lists for item in sublist]\n    plt.hist(combined_data, bins=10, color='skyblue', edgecolor='black')\n    plt.title('Histogram of Combined Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    return plt.gca()\nlist_of_lists = [[1, 2, 3], [], [4, 5], [6, 7, 8, 9, 10]]"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Create a dictionary to hold the data\n    data = {field: [] for field in FIELDS}\n    for student in STUDENTS:\n        for field in FIELDS:\n            grade = random.randint(0, 100)\n            data[field].append(grade)\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject\n    avg_subjects = df.mean()\n    avg_subjects.name = 'Average'\n    \n    # Append the average grades to the DataFrame\n    df = df.append(avg_subjects)\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/309", "solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    for i, lst in enumerate(list_of_lists):\n        if not lst:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(5)]\n    scaler = MinMaxScaler()\n    list_of_lists = [scaler.fit_transform(np.array(lst).reshape(-1,1)).flatten().tolist() for lst in list_of_lists]\n    return list_of_lists"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Create a list to hold the data for all people\n    people_data = []\n    \n    # Generate data for each person\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{random.randint(1, 1000)}\"\n        age = random.randint(18, 80)\n        height = random.uniform(1.5, 2.0)\n        weight = random.uniform(50, 100)\n        people_data.append([name, age, height, weight])\n    \n    # Calculate averages\n    ages = [person[1] for person in people_data]\n    heights = [person[2] for person in people_data]\n    weights = [person[3] for person in people_data]\n    avg_age = mean(ages)\n    avg_height = mean(heights)\n    avg_weight = mean(weights)\n    \n    # Append averages to the data\n    people_data.append([f\"Average\", avg_age, avg_height, avg_weight])\n    \n    # Write to CSV\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(people_data)\n    \n    return filename"}
{"task_id": "BigCodeBench/311", "solution": "import numpy as np\nimport random\nfrom scipy import stats\ndef task_func(list_of_lists, size=5, seed=0):\n    # Set the seed for reproducibility\n    random.seed(seed)\n    \n    # Initialize an empty list to store all values\n    all_values = []\n    \n    # Iterate through each sublist in the list of lists\n    for sublist in list_of_lists:\n        # If the sublist is empty, fill it with 'size' random integers between 0 and 100\n        if not sublist:\n            sublist = [random.randint(0, 100) for _ in range(size)]\n        \n        # Extend the all_values list with the sublist\n        all_values.extend(sublist)\n    \n    # Calculate mean, median, and mode\n    mean = np.mean(all_values)\n    median = np.median(all_values)\n    mode = stats.mode(all_values)\n    \n    # Return the results in a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode\n    }"}
{"task_id": "BigCodeBench/312", "solution": "import random\nimport matplotlib.pyplot as plt\nDISTRIBUTION_SIZE = 1000\ndef task_func(bins=30):\n    # Generate a Gaussian distribution\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    hist = ax.hist(distribution, bins=bins)\n    \n    # Return the distribution list and the Axes patch object of the histogram plot\n    return (distribution, hist)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Create a dictionary to hold the files organized by their first text outside of square brackets\n    organized_files = {}\n    \n    # Walk through all files in the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Read the first line of the file\n            with open(file_path, 'r') as f:\n                first_line = f.readline()\n            # Use regular expression to find the first text not enclosed in square brackets\n            match = re.search(r'(?<=\\[).+?(?=\\])', first_line)\n            if match:\n                # Extract the text\n                text = match.group().strip()\n                # If the text is not already a key in the dictionary, add it\n                if text not in organized_files:\n                    organized_files[text] = []\n                # Move the file to the corresponding subdirectory\n                sub_dir = os.path.join(directory, text)\n                if not os.path.exists(sub_dir):\n                    os.makedirs(sub_dir)\n                shutil.move(file_path, os.path.join(sub_dir, file))\n                organized_files[text].append(file)\n            else:\n                # If no matching text is found, do not move the file\n                pass\n    \n    # Return the directory path with organized files and the dictionary of organized files\n    return (directory, organized_files)"}
{"task_id": "BigCodeBench/314", "solution": "import socket\nimport ssl\nimport http.client\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    try:\n        # Create a socket object\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        \n        # Wrap the socket with SSL context\n        context = ssl.create_default_context()\n        context.check_hostname = False\n        context.verify_mode = ssl.CERT_NONE\n        secure_sock = context.wrap_socket(sock, server_hostname=SERVER_NAME)\n        \n        # Connect to the server\n        secure_sock.connect((SERVER_NAME, SERVER_PORT))\n        \n        # Send the GET request\n        request = f\"GET {path} HTTP/1.1\\r\\nHost: {SERVER_NAME}\\r\\n\\r\\n\"\n        secure_sock.sendall(request.encode())\n        \n        # Receive the response\n        response = b\"\"\n        while True:\n            data = secure_sock.recv(1024)\n            if not data:\n                break\n            response += data\n        \n        # Close the connection\n        secure_sock.close()\n        \n        # Decode the response body to string\n        response_body = response.decode('utf-8')\n        \n        return response_body\n    except ssl.SSLError as e:\n        # Raise the exception for SSL handshake error\n        raise e\n    except Exception as e:\n        # Handle other exceptions\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/315", "solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\ndef task_func(dir, api_key, recipient_email):\n    try:\n        if not os.path.exists(dir):\n            raise FileNotFoundError(f\"The directory {dir} does not exist.\")\n        \n        files = os.listdir(dir)\n        message = Mail(\n            from_email='your_email@example.com',\n            to_emails=recipient_email,\n            subject='List of files in directory',\n            plain_text_content='\\n'.join(files)\n        )\n        \n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n        \n        if response.status_code // 100 == 2:\n            return True\n        else:\n            raise HTTPError(f\"HTTP error occurred: {response.status_code}\")\n    \n    except FileNotFoundError as e:\n        print(e)\n        return False\n    except HTTPError as e:\n        print(e)\n        return False\n    except Exception as e:\n        print(e)\n        return False"}
{"task_id": "BigCodeBench/316", "solution": "import pandas as pd\nimport random\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Generate a category distribution within a specified range and return as a DataFrame.\n\n    Parameters:\n    - value_range (tuple): A tuple of two integers representing the range (min, max) for generating values.\n\n    Returns:\n    - DataFrame: A pandas DataFrame with two columns: 'Category' and 'Count', representing the category names and their counts.\n    \"\"\"\n    # Generate a list of random values within the specified range\n    values = [random.randint(value_range[0], value_range[1]) for _ in range(1000)]\n    \n    # Count the occurrences of each category based on the generated values\n    category_counts = {}\n    for value in values:\n        if value < 20:\n            category = 'A'\n        elif 20 <= value < 40:\n            category = 'B'\n        elif 40 <= value < 60:\n            category = 'C'\n        elif 60 <= value < 80:\n            category = 'D'\n        else:\n            category = 'E'\n        if category in category_counts:\n            category_counts[category] += 1\n        else:\n            category_counts[category] = 1\n    \n    # Convert the counts to a DataFrame\n    df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n    \n    return df"}
{"task_id": "BigCodeBench/317", "solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\ndef task_func(example_str):\n    # Extract texts not enclosed in square brackets\n    pattern = r'(?<=\\]).*?(?=\\[)|^.*?(?=\\[)|(?<=\\]).*$'\n    texts = re.findall(pattern, example_str)\n    texts = [text.strip() for text in texts if text.strip()]\n\n    # Calculate TF-IDF values\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(texts)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Calculate TF-IDF scores\n    tfidf_scores = {}\n    for i, text in enumerate(texts):\n        tfidf_scores[text] = {}\n        for j, feature in enumerate(feature_names):\n            tfidf_scores[text][feature] = tfidf_matrix[i, j]\n\n    return tfidf_scores"}
{"task_id": "BigCodeBench/318", "solution": "import random\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(points_count=1000, radius=1):\n    # Generate random points within a circle\n    points = []\n    for _ in range(points_count):\n        angle = random.uniform(0, 2 * math.pi)\n        distance = random.uniform(0, radius)\n        x = distance * math.cos(angle)\n        y = distance * math.sin(angle)\n        points.append((x, y))\n    \n    # Plot the points\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n    ax.set_aspect('equal')\n    \n    return ax"}
{"task_id": "BigCodeBench/319", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\ndef task_func(example_str, top_n=30):\n    # Extract all texts that are not enclosed in square brackets\n    pattern = r'(?:(?!\\[).)*?(?:(?!\\]).)*?'\n    texts = re.findall(pattern, example_str)\n    # Join the extracted texts into a single string\n    text = ' '.join(texts)\n    # Tokenize the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    # Calculate the frequency distribution of words\n    fdist = FreqDist(words)\n    # Get the top_n most common words\n    top_words = fdist.most_common(top_n)\n    # Plot the frequency distribution\n    fdist.plot(top_n, cumulative=False)\n    # Return the top_n most common words as a dictionary\n    top_words_dict = dict(top_words)\n    return plt.gca(), top_words_dict"}
{"task_id": "BigCodeBench/320", "solution": "import subprocess\nimport os\nimport random\ndef task_func(directory, file_list):\n    if not file_list:\n        return None\n    random_file = random.choice(file_list)\n    file_path = os.path.join(directory, random_file)\n    if not os.path.isfile(file_path):\n        return None\n    try:\n        process = subprocess.Popen(file_path, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        return process.returncode\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/321", "solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Extract names not surrounded by square brackets\n    pattern = r'(?<!\\[)[A-Za-z]+(?!\\])'\n    names = re.findall(pattern, text)\n    \n    # Count the frequency of each name\n    name_freq = pd.Series(names).value_counts()\n    \n    # Create a bar chart of the name frequencies\n    if not name_freq.empty:\n        ax = name_freq.plot(kind='bar')\n    else:\n        ax = None\n    \n    # Calculate skewness and kurtosis\n    skew = name_freq.skew() if not name_freq.empty else None\n    kurt = name_freq.kurt() if not name_freq.empty else None\n    \n    return name_freq, ax, skew, kurt"}
{"task_id": "BigCodeBench/322", "solution": "import subprocess\nimport os\nimport shutil\nimport sys\nDIRECTORY = 'c:\\Program Files\\VMware\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\Program Files\\VMware\\VMware Server\\Backup'\ndef task_func(filename):\n    # Check if the file exists in the directory\n    filepath = os.path.join(DIRECTORY, filename)\n    if not os.path.exists(filepath):\n        print(f\"File {filename} does not exist in {DIRECTORY}\")\n        return -1\n\n    # Create backup directory if it doesn't exist\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Backup the file\n    backup_path = os.path.join(BACKUP_DIRECTORY, filename)\n    try:\n        shutil.copy2(filepath, backup_path)\n    except Exception as e:\n        print(f\"Failed to backup {filename}: {e}\")\n        return -1\n\n    # Execute the file as a subprocess\n    try:\n        result = subprocess.run([filepath], capture_output=True, text=True)\n        print(f\"Subprocess output: {result.stdout}\")\n        return result.returncode\n    except Exception as e:\n        print(f\"Failed to execute {filename}: {e}\")\n        return -1"}
{"task_id": "BigCodeBench/323", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\ndef task_func(text, num_gaussians=1, seed=42):\n    # Extract names from a string that aren't enclosed by square brackets\n    pattern = r'(?<!\\[)[A-Za-z]+(?!\\])'\n    names = re.findall(pattern, text)\n    \n    # Tokenize the names into words\n    words = [name for name in names if name.isalpha()]\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Fit a mixture of num_gaussians 1-D Gaussian distributions to the word frequencies\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians must be greater than 0\")\n    if num_gaussians > len(word_counts):\n        raise Exception(\"num_gaussians cannot be greater than the number of unique words\")\n    \n    frequencies = np.array(list(word_counts.values()))\n    frequencies = frequencies.reshape(-1, 1)\n    \n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    gmm.fit(frequencies)\n    \n    # Return the means and variances of the fitted Gaussians\n    means = gmm.means_\n    covariances = gmm.covariances_\n    \n    return {'means': means, 'variances': covariances}"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    exit_codes = []\n    threads = []\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n    for thread in threads:\n        thread.join()\n    return exit_codes\ndef run_file(file):\n    try:\n        result = subprocess.run(file, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        exit_code = result.returncode\n        print(f\"File {file} exited with code {exit_code}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"File {file} failed with error: {e}\")\n        exit_code = e.returncode\n    return exit_code"}
{"task_id": "BigCodeBench/325", "solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\ndef task_func(directory_path: str, regex_pattern: str = r'\\\\(.+?\\\\)|\\\\w') -> dict:\n    # Create a dictionary to store the results\n    result = {}\n    \n    # Use glob to get all text files in the directory\n    text_files = glob.glob(os.path.join(directory_path, '*.txt'))\n    \n    # Iterate over each file\n    for file_path in text_files:\n        # Open the file and read its content\n        with open(file_path, 'r') as file:\n            content = file.read()\n        \n        # Use re.findall to find all matches based on the regex pattern\n        matches = re.findall(regex_pattern, content)\n        \n        # Extract the file name without the path\n        file_name = Path(file_path).name\n        \n        # Store the matches in the result dictionary\n        result[file_name] = matches\n    \n    return result"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    # List to store the results\n    results = []\n    \n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    # Iterate over each .bat file\n    for file in bat_files:\n        try:\n            # Execute the .bat file\n            result = subprocess.run(file, capture_output=True, text=True)\n            # Append the file name and its exit code to the results list\n            results.append((os.path.basename(file), result.returncode))\n        except Exception as e:\n            # If there is an error, append the file name and None\n            results.append((os.path.basename(file), None))\n    \n    return results"}
{"task_id": "BigCodeBench/327", "solution": "import csv\nimport re\nfrom collections import Counter\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        text = ''\n        for row in reader:\n            text += ' '.join(row)\n    matches = re.findall(regex_pattern, text)\n    counts = Counter(matches)\n    return dict(counts)"}
{"task_id": "BigCodeBench/328", "solution": "import collections\nimport random\nfrom queue import PriorityQueue\ndef task_func(number_teams=5):\n    teams = {}\n    for i in range(1, number_teams + 1):\n        team_name = f\"Team {i}\"\n        points = random.randint(0, 100)\n        teams[team_name] = points\n    sorted_teams = collections.OrderedDict(sorted(teams.items(), key=lambda item: item[1], reverse=True))\n    return sorted_teams"}
{"task_id": "BigCodeBench/329", "solution": "import re\nimport json\nimport os\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    # Read the JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize a dictionary to store the results\n    result = {}\n    \n    # Iterate over each key-value pair in the JSON data\n    for key, value in data.items():\n        # If the value is a string, apply the regex pattern\n        if isinstance(value, str):\n            # Find all matches using the regex pattern\n            matches = re.findall(regex_pattern, value)\n            # Append the matches to the result dictionary\n            result[key] = matches\n        else:\n            # If the value is not a string, skip it\n            continue\n    \n    return result"}
{"task_id": "BigCodeBench/330", "solution": "import heapq\nimport random\ndef task_func(list_length: int, k: int):\n    # Generate a random list of integers\n    random_list = [random.randint(1, 100) for _ in range(list_length)]\n    \n    # Find the k largest numbers using heapq\n    k_largest = heapq.nlargest(k, random_list)\n    \n    return (random_list, k_largest)"}
{"task_id": "BigCodeBench/331", "solution": "import bisect\nimport random\ndef task_func(num, list_length=5, min_value=0, max_value=0):\n    if max_value == 0:\n        max_value = num + 10  # Set a default max_value if not provided\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    bisect.insort(random_list, num)\n    return (random_list, random_list)"}
{"task_id": "BigCodeBench/332", "solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    # Download the stopwords if not already downloaded\n    import nltk\n    nltk.download('stopwords')\n    \n    # Define the stop words\n    stop_words = set(stopwords.words('english'))\n    \n    # Use regular expression to find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Filter out the stop words\n    filtered_words = [word for word in words if word not in stop_words]\n    \n    # Count the occurrences of each word\n    word_counts = Counter(filtered_words)\n    \n    return dict(word_counts)"}
{"task_id": "BigCodeBench/333", "solution": "import heapq\nimport random\ndef task_func(k, list_length=5, min_value=0, max_value=100):\n    # Generate a list of random integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    \n    # Find the k smallest numbers using heapq\n    k_smallest = heapq.nsmallest(k, random_list)\n    \n    # Return a tuple containing the original list and the k smallest numbers\n    return (random_list, k_smallest)"}
{"task_id": "BigCodeBench/334", "solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ndef task_func(documents):\n    # Create a TfidfVectorizer object\n    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')\n    \n    # Fit and transform the documents to get the TF-IDF matrix\n    tfidf_matrix = vectorizer.fit_transform(documents)\n    \n    # Get the feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Create a DataFrame from the TF-IDF matrix\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=range(1, len(documents)+1))\n    \n    return df"}
{"task_id": "BigCodeBench/335", "solution": "import collections\nfrom queue import PriorityQueue\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(string_length=100):\n    # Generate a random string of the given length from the predefined list of letters\n    random_string = ''.join(random.choice(LETTERS) for _ in range(string_length))\n    \n    # Count the frequency of each letter in the string\n    frequency = collections.Counter(random_string)\n    \n    # Create an ordered dictionary sorted by frequency in descending order\n    ordered_frequency = collections.OrderedDict(sorted(frequency.items(), key=lambda item: item[1], reverse=True))\n    \n    return ordered_frequency"}
{"task_id": "BigCodeBench/336", "solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\ndef task_func(pattern, directory, extensions):\n    # Create a regular expression pattern that matches the given pattern in a case-insensitive manner\n    regex = re.compile(pattern, re.IGNORECASE)\n    \n    # Use glob to get all files in the specified directory with the specified extensions\n    files = glob.glob(os.path.join(directory, f'*.{extensions}'))\n    \n    # Initialize an empty list to store the file paths that contain the pattern\n    matching_files = []\n    \n    # Iterate over each file in the list of files\n    for file in files:\n        # Open the file in read mode\n        with open(file, 'r') as f:\n            # Read the contents of the file\n            contents = f.read()\n            # Search for the pattern in the contents\n            if regex.search(contents):\n                # If the pattern is found, append the absolute file path to the list\n                matching_files.append(os.path.abspath(file))\n    \n    # Return the list of matching file paths\n    return matching_files"}
{"task_id": "BigCodeBench/337", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col):\n    # Check if 'Value' has non-numeric values\n    if not df[value_col].apply(np.isreal).all():\n        raise TypeError(\"The 'Value' column contains non-numeric values.\")\n    \n    # Get unique groups\n    groups = df[group_col].unique()\n    \n    # Number of groups\n    n_groups = len(groups)\n    \n    # Number of colors\n    n_colors = len(COLORS)\n    \n    # Create figure and axis\n    fig, ax = plt.subplots()\n    \n    # Width of a bar\n    bar_width = 0.8 / n_groups\n    \n    # Create index for each group\n    index = np.arange(len(df[group_col].unique()))\n    \n    # Create bar chart\n    for i, group in enumerate(groups):\n        # Get data for the group\n        group_data = df[df[group_col] == group][value_col]\n        \n        # Calculate mean and standard deviation\n        mean_val = group_data.mean()\n        std_val = group_data.std()\n        \n        # Set color for the bar\n        color = COLORS[i % n_colors]\n        \n        # Create bar\n        ax.bar(index + i * bar_width, mean_val, bar_width, yerr=std_val, color=color, label=group)\n    \n    # Set title and labels\n    ax.set_title(f\"Bar chart of {value_col} by {group_col}\")\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n    \n    # Add legend\n    ax.legend()\n    \n    # Show plot\n    plt.show()\n    \n    # Return axes object\n    return ax"}
{"task_id": "BigCodeBench/338", "solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\ndef task_func(elements, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    all_chars = []\n    for element in elements:\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        formatted_element = f\"% {random_string}%\"\n        formatted_elements.append(formatted_element)\n        all_chars.extend(random_string)\n    \n    char_count = {}\n    for char in all_chars:\n        if char in char_count:\n            char_count[char] += 1\n        else:\n            char_count[char] = 1\n    \n    plt.hist(char_count.values(), bins=10, color='skyblue', edgecolor='black')\n    plt.title('Histogram of Character Occurrences')\n    plt.xlabel('Frequency')\n    plt.ylabel('Count')\n    plt.show()\n    \n    return formatted_elements, plt.gca(), char_count\nelements = [\"apple\", \"banana\", \"cherry\"]"}
{"task_id": "BigCodeBench/339", "solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"req_data must be a dictionary\")\n    req_data_str = json.dumps(req_data, separators=(',', ':'), sort_keys=True)\n    signature = hmac.new(secret_key.encode(), req_data_str.encode(), hashlib.sha256).digest()\n    url_encoded_signature = urllib.parse.quote(signature.hex())\n    return url_encoded_signature.replace('%20', '+')"}
{"task_id": "BigCodeBench/340", "solution": "import json\nimport hashlib\nimport blake3\ndef task_func(req_data):\n    # Convert the request data to a JSON string\n    json_str = json.dumps(req_data, sort_keys=True)\n    \n    # Hash the JSON string with BLAKE3\n    blake3_hash = blake3.blake3(json_str.encode()).hexdigest()\n    \n    # Generate an MD5 hash of the BLAKE3 hash for demonstration purposes\n    md5_hash = hashlib.md5(blake3_hash.encode()).hexdigest()\n    \n    return (blake3_hash, md5_hash)"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' is not found in the DataFrame.\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Histogram with KDE\n    sns.histplot(df[col], kde=True, ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n    \n    # Box plot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n    \n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/342", "solution": "import string\nimport random\nimport re\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    result_list = []\n    for element in elements:\n        replaced_element = ''.join(random.choice(string.ascii_letters) for _ in element)\n        formatted_element = f\"%{replaced_element}%\"\n        result_list.append(formatted_element)\n    concatenated_string = ''.join(result_list)\n    search_result = re.search(pattern, concatenated_string) is not None\n    return result_list, search_result"}
{"task_id": "BigCodeBench/343", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b', 'y', 'm']\ndef task_func(df, col, title=None):\n    \"\"\"\n    Draw a pie chart of the number of unique values in a given DataFrame column with an optional title.\n\n    Parameters:\n    - df: pandas DataFrame\n    - col: str, the name of the column to analyze\n    - title: str, optional, the title of the pie chart\n\n    Returns:\n    - matplotlib.axes._subplots.AxesSubplot: the axes object containing the pie chart\n\n    Raises:\n    - ValueError: if df is not a DataFrame, is empty, or does not contain the specified column\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n    \n    unique_values = df[col].unique()\n    counts = df[col].value_counts()\n    \n    # Ensure there are enough colors for all unique values\n    if len(unique_values) > len(COLORS):\n        raise ValueError(\"Not enough colors for unique values. Please provide more colors in the COLORS list.\")\n    \n    # Create pie chart\n    fig, ax = plt.subplots()\n    ax.pie(counts, labels=unique_values, colors=COLORS[:len(unique_values)], autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    \n    if title:\n        ax.set_title(title)\n    \n    return ax"}
{"task_id": "BigCodeBench/344", "solution": "import os\nimport shutil\ndef task_func(src_folder, backup_dir):\n    try:\n        if not os.path.exists(src_folder):\n            raise ValueError(\"Source folder does not exist.\")\n        \n        # Backup the source folder to the specified backup directory\n        shutil.copytree(src_folder, os.path.join(backup_dir, os.path.basename(src_folder)))\n        \n        # Delete the source folder\n        shutil.rmtree(src_folder)\n        \n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False"}
{"task_id": "BigCodeBench/345", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Columns {col1} and {col2} must be present in the DataFrame.\")\n    if not df[col1].dtype in [int, float] or not df[col2].dtype in [int, float]:\n        raise TypeError(\"Both columns must contain numeric data.\")\n    \n    axes = sns.regplot(x=col1, y=col2, data=df)\n    plt.show()\n    return axes"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n    \n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr)"}
{"task_id": "BigCodeBench/347", "solution": "import pandas as pd\nimport re\nimport numpy as np\nPATTERN = r\"([a-fA-F\\d]{32})\"\ndef task_func(df, column):\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    # Find all matches of the regex pattern in the specified column\n    matches = df[column].astype(str).str.findall(PATTERN)\n    \n    # Flatten the list of lists into a single list\n    all_matches = [match for sublist in matches for match in sublist]\n    \n    # Count the occurrence of each unique match\n    match_counts = pd.Series(all_matches).value_counts()\n    \n    return match_counts"}
{"task_id": "BigCodeBench/348", "solution": "import subprocess\nimport os\nimport signal\nimport time\ndef task_func(process_name: str) -> int:\n    # Get a list of all running processes with the given name\n    try:\n        # Use the 'pgrep' command to get the process IDs\n        result = subprocess.run(['pgrep', process_name], stdout=subprocess.PIPE, text=True, check=True)\n        pids = result.stdout.strip().split('\\n')\n    except subprocess.CalledProcessError:\n        # No processes found with the given name\n        return 0\n\n    # Send a termination signal to each process\n    for pid in pids:\n        try:\n            os.kill(int(pid), signal.SIGTERM)\n            time.sleep(1)  # Wait for 1 second\n        except ProcessLookupError:\n            # Process already terminated\n            pass\n\n    # Check if the processes have terminated\n    stopped_count = 0\n    for pid in pids:\n        try:\n            os.kill(int(pid), 0)\n            # Process is still running\n        except ProcessLookupError:\n            # Process has terminated\n            stopped_count += 1\n\n    return stopped_count"}
{"task_id": "BigCodeBench/349", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories):\n    data = []\n    for product in product_list:\n        for category in categories:\n            quantity_sold = random.randint(1, 100)\n            revenue = quantity_sold * random.randint(10, 100)\n            data.append({'Product': product, 'Category': category, 'Quantity Sold': quantity_sold, 'Revenue': revenue})\n    df = pd.DataFrame(data)\n    return df\ncategories = ['Category X', 'Category Y']"}
{"task_id": "BigCodeBench/350", "solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\ndef task_func(src_folder, dst_folder):\n    \"\"\"\n    Compress all files in the specified source folder and move the compressed files to a destination folder.\n    This operation is executed as a background process using the 'gzip' command.\n\n    Parameters:\n    - src_folder (str): The path to the source folder containing files to be compressed.\n    - dst_folder (str): The path to the destination folder where compressed files will be moved.\n\n    Returns:\n    - dict: A dictionary containing:\n        - 'success': A boolean indicating if all files were compressed and moved successfully.\n        - 'message': A descriptive message about the operation's result.\n        - 'failed_files': A list of filenames that failed to compress or move.\n    \"\"\"\n    # Ensure the destination folder exists\n    if not os.path.exists(dst_folder):\n        os.makedirs(dst_folder)\n\n    # Find all files in the source folder\n    files = glob(os.path.join(src_folder, '*'))\n\n    failed_files = []\n\n    for file in files:\n        if os.path.isfile(file):\n            # Compress the file using gzip in the background\n            try:\n                subprocess.run(['gzip', file], check=True)\n            except subprocess.CalledProcessError as e:\n                print(f\"Error compressing {file}: {e}\")\n                failed_files.append(file)\n                continue\n\n            # Move the compressed file to the destination folder\n            compressed_file = file + '.gz'\n            try:\n                shutil.move(compressed_file, dst_folder)\n            except Exception as e:\n                print(f\"Error moving {compressed_file} to {dst_folder}: {e}\")\n                failed_files.append(file)\n                continue\n\n    # Prepare the result dictionary\n    success = len(failed_files) == 0\n    message = \"All files were compressed and moved successfully.\" if success else f\"Failed to compress or move {len(failed_files)} files.\"\n    result = {\n        'success': success,\n        'message': message,\n        'failed_files': failed_files\n    }\n\n    return result"}
{"task_id": "BigCodeBench/351", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value=10, max_value=100):\n    data = []\n    for product in product_list:\n        for category in categories:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = quantity_sold * random.randint(1, 100)\n            data.append({'Product': product, 'Category': category, 'Quantity Sold': quantity_sold, 'Revenue': revenue})\n    df = pd.DataFrame(data)\n    return df\nproducts = ['Product A', 'Product B', 'Product C']\ncategories = ['Category X', 'Category Y', 'Category Z']"}
{"task_id": "BigCodeBench/352", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k cannot be a negative integer\")\n    \n    # Calculate frequencies of words in word_keys\n    word_freq = {word: text_dict.get(word, 0) for word in word_keys}\n    \n    # Get top_k most common words from text_dict\n    top_words = Counter(text_dict).most_common(top_k)\n    top_words_dict = dict(top_words)\n    \n    # Combine word_freq and top_words_dict, with top_words_dict taking precedence\n    combined_freq = {**top_words_dict, **word_freq}\n    \n    # Create a DataFrame for plotting\n    df = pd.DataFrame(list(combined_freq.items()), columns=['Word', 'Frequency'])\n    \n    # Create bar chart\n    ax = df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    \n    return ax, combined_freq\ntext_dict = {'apple': 3, 'banana': 1, 'cherry': 2, 'date': 5}\nword_keys = ['banana', 'cherry', 'fig']\ntop_k = 3"}
{"task_id": "BigCodeBench/353", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value=10, max_value=100):\n    # Create a list to hold the sales data\n    sales_data = []\n    \n    # Iterate over each product\n    for product in product_list:\n        # Select a random category for the product\n        category = random.choice(categories)\n        \n        # Generate a random quantity sold between min_value and max_value\n        quantity_sold = random.randint(min_value, max_value)\n        \n        # Generate a random revenue for the product\n        revenue = random.randint(min_value, max_value)\n        \n        # Calculate the total revenue for the product\n        total_revenue = quantity_sold * revenue\n        \n        # Append the sales data for the product to the sales_data list\n        sales_data.append({\n            'Product': product,\n            'Category': category,\n            'Quantity Sold': quantity_sold,\n            'Revenue': revenue,\n            'Total Revenue': total_revenue\n        })\n    \n    # Create a pandas DataFrame from the sales_data list\n    df = pd.DataFrame(sales_data)\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/354", "solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\ndef task_func(sentences_dict, word_keys):\n    # Initialize a dictionary to store word frequencies\n    word_freq = collections.defaultdict(int)\n    \n    # Iterate through each sentence in the sentences_dict\n    for key in word_keys:\n        sentence = sentences_dict.get(key, '')\n        # Split the sentence into words\n        words = sentence.lower().split()\n        # Count the frequency of each word\n        for word in words:\n            if word in WORDS:\n                word_freq[word] += 1\n    \n    # Create a DataFrame from the word frequencies\n    df = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])\n    \n    # Create a bar chart using matplotlib\n    ax = df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n    \n    # Set the title and labels\n    ax.set_title('Word Frequencies')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/355", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\ndef task_func(amplitude, frequency, time):\n    # Generate time points\n    t = np.linspace(0, time, 1000)\n    \n    # Generate complex wave\n    real_part = amplitude * np.cos(2 * np.pi * frequency * t)\n    imag_part = amplitude * np.sin(2 * np.pi * frequency * t)\n    complex_wave = real_part + 1j * imag_part\n    \n    # Apply Hann window\n    window = get_window('hann', len(t))\n    complex_wave_windowed = complex_wave * window\n    \n    # Plot the complex wave\n    fig, ax = plt.subplots()\n    ax.plot(t, np.real(complex_wave_windowed), label='Real part')\n    ax.plot(t, np.imag(complex_wave_windowed), label='Imaginary part')\n    ax.set_title('Complex Wave with Hann Window')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    \n    return complex_wave_windowed, fig, ax"}
{"task_id": "BigCodeBench/356", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\ndef task_func(x, y):\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"Both x and y must be numpy arrays.\")\n    if x.shape != y.shape:\n        raise ValueError(\"x and y must have the same shape.\")\n    \n    # Calculate the phase\n    phase = np.angle(x + 1j*y)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Plot the phase\n    cax = ax.imshow(phase, cmap='viridis')\n    \n    # Add a color bar\n    fig.colorbar(cax)\n    \n    return ax, phase"}
{"task_id": "BigCodeBench/357", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray\")\n    real_part = norm.pdf(x, loc=0, scale=1)\n    imag_part = norm.pdf(x, loc=2, scale=2)\n    complex_dist = real_part + 1j*imag_part\n    return complex_dist"}
{"task_id": "BigCodeBench/358", "solution": "import itertools\nimport json\ndef task_func(json_list, r):\n    try:\n        data = json.loads(json_list)\n    except json.JSONDecodeError:\n        raise Exception(\"Invalid JSON\")\n    if not data or 'number_list' not in data:\n        raise Exception(\"Invalid JSON or missing 'number_list' key\")\n    number_list = data['number_list']\n    if not isinstance(number_list, list):\n        raise Exception(\"'number_list' is not a list\")\n    if not all(isinstance(x, int) for x in number_list):\n        raise Exception(\"'number_list' contains non-integer values\")\n    if r < 0 or r > len(number_list):\n        raise Exception(\"Invalid r value\")\n    return list(itertools.combinations(number_list, r))"}
{"task_id": "BigCodeBench/359", "solution": "from scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(data_dict, data_keys):\n    # Extract the data series from the dictionary using the provided keys\n    series1 = data_dict[data_keys[0]]\n    series2 = data_dict[data_keys[1]]\n    \n    # Calculate the correlation coefficient\n    correlation_coefficient, _ = stats.pearsonr(series1, series2)\n    \n    # Create a scatter plot\n    plt.scatter(series1, series2)\n    plt.xlabel(data_keys[0])\n    plt.ylabel(data_keys[1])\n    plt.title('Scatter plot of {} and {}'.format(data_keys[0], data_keys[1]))\n    \n    # Return the correlation coefficient and the scatter plot\n    return correlation_coefficient, plt"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    try:\n        # Load data from Excel spreadsheet\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The specified Excel file does not exist at the given path.\")\n    except KeyError:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n    \n    # Calculate mean and standard deviation of each column\n    mean_values = df.mean().to_dict()\n    std_dev_values = df.std().to_dict()\n    \n    # Create a dictionary to hold the results\n    results = {\n        'mean': mean_values,\n        'std_dev': std_dev_values\n    }\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    x = np.arange(len(mean_values))  # the label locations\n    width = 0.35  # the width of the bars\n    \n    # Create bars for mean and standard deviation\n    rects1 = ax.bar(x - width/2, mean_values.values(), width, label='Mean')\n    rects2 = ax.bar(x + width/2, std_dev_values.values(), width, label='Standard Deviation')\n    \n    # Add some text for labels, title, and custom x-axis tick labels, etc.\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(mean_values.keys())\n    ax.legend()\n    \n    # Attach a text label above each bar in *both* means and standard deviations\n    def autolabel(rects):\n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate('{}'.format(height),\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')\n    \n    autolabel(rects1)\n    autolabel(rects2)\n    \n    # Show the plot\n    plt.show()\n    \n    return results, fig"}
{"task_id": "BigCodeBench/361", "solution": "import pandas as pd\nimport logging\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    try:\n        # Read the Excel file\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except FileNotFoundError:\n        logging.error(f\"Excel file not found at {excel_file_location}\")\n        raise FileNotFoundError(f\"Excel file not found at {excel_file_location}\")\n    except KeyError:\n        logging.error(f\"Sheet name {sheet_name} not found in the Excel file\")\n        raise ValueError(f\"Sheet name {sheet_name} not found in the Excel file\")\n    \n    # Convert the DataFrame to a CSV file\n    df.to_csv(csv_file_location, index=False)\n    \n    # Calculate the sum of each column\n    column_sums = df.sum().to_dict()\n    \n    return column_sums"}
{"task_id": "BigCodeBench/362", "solution": "import pandas as pd\nimport os\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    try:\n        # Check if the original file exists\n        if not os.path.exists(original_file_location):\n            raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n        \n        # Read the original Excel file\n        df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n        \n        # Write the DataFrame to a new Excel file\n        df.to_excel(new_file_location, sheet_name=sheet_name, index=False)\n        \n        # Read the new Excel file and return its contents as a DataFrame\n        new_df = pd.read_excel(new_file_location, sheet_name=sheet_name)\n        return new_df\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(e)\n    except ValueError as e:\n        raise ValueError(e)"}
{"task_id": "BigCodeBench/363", "solution": "from multiprocessing import Pool\nimport math\ndef calculate_factorial(number: int) -> tuple:\n    if not isinstance(number, int) or number < 0:\n        raise ValueError(\"Input must be a non-negative integer.\")\n    return number, math.factorial(number)\ndef task_func(numbers: list) -> dict:\n    if not all(isinstance(num, int) and num >= 0 for num in numbers):\n        raise ValueError(\"All elements in the list must be non-negative integers.\")\n    \n    with Pool() as pool:\n        results = pool.map(calculate_factorial, numbers)\n    \n    return dict(results)"}
{"task_id": "BigCodeBench/364", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    X = df[FEATURES]\n    y = df[TARGET]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    return model"}
{"task_id": "BigCodeBench/365", "solution": "from collections import Counter\nimport json\nimport random\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\ndef task_func(n, file_name, seed=77):\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n    with open(file_name, 'w') as f:\n        json.dump(selected_words, f)\n    return file_name"}
{"task_id": "BigCodeBench/366", "solution": "import matplotlib.pyplot as plt\nimport random\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    \"\"\"\n    Create a histogram subplot of a list of numbers.\n\n    Parameters:\n    - number_list: List of numbers to be plotted in the histogram.\n    - bins: Number of bins in the histogram.\n\n    Returns:\n    - matplotlib.axes._axes.Axes: The axes object representing the histogram plot.\n    \"\"\"\n    # Randomly select a color from the predefined set\n    color = random.choice(COLORS)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram\n    ax.hist(number_list, bins=bins, color=color)\n    \n    # Set the title and labels\n    ax.set_title('Histogram')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    \n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n    \n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n    \n    days = list(day_counts.keys())\n    counts = list(day_counts.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    random.seed(seed)\n    files = os.listdir(src_dir)\n    if not files:\n        return \"No files to move\"\n    file_to_move = random.choice(files)\n    shutil.move(os.path.join(src_dir, file_to_move), os.path.join(dest_dir, file_to_move))\n    return file_to_move"}
{"task_id": "BigCodeBench/369", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    # Convert the input list to a numpy array\n    data = np.array(l)\n    \n    # Fit a Gaussian distribution to the data\n    mu, std = stats.norm.fit(data)\n    \n    # Create a histogram of the data\n    ax = plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the fitted Gaussian distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    \n    # Set the title of the plot\n    plt.title(f'Fit results: mu = {mu:.2f},  std = {std:.2f}')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/370", "solution": "import os\nimport re\nimport json\nimport glob\ndef task_func(directory_path: str) -> list:\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n    \n    json_files = glob.glob(os.path.join(directory_path, '*.json'))\n    processed_files = []\n    \n    for file_path in json_files:\n        with open(file_path, 'r') as file:\n            content = file.read()\n        \n        # Escape double quotes by prepending them with a double backslash\n        escaped_content = re.sub(r'(?<!\\\\)\"', r'\\\"', content)\n        \n        with open(file_path, 'w') as file:\n            file.write(escaped_content)\n        \n        processed_files.append(file_path)\n    \n    return processed_files"}
{"task_id": "BigCodeBench/371", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(l):\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(l)\n    df = pd.DataFrame(scaled_data, columns=['Scaled Values'])\n    return df"}
{"task_id": "BigCodeBench/372", "solution": "import re\nimport glob\nfrom docx import Document\ndef task_func(directory_path: str) -> int:\n    # Find all .docx files in the directory\n    docx_files = glob.glob(directory_path + '/*.docx')\n    count = 0\n    for file in docx_files:\n        # Open the Word document\n        doc = Document(file)\n        # Iterate through all paragraphs in the document\n        for para in doc.paragraphs:\n            # Search for double quotes in the text\n            if re.search(r'\"', para.text):\n                # Replace each double quote with a backslash followed by a double quote\n                para.text = re.sub(r'\"', r'\\\"', para.text)\n        # Save the modified document\n        doc.save(file)\n        count += 1\n    return count"}
{"task_id": "BigCodeBench/373", "solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\ndef task_func(l, x_data, plot=False):\n    \"\"\"\n    Adjust a quadratic curve to the specified data and return the parameters and fitted values.\n\n    Parameters:\n    l (list): List of tuples, each containing x and y values.\n    x_data (list): List of x-values for which the curve is to be fitted.\n    plot (bool, optional): If True, plot the fitted curve along with the data points.\n\n    Returns:\n    tuple: A tuple containing:\n        params (numpy array): Parameters of the fitted curve.\n        fitted_values (numpy array): Fitted y-values for the provided x_data.\n        ax (matplotlib.axes._axes.Axes, optional): Axes object of the plot if plot=True.\n    \"\"\"\n    # Convert list of tuples to numpy arrays\n    data = np.array(l)\n    x = data[:, 0]\n    y = data[:, 1]\n\n    # Define the quadratic function\n    def quadratic_func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    # Perform curve fitting\n    params, _ = curve_fit(quadratic_func, x, y)\n\n    # Generate fitted y-values\n    fitted_values = quadratic_func(x_data, *params)\n\n    # Plotting if required\n    if plot:\n        ax = plt.axes()\n        ax.scatter(x, y, label='Data points')\n        ax.plot(x_data, fitted_values, 'r', label='Fitted curve')\n        ax.legend()\n        return params, fitted_values, ax\n    else:\n        return params, fitted_values, None\nl = [(1, 2), (2, 3), (3, 5), (4, 7), (5, 11)]\nx_data = np.linspace(0, 6, 100)"}
{"task_id": "BigCodeBench/374", "solution": "import re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Initialize a counter for the number of Excel files processed\n    file_count = 0\n\n    # Use glob to find all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Iterate over each file in the list\n    for file in xlsx_files:\n        # Load the workbook\n        wb = load_workbook(file, data_only=True)\n\n        # Iterate over each sheet in the workbook\n        for sheet in wb.sheetnames:\n            ws = wb[sheet]\n\n            # Iterate over each cell in the sheet\n            for row in ws.iter_rows():\n                for cell in row:\n                    # Check if the cell contains a string\n                    if cell.data_type == 's':\n                        # Use regex to find all double quotes and replace them with two double backslashes\n                        cell.value = re.sub(r'\\\"', r'\\\\\\\"', str(cell.value))\n\n        # Save the changes back to the file\n        wb.save(file)\n\n        # Increment the file counter\n        file_count += 1\n\n    # Return the number of files processed\n    return file_count"}
{"task_id": "BigCodeBench/375", "solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(l):\n    # Convert the list to a numpy array\n    data = np.array(l)\n    \n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(data)\n    transformed = pca.transform(data)\n    \n    # Plot the first two principal components\n    ax = plt.axes()\n    ax.scatter(transformed[:, 0], transformed[:, 1])\n    ax.set_title('PCA Result')\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n    \n    return ax"}
{"task_id": "BigCodeBench/376", "solution": "import nltk\nimport re\nfrom collections import Counter\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(text):\n    # Download punkt tokenizer models\n    nltk.download('punkt')\n    # Tokenize the text into words\n    words = nltk.word_tokenize(text)\n    # Convert to lowercase\n    words = [word.lower() for word in words]\n    # Remove punctuation marks\n    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n    # Remove common stopwords\n    words = [word for word in words if word not in STOPWORDS]\n    # Calculate the frequency of each word\n    word_counts = Counter(words)\n    return dict(word_counts)"}
{"task_id": "BigCodeBench/377", "solution": "from texttable import Texttable\nimport os\nimport psutil\ndef task_func():\n    # Get CPU usage\n    cpu_usage = psutil.cpu_percent(interval=1)\n    \n    # Get memory usage\n    memory = psutil.virtual_memory()\n    memory_usage = memory.percent\n    \n    # Get disk usage\n    disk_usage = psutil.disk_usage('/')\n    disk_usage = disk_usage.percent\n    \n    # Create a table\n    table = Texttable()\n    table.set_deco(Texttable.HEADER)\n    table.set_cols_dtype(['t', 't'])\n    table.set_cols_align(['l', 'r'])\n    table.add_row(['Item', 'Value'])\n    table.add_row(['CPU Usage (%)', cpu_usage])\n    table.add_row(['Memory Usage (%)', memory_usage])\n    table.add_row(['Disk Usage (%)', disk_usage])\n    \n    # Return the table as a string\n    return table.draw()"}
{"task_id": "BigCodeBench/378", "solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\ndef task_func(data_dir='./data/'):\n    # Check if the directory exists\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n    \n    # Find all CSV files in the directory\n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n    \n    # Check if there are any CSV files\n    if not csv_files:\n        raise ValueError(\"No CSV files found in the specified directory.\")\n    \n    # Initialize the table\n    table = Texttable()\n    table.set_deco(Texttable.HEADER)\n    table.set_cols_dtype(['t', 'i', 'i'])  # text, integer, integer\n    table.set_cols_align(['l', 'r', 'r'])  # left, right, right\n    table.header(['File Name', 'Rows', 'Columns'])\n    \n    for file in csv_files:\n        try:\n            df = pd.read_csv(file)\n            if df.empty:\n                raise pd.errors.EmptyDataError(f\"The file {file} is empty.\")\n            rows = len(df)\n            cols = len(df.columns)\n            table.add_row([os.path.basename(file), rows, cols])\n        except pd.errors.EmptyDataError as e:\n            print(e)\n        except Exception as e:\n            print(f\"An error occurred while processing {file}: {e}\")\n    \n    return table.draw()"}
{"task_id": "BigCodeBench/379", "solution": "import pandas as pd\nimport numpy as np\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length):\n    # Generate random data\n    data = {\n        'Column1': np.random.rand(length),\n        'Column2': np.random.rand(length),\n        'Column3': np.random.rand(length),\n        'Column4': np.random.rand(length),\n        'Column5': np.random.rand(length)\n    }\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    # Record the data\n    with open('data.txt', 'w') as f:\n        f.write(str(df))\n    return df"}
{"task_id": "BigCodeBench/380", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # Get all files in the directory\n    files = os.listdir(directory)\n    \n    # Create a dictionary to hold lists of files by their extensions\n    extensions = {}\n    \n    # Iterate over each file in the directory\n    for file in files:\n        # Check if the file is not a directory\n        if os.path.isfile(os.path.join(directory, file)):\n            # Get the extension of the file\n            _, ext = os.path.splitext(file)\n            # If the extension is not in the dictionary, add it\n            if ext not in extensions:\n                extensions[ext] = []\n            # Add the file to the corresponding extension list\n            extensions[ext].append(file)\n    \n    # Create a new directory for each extension\n    for ext in extensions:\n        # Create the directory name by removing the dot from the extension\n        dir_name = ext[1:]\n        # Create the directory if it doesn't exist\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n        # Move each file to the corresponding directory\n        for file in extensions[ext]:\n            shutil.move(os.path.join(directory, file), dir_name)\n    \n    # Return None as per the requirement\n    return None"}
{"task_id": "BigCodeBench/381", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    # Load the data\n    df = pd.read_csv(file_path)\n    \n    # Check if the target column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} is not found in the CSV file.\")\n    \n    # Drop rows with any NaN values\n    df = df.dropna()\n    \n    # Check for infinite values\n    if df.isin([np.inf, -np.inf]).any().any():\n        raise ValueError(\"The input data contains infinite values.\")\n    \n    # Check for values too large for float32\n    if df.dtypes[df.dtypes == 'float32'].any():\n        raise ValueError(\"The input data contains values too large for dtype('float32').\")\n    \n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the model\n    model = RandomForestClassifier(random_state=seed)\n    model.fit(X, y)\n    \n    # Get feature importances\n    importances = model.feature_importances_\n    \n    # Plot feature importances\n    plt.figure(figsize=(10,6))\n    sns.barplot(x=importances, y=X.columns, orient='h')\n    plt.title('Feature Importances')\n    plt.show()\n    \n    return plt.gca(), importances\nfile_path = 'arena.csv'"}
{"task_id": "BigCodeBench/382", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(length):\n    # Generate a normal distribution with mean 0 and standard deviation 1\n    distribution = norm.rvs(loc=0, scale=1, size=length)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the distribution\n    ax.hist(distribution, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the probability density function\n    x = np.linspace(norm.ppf(0.01, loc=0, scale=1), norm.ppf(0.99, loc=0, scale=1), 100)\n    ax.plot(x, norm.pdf(x, loc=0, scale=1), 'k-', lw=2, label='norm pdf')\n    \n    # Set the title and labels\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Show the legend\n    ax.legend()\n    \n    # Return the distribution and the plot\n    return distribution, ax"}
{"task_id": "BigCodeBench/383", "solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\ndef task_func(text, n, top_k):\n    # Tokenize the text into words\n    words = TextBlob(text).words\n\n    # Generate n-grams\n    ngrams = zip(*[words[i:] for i in range(n)])\n    ngrams = [' '.join(ngram) for ngram in ngrams]\n\n    # Count the frequency of each n-gram\n    ngram_counts = Counter(ngrams)\n\n    # Get the top K n-grams\n    top_ngrams = ngram_counts.most_common(top_k)\n\n    # Create a DataFrame\n    df = pd.DataFrame(top_ngrams, columns=['n-gram', 'frequency'])\n\n    # Plot the n-grams\n    plt.figure(figsize=(10,6))\n    sns.barplot(x='frequency', y='n-gram', data=df, orient='h')\n    plt.title(f'Top {top_k} {n}-grams')\n    plt.show()\n\n    return None"}
{"task_id": "BigCodeBench/384", "solution": "import collections\nimport random\nimport itertools\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    reversed_dict = {}\n    counter = collections.Counter()\n    \n    for key, value in animal_dict.items():\n        if value in reversed_dict:\n            reversed_dict[value].append(key)\n        else:\n            reversed_dict[value] = [key]\n    \n    for animal in ANIMALS:\n        count = random.randint(1, max_count)\n        counter[animal] += count\n    \n    return (reversed_dict, counter)\nanimal_dict = {\n    'Alice': 'Cat',\n    'Bob': 'Dog',\n    'Charlie': 'Elephant',\n    'David': 'Lion',\n    'Eve': 'Tiger',\n    'Frank': 'Bear',\n    'Grace': 'Giraffe',\n    'Hannah': 'Horse',\n    'Ivy': 'Rabbit',\n    'Jack': 'Snake',\n    'Kate': 'Zebra'\n}"}
{"task_id": "BigCodeBench/385", "solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit\n    fruit_counts = Counter()\n    for fruits in fruit_dict.values():\n        for fruit in fruits:\n            fruit_counts[fruit] += 1\n\n    # Create a bar chart of the fruit frequencies\n    fruits = list(fruit_counts.keys())\n    counts = list(fruit_counts.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(fruits, counts)\n    ax.set_xlabel('Fruits')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Fruit Frequency')\n\n    return fruit_counts, ax\nfruit_dict = {\n    'Alice': ['Apple', 'Banana', 'Cherry'],\n    'Bob': ['Date', 'Elderberry', 'Fig'],\n    'Charlie': ['Grape', 'Honeydew', 'Indian Prune'],\n    'David': ['Jackfruit', 'Apple', 'Banana']\n}"}
{"task_id": "BigCodeBench/386", "solution": "import numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length, min_value=0, max_value=100):\n    # Generate random data for each column\n    data = {col: np.random.randint(min_value, max_value, length) for col in COLUMNS}\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate CDF for each column\n    for col in COLUMNS:\n        sorted_data = np.sort(df[col])\n        cdf = np.arange(1, len(sorted_data)+1) / len(sorted_data)\n        df[f'{col}_cdf'] = np.interp(df[col], sorted_data, cdf)\n    \n    return df"}
{"task_id": "BigCodeBench/387", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\ndef task_func(city_dict, max_range=1000000, seed=0):\n    # Initialize a dictionary to store city populations\n    city_populations = {}\n    \n    # Seed the random number generator\n    np.random.seed(seed)\n    \n    # Iterate through the list of cities\n    for city in CITIES:\n        # If the city is in the city_dict, use the favorite city; otherwise, use a random city\n        if city in city_dict.values():\n            # Find the person who has this city as their favorite\n            for person, favorite_city in city_dict.items():\n                if favorite_city == city:\n                    # Assign the person's name to the city in the populations dictionary\n                    city_populations[city] = np.random.randint(1, max_range + 1)\n                    break\n        else:\n            # If the city is not in the city_dict, assign -1\n            city_populations[city] = -1\n    \n    # Extract city names and populations for plotting\n    cities = list(city_populations.keys())\n    populations = list(city_populations.values())\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar(cities, populations, color='skyblue')\n    \n    # Add labels and title\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('City Populations')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the dictionary of city populations and the Axes object\n    return city_populations, ax\ncity_dict = {\n    'Alice': 'New York',\n    'Bob': 'London',\n    'Charlie': 'Beijing',\n    'David': 'Tokyo',\n    'Eve': 'Sydney',\n    'Frank': 'Paris',\n    'Grace': 'Berlin',\n    'Hannah': 'Moscow',\n    'Ivy': 'Madrid',\n    'Jack': 'Rome'\n}"}
{"task_id": "BigCodeBench/388", "solution": "import collections\nimport pandas as pd\ndef task_func(my_tuple, path_csv_files):\n    result = {}\n    for file in path_csv_files:\n        df = pd.read_csv(file)\n        for col in my_tuple:\n            if col in df.columns:\n                col_data = df[col]\n                col_counts = collections.Counter(col_data)\n                result[col] = dict(col_counts)\n    return result"}
{"task_id": "BigCodeBench/389", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    # Define the pattern to search for filenames containing \"like\" or \"what\"\n    pattern = re.compile(r'like|what')\n    \n    # List to hold the files that will be moved\n    moved_files = []\n    \n    # Walk through the directory tree\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the filename contains \"like\" or \"what\"\n            if pattern.search(file):\n                # Construct the full path of the file\n                file_path = os.path.join(root, file)\n                \n                # Create the \"Interesting Files\" subdirectory if it doesn't exist\n                interesting_dir = os.path.join(root, 'Interesting Files')\n                if not os.path.exists(interesting_dir):\n                    os.makedirs(interesting_dir)\n                \n                # Construct the destination path\n                dest_path = os.path.join(interesting_dir, file)\n                \n                # Move the file to the \"Interesting Files\" subdirectory\n                shutil.move(file_path, dest_path)\n                \n                # Add the file to the list of moved files\n                moved_files.append(file)\n    \n    # Output the list of moved files\n    print(\"List of files moved:\")\n    for file in moved_files:\n        print(file)"}
{"task_id": "BigCodeBench/390", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict or \"URL\" not in csv_url_dict:\n        raise ValueError(\"The dictionary must contain a 'URL' key.\")\n    \n    url = csv_url_dict[\"URL\"]\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to retrieve data from {url}\")\n    \n    data = StringIO(response.text)\n    df = pd.read_csv(data)\n    \n    if sort_by_column not in df.columns:\n        raise ValueError(f\"Column '{sort_by_column}' not found in the DataFrame.\")\n    \n    df_sorted = df.sort_values(by=sort_by_column)\n    return df_sorted"}
{"task_id": "BigCodeBench/391", "solution": "import os\nimport glob\nimport shutil\ndef task_func(directory, archive_dir='archive'):\n    \"\"\"\n    Archive all JSON files in a given directory by moving them to a specified archive directory.\n    \n    Parameters:\n    - directory (str): The path to the directory containing JSON files.\n    - archive_dir (str): The name of the archive directory. Defaults to 'archive'.\n    \n    Returns:\n    - tuple: A tuple containing a boolean value and a list of error messages.\n             The boolean is True if all files are successfully moved, and False otherwise.\n             The list contains error messages for each file that failed to move.\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n    \n    # Find all JSON files in the specified directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n    \n    all_success = True\n    error_messages = []\n    \n    for file_path in json_files:\n        try:\n            # Construct the destination path\n            file_name = os.path.basename(file_path)\n            dest_path = os.path.join(archive_dir, file_name)\n            \n            # Move the file to the archive directory\n            shutil.move(file_path, dest_path)\n        except Exception as e:\n            all_success = False\n            error_messages.append(f\"Error moving {file_path}: {str(e)}\")\n    \n    return (all_success, error_messages)"}
{"task_id": "BigCodeBench/392", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col, group_name):\n    if group_name not in df[group_col].unique():\n        raise ValueError(f\"Group name '{group_name}' does not exist in the dataframe.\")\n    \n    group_data = df[df[group_col] == group_name][value_col]\n    x = np.arange(len(group_data))\n    plt.bar(x, group_data, color=COLORS[0])\n    plt.title(f'Bar chart of {value_col} for {group_name}')\n    plt.xlabel(group_col)\n    plt.ylabel(value_col)\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/393", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n    \n    # Create a figure with two subplots: histogram and Q-Q plot\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Plot histogram\n    axes[0].hist(samples, bins=30, density=True, color='skyblue', edgecolor='black')\n    axes[0].set_title('Histogram of Samples')\n    axes[0].set_xlabel('Value')\n    axes[0].set_ylabel('Frequency')\n    \n    # Plot Q-Q plot\n    stats.probplot(samples, dist='norm', sparams=(mu, sigma), plot=axes[1])\n    axes[1].set_title('Q-Q Plot')\n    axes[1].set_xlabel('Theoretical Quantiles')\n    axes[1].set_ylabel('Sample Quantiles')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return fig"}
{"task_id": "BigCodeBench/394", "solution": "import collections\nimport string\nimport random\ndef task_func(length, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n    frequency = collections.Counter(random_string)\n    return frequency"}
{"task_id": "BigCodeBench/395", "solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    \"\"\"\n    Extract numeric data from all text files matching a given pattern in a directory and compile it into a Pandas DataFrame.\n\n    Parameters:\n    - directory (str): The directory path to search for files. Defaults to the current directory ('./').\n    - file_pattern (str): The pattern to match files in the directory. Defaults to '*.txt'.\n    - regex (str): The regular expression pattern to extract numeric data from the files. Defaults to r'([0-9]+)'.\n\n    Returns:\n    - DataFrame: A pandas DataFrame with two columns: 'Filename' and 'Numeric Data'. Each row represents a file and its extracted numeric data.\n\n    Raises:\n    - FileNotFoundError: If the specified directory does not exist.\n    - ValueError: If no files matching the pattern are found.\n    \"\"\"\n\n    # Check if the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Find all files matching the pattern\n    files = glob.glob(os.path.join(directory, file_pattern))\n    if not files:\n        raise ValueError(f\"No files matching the pattern {file_pattern} found in {directory}.\")\n\n    # Initialize lists to hold filenames and extracted numeric data\n    filenames = []\n    numeric_data = []\n\n    # Iterate over each file\n    for file in natsort.natsorted(files):\n        with open(file, 'r') as f:\n            content = f.read()\n            # Find all matches of the regex in the content\n            matches = re.findall(regex, content)\n            # Convert matches to integers and sum them up\n            if matches:\n                total = sum(map(int, matches))\n                filenames.append(os.path.basename(file))\n                numeric_data.append(total)\n\n    # Create a DataFrame\n    df = pd.DataFrame({'Filename': filenames, 'Numeric Data': numeric_data})\n\n    return df"}
{"task_id": "BigCodeBench/396", "solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, sample_size, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    kde = stats.gaussian_kde(sample)\n    plt.plot(x, kde(x))\n    plt.title('Gaussian Kernel Density Estimate')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/397", "solution": "import re\nimport urllib.request\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(API_URL):\n    try:\n        with urllib.request.urlopen(API_URL) as response:\n            data = response.read().decode('utf-8')\n            match = re.search(IP_REGEX, data)\n            if match:\n                return match.group()\n            else:\n                raise ValueError(\"No IP address found in the API response.\")\n    except urllib.error.URLError as e:\n        return str(e)"}
{"task_id": "BigCodeBench/398", "solution": "import json\nimport os\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return False\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    return isinstance(data, list) and all(isinstance(item, dict) for item in data)"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    t = np.linspace(0, 1, sample_size)\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves')\n    ax.legend()\n    \n    return fig, ax"}
{"task_id": "BigCodeBench/400", "solution": "import json\nfrom glob import glob\ndef task_func(directory, string):\n    json_files = glob(directory + '/**/*.json', recursive=True)\n    result = []\n    for file in json_files:\n        with open(file, 'r') as f:\n            data = json.load(f)\n            if string in str(data):\n                result.append(file)\n    return result"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.getenv('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = bool(os.getenv('MAIL_USE_TLS', False))\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', None)\n    mail = Mail(app)\n    return (mail, app.config)"}
{"task_id": "BigCodeBench/402", "solution": "import re\nimport requests\nimport json\nimport csv\nimport os\nAPI_URL = 'https://api.example.com/data'\ndef task_func(pattern):\n    # Make a GET request to the API\n    response = requests.get(API_URL)\n    data = response.json()\n\n    # Extract the data from the JSON response\n    extracted_data = data.get('data', [])\n\n    # Initialize a list to hold the matched data\n    matched_data = []\n\n    # Iterate over each item in the extracted data\n    for item in extracted_data:\n        # Check if the item is a string\n        if isinstance(item, str):\n            # Use the RegEx pattern to find matches in the string\n            matches = re.findall(pattern, item)\n            # Extend the matched_data list with the matches\n            matched_data.extend(matches)\n\n    # Create a CSV file and write the matched data to it\n    csv_file_path = 'matched_data.csv'\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write each match as a separate row in the CSV file\n        for match in matched_data:\n            writer.writerow([match])\n\n    # Return the absolute path to the CSV file\n    return os.path.abspath(csv_file_path)"}
{"task_id": "BigCodeBench/403", "solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\ndef task_func(img_path, blur_radius=5):\n    try:\n        # Open the image using PIL\n        original_img = Image.open(img_path)\n        \n        # Convert the image to a numpy array\n        original_img_np = np.array(original_img)\n        \n        # Apply a blur filter using OpenCV\n        blurred_img = cv2.GaussianBlur(original_img_np, (blur_radius, blur_radius), 0)\n        \n        # Convert the blurred image to grayscale using OpenCV\n        grayscale_img = cv2.cvtColor(blurred_img, cv2.COLOR_BGR2GRAY)\n        \n        # Convert the grayscale image back to a numpy array\n        grayscale_img_np = np.array(grayscale_img)\n        \n        # Display both the original and the processed images side by side\n        # Convert both images to RGB for display\n        original_img_rgb = cv2.cvtColor(original_img_np, cv2.COLOR_BGR2RGB)\n        grayscale_img_rgb = cv2.cvtColor(grayscale_img, cv2.COLOR_GRAY2RGB)\n        \n        # Concatenate the images horizontally\n        combined_img = np.hstack((original_img_rgb, grayscale_img_rgb))\n        \n        # Display the combined image\n        cv2.imshow('Original and Processed Images', combined_img)\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n        \n        # Return the numpy arrays of the original and processed images\n        return original_img_np, grayscale_img_np\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The image file does not exist at the specified path.\")"}
{"task_id": "BigCodeBench/404", "solution": "import cv2\nimport numpy as np\nimport os\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {img_path}\")\n    \n    # Open the image\n    img = cv2.imread(img_path)\n    if img is None:\n        raise ValueError(f\"Failed to open the image file: {img_path}\")\n    \n    # Convert to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Find contours\n    contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    return img, contours"}
{"task_id": "BigCodeBench/405", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(points: int):\n    # Generate a list of random numbers\n    random_numbers = [random.random() for _ in range(points)]\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Plot the random numbers\n    ax.plot(range(points), random_numbers)\n    \n    # Set the title and labels\n    ax.set_title('Plot of Random Numbers')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Number')\n    \n    # Return the list of random numbers and the Axes object\n    return random_numbers, ax"}
{"task_id": "BigCodeBench/406", "solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\ndef task_func(img_path, angle):\n    try:\n        # Open the image\n        img = Image.open(img_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The image file does not exist at the specified path.\")\n    \n    # Convert the image to a numpy array\n    img_np = np.array(img)\n    \n    # Rotate the image\n    rotated_img = img.rotate(angle)\n    \n    # Convert the rotated image to a numpy array\n    rotated_img_np = np.array(rotated_img)\n    \n    # Display both images side by side\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(img_np)\n    axes[0].set_title('Original Image')\n    axes[1].imshow(rotated_img_np)\n    axes[1].set_title('Rotated Image')\n    plt.show()\n    \n    # Return both images as numpy arrays\n    return img_np, rotated_img_np"}
{"task_id": "BigCodeBench/407", "solution": "import os\nimport csv\nfrom openpyxl import load_workbook\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    # Load the Excel file\n    wb = load_workbook(os.path.join(excel_file_path, file_name), data_only=True)\n    # Select the active sheet\n    sheet = wb.active\n    # Create a new CSV file\n    csv_file_name = os.path.join(csv_file_path, os.path.splitext(file_name)[0] + '.csv')\n    with open(csv_file_name, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        # Write the header\n        writer.writerow([cell.value for cell in sheet[1]])\n        # Write the data rows\n        for row in sheet.iter_rows(min_row=2, values_only=True):\n            writer.writerow(row)\n    return csv_file_name"}
{"task_id": "BigCodeBench/408", "solution": "import sqlite3\nimport pandas as pd\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n    \n    # Execute the query and load the results into a pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the connection\n    conn.close()\n    \n    return df"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the path {excel_file_path}\")\n    \n    # Read the Excel file\n    df = pd.read_excel(full_path)\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file\")\n    \n    # Calculate mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n    \n    # Return the results in a dictionary\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"standard_deviation\": std_dev\n    }"}
{"task_id": "BigCodeBench/410", "solution": "import os\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    # Check if the Excel file exists\n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist in the directory {excel_directory}\")\n    \n    # Read the Excel file into a DataFrame\n    df = pd.read_excel(file_path)\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame\")\n    \n    # Convert start_date and end_date to datetime objects\n    try:\n        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    except ValueError:\n        raise ValueError(\"start_date or end_date are in an incorrect format. Please use the format YYYY-MM-DD\")\n    \n    # Filter the DataFrame based on the date range\n    df['date'] = pd.to_datetime(df[column_name])\n    filtered_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n    \n    return filtered_df"}
{"task_id": "BigCodeBench/411", "solution": "import pandas as pd\nimport json\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    df = pd.DataFrame(data)\n    if 'c' in df.columns:\n        df = df.drop(columns=['c'])\n    json_data = df.to_json(orient='records')\n    with open(output_path, 'w') as f:\n        f.write(json_data)\n    return output_path"}
{"task_id": "BigCodeBench/412", "solution": "import json\nimport base64\nimport unicodedata\ndef task_func(json_file: str) -> dict:\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n    normalized_dict = {}\n    for key, value in data.items():\n        decoded = base64.b64decode(value).decode('utf-8')\n        normalized = unicodedata.normalize('NFC', decoded)\n        normalized_dict[key] = normalized\n    return normalized_dict"}
{"task_id": "BigCodeBench/413", "solution": "import os\nfrom flask_mail import Mail\ndef task_func(app):\n    # Retrieve email server details from environment variables\n    mail_server = os.getenv('MAIL_SERVER', 'smtp.example.com')\n    mail_port = int(os.getenv('MAIL_PORT', 587))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'true').lower() == 'true'\n    mail_username = os.getenv('MAIL_USERNAME', 'user@example.com')\n    mail_password = os.getenv('MAIL_PASSWORD', 'password')\n    \n    # Configure the app's mail settings\n    app.config['MAIL_SERVER'] = mail_server\n    app.config['MAIL_PORT'] = mail_port\n    app.config['MAIL_USE_TLS'] = mail_use_tls\n    app.config['MAIL_USERNAME'] = mail_username\n    app.config['MAIL_PASSWORD'] = mail_password\n    \n    # Initialize Flask-Mail\n    mail = Mail(app)\n    \n    # Return the Flask-Mail instance and the app's mail configurations\n    return mail, app.config['MAIL']"}
{"task_id": "BigCodeBench/414", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    \"\"\"\n    Remove a column from a data dictionary if it exists, and then plot the remaining data if it contains numeric data.\n\n    Parameters:\n    - data (pd.DataFrame): The input DataFrame.\n    - column (str): The name of the column to remove.\n\n    Returns:\n    - df (pd.DataFrame): The modified DataFrame after removing the specified column.\n    - ax (matplotlib.axes._axes.Axes or None): The plot of the modified DataFrame if there's\n      numeric data to plot, otherwise None.\n    \"\"\"\n    # Remove the specified column if it exists\n    if column in data.columns:\n        data = data.drop(columns=column)\n    \n    # Check if there are any numeric columns left to plot\n    numeric_cols = data.select_dtypes(include=[np.number]).columns\n    if numeric_cols.empty:\n        return data, None\n    \n    # Plot the remaining numeric data\n    ax = data[numeric_cols].plot(kind='bar')\n    return data, ax"}
{"task_id": "BigCodeBench/415", "solution": "import pandas as pd\nimport codecs\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input must be a Pandas DataFrame.\")\n    if \"UnicodeString\" not in dataframe.columns:\n        raise KeyError(\"Column 'UnicodeString' does not exist in the DataFrame.\")\n    dataframe[\"UnicodeString\"] = dataframe[\"UnicodeString\"].apply(lambda x: codecs.decode(x, \"unicode_escape\"))\n    return dataframe"}
{"task_id": "BigCodeBench/416", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    if not data:\n        return None\n    if column in data:\n        del data[column]\n    data = pd.DataFrame(data)\n    numeric_data = data.select_dtypes(include=[np.number])\n    if numeric_data.empty:\n        return None\n    corr = numeric_data.corr()\n    plt.figure(figsize=(10,8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Create a Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n    \n    # Compile the model\n    optimizer = SGD(learning_rate=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=10, validation_data=(X_test, Y_test), verbose=0)\n    \n    # Plot the model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n    \n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Create a Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid')\n    ])\n    \n    # Compile the model with binary cross-entropy loss and an SGD optimizer specifying a learning rate\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=learning_rate), metrics=['accuracy'])\n    \n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n    \n    # Predict probabilities on the test set\n    Y_pred = model.predict(X_test)\n    \n    # Compute ROC curve and ROC area\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    # Return the trained model and the axes object for the ROC curve plot\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/419", "solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Determine the input dimension based on the first feature set of X\n    input_dim = X_train.shape[1]\n    \n    # Construct a Keras Sequential model with one hidden dense layer and sigmoid activation\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=input_dim, activation='sigmoid')\n    ])\n    \n    # Compile the model using binary cross-entropy loss and SGD optimizer\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n    \n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n    \n    # Predict probabilities for the test set\n    Y_pred = model.predict(X_test)\n    \n    # Compute precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred)\n    \n    # Plot the Precision-Recall curve\n    plt.plot(recall, precision, marker='.')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    \n    # Return the trained model and the axes object for the plot\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/420", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data):\n    \"\"\"\n    Scales numeric columns of a data dictionary using the StandardScaler.\n    Non-numeric columns remain unchanged. If a column contains mixed data types,\n    it tries to convert the entire column to float. If any value in the column\n    cannot be converted to float, the entire column is left unchanged.\n\n    Parameters:\n    - data (pd.DataFrame): The input dataframe to be scaled.\n\n    Returns:\n    - pd.DataFrame: Dataframe with scaled numeric columns.\n    \"\"\"\n    scaler = StandardScaler()\n    numeric_cols = data.select_dtypes(include=['number']).columns\n    for col in numeric_cols:\n        try:\n            data[col] = data[col].astype(float)\n            data[col] = scaler.fit_transform(data[[col]])\n        except ValueError:\n            # If conversion to float fails, leave the column unchanged\n            pass\n    return data"}
{"task_id": "BigCodeBench/421", "solution": "import requests\nimport os\nimport json\nimport time\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    if not url:\n        raise TypeError(\"The URL is invalid.\")\n    \n    status_codes = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as file:\n                files = {'file': (filename, file)}\n                response = requests.post(url, headers=HEADERS, files=files, data=json.dumps(metadata))\n                status_codes.append(response.status_code)\n                time.sleep(1)\n    return status_codes"}
{"task_id": "BigCodeBench/422", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    # Check if the column to remove exists in the dataframe\n    if column_to_remove in df.columns:\n        df = df.drop(columns=column_to_remove)\n    \n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    return X_train, X_test, y_train, y_test"}
{"task_id": "BigCodeBench/423", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    \n    # Check if the image was read successfully\n    if image is None:\n        raise ValueError(\"Failed to read the image.\")\n    \n    # Convert the image to grayscale\n    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # Validate the threshold\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n    \n    # Binarize the image\n    _, binarized_image = cv2.threshold(grayscale_image, threshold, 255, cv2.THRESH_BINARY)\n    \n    return grayscale_image, binarized_image"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    \n    # Check if the image was read successfully\n    if image is None:\n        raise FileNotFoundError(f\"Failed to read the image file {image_path}.\")\n    \n    # Reshape the image to a 2D array of pixels\n    pixels = image.reshape((-1, 3))\n    \n    # Initialize KMeans\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    \n    # Fit the model and predict the cluster labels\n    labels = kmeans.fit_predict(pixels)\n    \n    # Replace each pixel with its centroid\n    segmented_pixels = kmeans.cluster_centers_[labels]\n    \n    # Reshape back to the original image shape\n    segmented_image = segmented_pixels.reshape(image.shape)\n    \n    return image, segmented_image"}
{"task_id": "BigCodeBench/425", "solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    try:\n        # Read the image\n        image = cv2.imread(image_path)\n        if image is None:\n            raise FileNotFoundError(f\"Image file not found at {image_path}\")\n        \n        # Create a histogram of the image pixel intensities\n        hist = cv2.calcHist([image], [0], None, [256], [0,256])\n        \n        # Save the histogram as a PNG file\n        plt.plot(hist)\n        plt.savefig(histogram_path)\n        \n        # Display the original image and its histogram\n        plt.figure()\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.figure()\n        plt.plot(hist)\n        plt.title('Histogram')\n        \n        # Return the histogram plot object\n        return plt.gca()\n    \n    except FileNotFoundError as e:\n        print(e)\n        return None"}
{"task_id": "BigCodeBench/426", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    # Check if the threshold is an integer and within the range 0-255\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(\"Threshold must be an integer between 0 and 255.\")\n    \n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    \n    # Check if the image was read successfully\n    if image is None:\n        raise FileNotFoundError(f\"Failed to read the image file {image_path}.\")\n    \n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n    \n    # Save the binarized image\n    cv2.imwrite('binary_image.jpg', binary_image)\n    \n    # Return the original and binarized images as numpy arrays\n    return gray_image, binary_image"}
{"task_id": "BigCodeBench/427", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes based on the 'id' column\n    merged_df = pd.merge(df1, df2, on='id')\n    \n    # Prepare the features and target\n    X = merged_df[features]\n    y = merged_df[target]\n    \n    # Initialize and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n    \n    # Predict the target values\n    y_pred = model.predict(X)\n    \n    # Calculate residuals\n    residuals = y - y_pred\n    \n    # Plot the residuals\n    plt.figure()\n    plt.scatter(y_pred, residuals)\n    plt.title('Residuals Plot')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    residuals_plot = plt.gca()\n    \n    # Return the results in a dictionary\n    return {\n        'coefficients': coefficients.tolist(),\n        'intercept': intercept,\n        'residuals_plot': residuals_plot\n    }"}
{"task_id": "BigCodeBench/428", "solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df1, df2):\n    # Merge two dataframes on 'id' column using outer join\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n    \n    # Select numeric columns from df1\n    numeric_cols = df1.select_dtypes(include=['number']).columns\n    \n    # Scale the numeric features from df1 to have mean=0 and std=1\n    scaler = StandardScaler()\n    merged_df[numeric_cols] = scaler.fit_transform(merged_df[numeric_cols])\n    \n    # Create a pair plot of the scaled features from df1\n    pair_plot = sns.pairplot(merged_df[numeric_cols])\n    \n    return merged_df, pair_plot"}
{"task_id": "BigCodeBench/429", "solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2):\n    # Concatenate the two dataframes\n    df = pd.concat([df1, df2], axis=1)\n    \n    # Perform feature selection\n    selector = SelectKBest(f_classif, k=2)\n    selector.fit(df, df['target'])\n    selected_features = df.columns[selector.get_support()]\n    \n    # Create a correlation matrix for the selected features\n    corr_matrix = df[selected_features].corr()\n    \n    # Create a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Heatmap of Selected Features')\n    plt.show()\n    \n    # Return the selected features and the heatmap\n    return (list(selected_features), plt.gcf())\ndf1 = pd.DataFrame({\n    'feature1': [1, 2, 3, 4, 5],\n    'feature2': [10, 20, 30, 40, 50],\n    'feature3': [100, 200, 300, 400, 500],\n    'target': [0, 1, 0, 1, 0]\n})\ndf2 = pd.DataFrame({\n    'feature1': [6, 7, 8, 9, 10],\n    'feature2': [60, 70, 80, 90, 100],\n    'feature3': [600, 700, 800, 900, 1000],\n    'target': [1, 0, 1, 0, 1]\n})"}
{"task_id": "BigCodeBench/430", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge datasets\n    merged_df = pd.merge(df1, df2, on='id')\n    \n    # Extract features\n    X = merged_df[[column1, column2]].values\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(X)\n    \n    # Plotting\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n    plt.xlabel(column1)\n    plt.ylabel(column2)\n    plt.title('KMeans Clustering')\n    ax = plt.gca()\n    \n    return labels, ax\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [1, 2, 3], 'feature2': [1, 2, 3]})\ndf2 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [4, 5, 6], 'feature2': [4, 5, 6]})"}
{"task_id": "BigCodeBench/431", "solution": "import cv2\nimport os\nimport numpy as np\ndef task_func(image_file: str) -> np.ndarray:\n    if not os.path.exists(image_file):\n        raise FileNotFoundError(f\"The file {image_file} does not exist.\")\n    try:\n        image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n        if image is None:\n            raise ValueError(f\"The file {image_file} is not a valid image.\")\n    except Exception as e:\n        raise ValueError(f\"Error reading the image: {e}\")\n    \n    histogram = cv2.calcHist([image], [0], None, [256], [0, 256])\n    return histogram.flatten()"}
{"task_id": "BigCodeBench/432", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the two dataframes based on the 'id' column\n    merged_df = pd.merge(df1, df2, on='id')\n    \n    # Create a contingency table from the features in column1 and column2\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n    \n    # Perform a chi-square independence test on the contingency table\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Draw a heatmap of the contingency table\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(contingency_table, annot=True, fmt='d', cmap='YlGnBu')\n    plt.title('Contingency Table Heatmap')\n    \n    # Return the p-value and the heatmap\n    return (p, heatmap)\ndf1 = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'feature1': ['A', 'B', 'A', 'B'],\n    'feature2': [1, 2, 3, 4]\n})\ndf2 = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'feature1': ['A', 'B', 'A', 'B'],\n    'feature2': [1, 2, 3, 4]\n})"}
{"task_id": "BigCodeBench/433", "solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\ndef task_func(s, signature, secret_key):\n    try:\n        # Decode the base64-encoded message\n        message = base64.b64decode(s).decode('utf-8')\n    except binascii.Error:\n        # If decoding fails, return False\n        return False\n\n    # Create a new HMAC object with the secret key and SHA-1 hash algorithm\n    hmac_obj = hmac.new(secret_key.encode('utf-8'), message.encode('utf-8'), hashlib.sha1)\n\n    # Compute the hexadecimal representation of the HMAC\n    computed_signature = hmac_obj.hexdigest()\n\n    # Compare the computed signature with the provided signature\n    return computed_signature == signature"}
{"task_id": "BigCodeBench/434", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    random.seed(seed)\n    product_names = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    lines = s.split('\\n')\n    data = []\n    for line in lines:\n        parts = re.split(r'\\s+', line.strip())\n        if len(parts) == 5:\n            id, quantity, code, price, description = parts\n            product = random.choice(product_names)\n            data.append([id, quantity, code, price, product, description])\n    columns = ['ID', 'Quantity', 'Code', 'Price', 'Product', 'Description']\n    df = pd.DataFrame(data, columns=columns)\n    df['Quantity'] = df['Quantity'].astype(int)\n    df['Price'] = df['Price'].astype(int)\n    return df"}
{"task_id": "BigCodeBench/435", "solution": "import pandas as pd\nfrom random import randint\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'Developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles)-1)]\n    data = {\n        'Name': [name],\n        'Age': [age],\n        'Code': [code],\n        'Salary': [salary],\n        'Bio': [bio],\n        'Job Title': [job_title]\n    }\n    data_df = pd.DataFrame(data)\n    return data_df"}
{"task_id": "BigCodeBench/436", "solution": "import string\nimport matplotlib.pyplot as plt\ndef task_func(s):\n    if not isinstance(s, str):\n        raise ValueError(\"Input must be a string\")\n    \n    s = s.lower()\n    freq = {}\n    for char in s:\n        if char in string.ascii_lowercase:\n            if char in freq:\n                freq[char] += 1\n            else:\n                freq[char] = 1\n    \n    letters = list(freq.keys())\n    frequencies = list(freq.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(letters, frequencies)\n    ax.set_title('Letter Frequencies')\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    \n    return (freq, ax)"}
{"task_id": "BigCodeBench/437", "solution": "import pickle\nimport os\nimport pandas as pd\ndef task_func(df, file_name=\"save.pkl\"):\n    # Save the DataFrame to a pickle file\n    with open(file_name, 'wb') as f:\n        pickle.dump(df, f)\n    \n    # Read the DataFrame back from the pickle file\n    with open(file_name, 'rb') as f:\n        loaded_df = pickle.load(f)\n    \n    # Delete the intermediate file\n    os.remove(file_name)\n    \n    # Return the loaded DataFrame\n    return loaded_df"}
{"task_id": "BigCodeBench/438", "solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(num, (int, float)) for num in numbers):\n        raise TypeError(\"Input must be a list of numbers.\")\n    \n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n    plt.savefig(file_path)\n    plt.close(fig)\n    \n    with open(file_path, 'rb') as f:\n        loaded_fig = pickle.load(f)\n    \n    os.remove(file_path)\n    \n    return loaded_fig"}
{"task_id": "BigCodeBench/439", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(P, T):\n    # Calculate the product of matrix 'P' and 3D tensor 'T'\n    result = np.matmul(P, T)\n    \n    # Visualize the results as a heatmap\n    # Assuming the result is a 2D array\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(result, annot=True, fmt=\"g\")\n    plt.show()\n    \n    return result\nP = np.array([[1, 2], [3, 4]])\nT = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"}
{"task_id": "BigCodeBench/440", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(P, T):\n    # Check if the shapes of P and T are compatible for multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"The number of columns in P must match the number of rows in T for multiplication.\")\n    \n    # Perform matrix-tensor multiplication\n    result = np.dot(P, T)\n    \n    # Flatten the result to 2D array\n    flattened_result = result.reshape(-1, result.shape[-1])\n    \n    # Normalize the flattened result using StandardScaler\n    scaler = StandardScaler()\n    normalized_result = scaler.fit_transform(flattened_result)\n    \n    # Create a DataFrame from the normalized result\n    df = pd.DataFrame(normalized_result)\n    \n    # Rename columns\n    df.columns = [f\"feature_{i}\" for i in range(df.shape[1])]\n    \n    return df"}
{"task_id": "BigCodeBench/441", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(P, T):\n    # Calculate the product of matrix P and tensor T using Einstein summation\n    result = np.einsum('ij,jkl->ikl', P, T)\n    \n    # Reshape the result to (N, 3)\n    result = result.reshape(-1, 3)\n    \n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Plot the points\n    ax.scatter(result[:,0], result[:,1], result[:,2])\n    \n    # Set labels\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    \n    # Show the plot\n    plt.show()\n    \n    return (result, ax)\nP = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nT = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])"}
{"task_id": "BigCodeBench/442", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Reshape the tensor to a 2D array\n    T_reshaped = T.reshape(-1, np.prod(tensor_shape))\n    \n    # Calculate the product of P and T\n    product = np.dot(P, T_reshaped)\n    \n    # Apply PCA to reduce the dimensionality to 2D\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(product)\n    \n    # Plot the PCA result\n    ax = plt.axes()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n    \n    return pca_result, ax"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', flatten the result,\n    apply KMeans clustering to the flattened data, and visualize it.\n\n    Parameters:\n    - P (np.ndarray): A matrix.\n    - T (np.ndarray): A 3D tensor.\n    - n_clusters (int): Number of clusters for KMeans.\n    - random_state (int): Random state for KMeans.\n    - n_init (int): Number of times the KMeans algorithm will be run with different centroid seeds.\n\n    Returns:\n    - cluster_result (np.ndarray): The result of KMeans clustering.\n    - ax (plt.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n    \n    # Flatten the result\n    flattened = product.flatten()\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n    \n    # Visualize the clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax"}
{"task_id": "BigCodeBench/444", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    points = np.random.rand(n_points, 3)\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:,0], points[:,1], points[:,2])\n    plt.show()\n    return (points, ax)"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n\n    Parameters:\n    - points: A list of points in 2D space, where each point is represented as a list or tuple of two coordinates.\n    - seed: An integer seed for the random number generator used for jittering.\n\n    Returns:\n    - A tuple containing:\n        - vor: A Voronoi object representing the Voronoi diagram of the points.\n        - ax: The axes of the plotted Voronoi diagram.\n\n    Raises:\n    - TypeError: If points is not a list of lists or tuples.\n    - ValueError: If points do not have exactly two coordinates each.\n    \"\"\"\n\n    # Check if points is a list\n    if not isinstance(points, list):\n        raise TypeError(\"Points must be a list of points.\")\n\n    # Check if each point is a list or tuple with exactly two coordinates\n    for point in points:\n        if not (isinstance(point, list) or isinstance(point, tuple)) or len(point) != 2:\n            raise ValueError(\"Each point must be a list or tuple with exactly two coordinates.\")\n\n    # Convert points to a numpy array for easier manipulation\n    points = np.array(points)\n\n    # Apply jittering\n    np.random.seed(seed)\n    jitter = 0.1  # Define the jittering amount\n    points_jittered = points + np.random.uniform(-jitter, jitter, size=points.shape)\n\n    # Calculate Voronoi diagram\n    vor = Voronoi(points_jittered)\n\n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n\n    # Return the Voronoi object and the axes\n    return vor, ax\npoints = [[0, 0], [1, 4], [2, 3], [4, 1], [1, 1], [2, 2], [5, 3]]"}
{"task_id": "BigCodeBench/446", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n    ax = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    plt.show()\n    return X, y, ax"}
{"task_id": "BigCodeBench/447", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, n_components=2, random_state=None):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the provided dataset to reduce its dimensionality,\n    and visualizes the results using a scatter plot.\n\n    Parameters:\n    - data (np.ndarray): The input dataset to be reduced.\n    - n_components (int): The number of principal components to keep.\n    - random_state (int): Seed for the random number generator.\n\n    Returns:\n    - dict: A dictionary containing:\n        - \"transformed_data\" (np.ndarray): The transformed data.\n        - \"ax\" (plt.Axes): The scatter plot visualizing the transformed data.\n    \"\"\"\n    # Perform PCA\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    # Create scatter plot\n    if n_components == 1:\n        # For 1D, plot along X-axis with Y=0\n        ax = plt.scatter(transformed_data, np.zeros_like(transformed_data))\n    else:\n        # For 2D or more, plot first two components\n        ax = plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n\n    # Return the transformed data and the plot\n    return {\n        \"transformed_data\": transformed_data,\n        \"ax\": ax\n    }"}
{"task_id": "BigCodeBench/448", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n    return ax"}
{"task_id": "BigCodeBench/449", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n    standardized_data = pd.DataFrame(standardized_data, columns=data.columns)\n    \n    # Draw histograms for each feature\n    axes_list = []\n    fig, axes = plt.subplots(nrows=1, ncols=len(data.columns), figsize=(15, 3))\n    for i, col in enumerate(data.columns):\n        axes[col].hist(standardized_data[col], bins=20)\n        axes_list.append(axes[col])\n    \n    return standardized_data, axes_list"}
{"task_id": "BigCodeBench/450", "solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic 2D dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=2, random_state=random_seed)\n    \n    # Calculate Euclidean distances between all pairs of samples\n    distances = cdist(X, X, 'euclidean')\n    \n    # Visualize the dataset\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('Synthetic 2D Dataset')\n    \n    if plot_path is None:\n        # Return the matplotlib Axes object\n        return plt.gca()\n    else:\n        # Save the plot to the provided path and return None\n        plt.savefig(plot_path)\n        plt.close()\n        return None"}
{"task_id": "BigCodeBench/451", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    # Set the random seed for reproducibility\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    # Generate a high-dimensional dataset\n    X = np.random.rand(N_SAMPLES, N_FEATURES)\n    \n    # Run PCA to reduce dimensionality\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(X)\n    \n    # Calculate the covariance matrix of the transformed data\n    cov_matrix = np.cov(transformed_data.T)\n    \n    # Draw a heatmap of the covariance matrix\n    if n_components > 1:\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cov_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n        plt.title('Covariance Matrix of Transformed Data')\n        plt.show()\n    else:\n        heatmap_axes = None\n    \n    return transformed_data, heatmap_axes"}
{"task_id": "BigCodeBench/452", "solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    np.random.seed(random_seed)\n    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, noise=0.1)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n    \n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n    \n    # Calculate the mean squared error\n    mse = mean_squared_error(y_test, predictions)\n    \n    # Return the results\n    return (predictions, coefficients, intercept, mse)"}
{"task_id": "BigCodeBench/453", "solution": "import re\nimport string\nfrom random import choice\ndef task_func(n, pattern):\n    while True:\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        if re.match(pattern, random_string):\n            return random_string"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n    \n    files = glob.glob(os.path.join(src_dir, f'*{ext}'))\n    moved_files = []\n    for file in files:\n        file_name = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, file_name)\n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_file)\n            moved_files.append(dest_file)\n    return moved_files"}
{"task_id": "BigCodeBench/455", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, n):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(loc=mean, scale=std_dev, size=n)\n    \n    # Plot histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Generate x values for the PDF\n    x = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 100)\n    \n    # Calculate the PDF values\n    pdf = stats.norm.pdf(x, loc=mean, scale=std_dev)\n    \n    # Plot the PDF\n    plt.plot(x, pdf, 'k-', linewidth=2)\n    \n    # Show the plot\n    plt.show()\n    \n    return samples"}
{"task_id": "BigCodeBench/456", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    # Normalize the data to a range [0, 1]\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n    normalized_df = pd.DataFrame(normalized_data, columns=data.columns, index=data.index)\n    \n    # Visualize the normalized data using a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(normalized_df, cmap=\"YlGnBu\", cbar=True, cbar_kws={'label': 'Normalized Value'})\n    heatmap.set_title('Normalized Data Heatmap')\n    \n    return normalized_df, heatmap"}
{"task_id": "BigCodeBench/457", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    if not isinstance(L, list):\n        raise TypeError(\"Input must be a list of lists of integers.\")\n    \n    # Flatten the list and filter out empty sublists\n    flat_list = [item for sublist in L if sublist for item in sublist]\n    \n    if not flat_list:\n        raise ValueError(\"The list contains no elements to plot.\")\n    \n    # Convert the list to a pandas Series\n    series = pd.Series(flat_list)\n    \n    # Plot the histogram\n    ax = series.hist(bins=range(min(flat_list), max(flat_list)+2), rwidth=0.8)\n    \n    return ax"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    \n    if not data:\n        return pd.DataFrame()\n    \n    def normalize_value(value):\n        if isinstance(value, list):\n            return [normalize_value(item) for item in value]\n        elif isinstance(value, str):\n            try:\n                return float(value)\n            except ValueError:\n                return value\n        elif isinstance(value, (int, float)):\n            return value * 2\n        else:\n            return value\n    \n    normalized_data = {key: normalize_value(value) for key, value in data.items()}\n    \n    df = pd.DataFrame(normalized_data)\n    return df"}
{"task_id": "BigCodeBench/459", "solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"Delay cannot be negative\")\n    if not scripts:\n        raise ValueError(\"No scripts provided\")\n    timestamps = []\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.exists(script_path):\n            raise FileNotFoundError(f\"Script {script} not found in {script_dir}\")\n        start_time = datetime.now()\n        timestamps.append(start_time)\n        subprocess.run([script_path], shell=True)\n        time.sleep(delay)\n    return timestamps"}
{"task_id": "BigCodeBench/460", "solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(script_path, output_file_path):\n    try:\n        # Execute the script\n        subprocess.run(['python', script_path], check=True)\n    except subprocess.CalledProcessError as e:\n        raise ValueError(\"Script failed to execute\") from e\n\n    try:\n        # Read the CSV\n        df = pd.read_csv(output_file_path)\n        if df.shape[1] != 2:\n            raise ValueError(\"CSV must have exactly 2 columns\")\n    except pd.errors.EmptyDataError:\n        raise ValueError(\"CSV is empty\")\n    except pd.errors.ParserError:\n        raise ValueError(\"CSV is invalid\")\n\n    # Plot the bar graph\n    ax = df.plot(kind='bar', x=0, y=1, legend=False)\n\n    return df, ax"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n    \n    try:\n        process = subprocess.Popen(script_path, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to execute the script: {e}\")\n    \n    start_time = time.time()\n    cpu_usage = 0\n    memory_usage = 0\n    while True:\n        if process.poll() is not None:\n            break\n        if time.time() - start_time > timeout:\n            process.terminate()\n            raise TimeoutError(\"Script execution timed out.\")\n        \n        try:\n            current_process = psutil.Process(process.pid)\n            cpu_usage += current_process.cpu_percent(interval=1)\n            memory_usage += current_process.memory_info().rss\n        except psutil.NoSuchProcess:\n            raise RuntimeError(\"The process was not found.\")\n        except psutil.AccessDenied:\n            raise PermissionError(\"Access denied to the process.\")\n        \n        time.sleep(1)\n    \n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }"}
{"task_id": "BigCodeBench/462", "solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be at least 1\")\n    \n    random.seed(random_seed)\n    data = []\n    for _ in range(num_rows):\n        category = random.choice(categories)\n        value = random.randint(1, 100)\n        data.append({'category': category, 'value': value})\n    \n    df = pd.DataFrame(data)\n    \n    category_counts = df['category'].value_counts()\n    fig, ax = plt.subplots()\n    ax.bar(category_counts.index, category_counts.values)\n    ax.set_title('Category Counts')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/463", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_str, separator=\",\", bins=20):\n    # Split the string into a list of strings\n    data_list = data_str.split(separator)\n    \n    # Check if the list is empty\n    if not data_list:\n        raise ValueError(\"Data is empty\")\n    \n    # Convert the list of strings to a list of integers\n    try:\n        data_int = [int(item) for item in data_list]\n    except ValueError:\n        raise ValueError(\"Failed to convert data to integers\")\n    \n    # Create a pandas Series from the list of integers\n    series = pd.Series(data_int)\n    \n    # Plot a histogram with the specified attributes\n    axes = series.hist(grid=True, rwidth=0.9, color='#607c8e', bins=bins)\n    \n    # Return the series and the axes object\n    return (series, axes)"}
{"task_id": "BigCodeBench/464", "solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\ndef task_func(my_obj):\n    class CustomEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            if isinstance(obj, Decimal):\n                return float(obj)\n            return super().default(obj)\n    \n    return json.dumps(my_obj, cls=CustomEncoder)"}
{"task_id": "BigCodeBench/465", "solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, Decimal):\n            return float(obj)\n        return super().default(obj)\ndef task_func(my_obj):\n    try:\n        return json.dumps(my_obj, cls=CustomEncoder)\n    except TypeError as e:\n        raise TypeError(f\"Error serializing object: {e}\")"}
{"task_id": "BigCodeBench/466", "solution": "import json\nfrom enum import Enum\ndef task_func(my_obj):\n    class EnumEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, Enum):\n                return obj.name\n            return super().default(obj)\n    return json.dumps(my_obj, cls=EnumEncoder)"}
{"task_id": "BigCodeBench/467", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    x = np.random.rand(n)\n    y = np.random.rand(n)\n    points = list(zip(x, y))\n    fig = plt.figure()\n    plt.scatter(x, y)\n    plt.title(\"Scatter plot of random points\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    return fig, points"}
{"task_id": "BigCodeBench/468", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n    \n    # Convert numeric values into floats\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            try:\n                df[col] = df[col].astype(float)\n            except ValueError:\n                pass  # If conversion fails, leave the column as is\n    \n    # Extract the specified columns\n    data = df[columns]\n    \n    # Compute the cube-root of the data\n    cube_root = data.applymap(lambda x: x**(1/3) if pd.notnull(x) else x)\n    \n    # Draw a line chart of the data\n    fig, ax = plt.subplots()\n    for col in columns:\n        ax.plot(data[col], label=col)\n    ax.legend()\n    \n    # Return the DataFrame, Axes, and Series\n    return df, ax, cube_root"}
{"task_id": "BigCodeBench/469", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Convert student_grades to uppercase to handle case-insensitivity\n    student_grades = [grade.upper() for grade in student_grades]\n    \n    # Filter out grades not in possible_grades\n    filtered_grades = [grade for grade in student_grades if grade in possible_grades]\n    \n    # Count the occurrences of each grade\n    grade_counts = Counter(filtered_grades)\n    \n    # Create a DataFrame from the grade counts\n    df = pd.DataFrame(list(grade_counts.items()), columns=['Grade', 'Count'])\n    df.set_index('Grade', inplace=True)\n    \n    # Create a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Grade Distribution')\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    \n    return df, ax\nstudent_grades = [\"A\", \"B\", \"C\", \"D\", \"F\", \"a\", \"b\", \"c\", \"d\", \"f\", \"A\", \"B\", \"C\", \"D\", \"F\", \"G\", \"H\", \"I\", \"J\"]"}
{"task_id": "BigCodeBench/470", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(myList):\n    # Convert the list to a numpy array for easier manipulation\n    data = np.array(myList)\n    \n    # Determine the bin edges based on the unique values in the list\n    unique_values = np.unique(data)\n    bin_edges = np.sort(unique_values)\n    \n    # Create the histogram\n    ax = plt.hist(data, bins=bin_edges, edgecolor='black')\n    \n    # Set labels and title\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n    ax[0].set_title('Histogram of Values')\n    \n    # Return the Axes object\n    return ax[0]\nmyList = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]"}
{"task_id": "BigCodeBench/471", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(myList):\n    # Count the frequency of each word in the list\n    word_counts = Counter(myList)\n    \n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n    \n    return df"}
{"task_id": "BigCodeBench/472", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\ndef task_func(myList, n_clusters):\n    if not isinstance(myList, list) or not all(isinstance(point, list) and len(point) == 2 for point in myList):\n        raise ValueError(\"Input must be a list of 2D points.\")\n    if not isinstance(n_clusters, int) or n_clusters < 1:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans.fit(myList)\n    \n    plt.scatter([point[0] for point in myList], [point[1] for point in myList], c=kmeans.labels_)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='x')\n    plt.show()\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/473", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(n_walks, n_steps, seed=None):\n    \"\"\"\n    Create and plot n_walks number of random walks, each with n_steps steps.\n    The function checks for valid n_walks and n_steps, then generates walks via numpy.\n    Each walk is plotted in a different color cycling through a predefined set of colors: ['b', 'g', 'r', 'c', 'm', 'y', 'k'].\n    The function should output with:\n        ax (plt.Axes): A Matplotlib Axes containing the plotted random walks.\n    \"\"\"\n    # Define the colors to cycle through\n    colors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n    \n    # Check if n_walks and n_steps are integers and n_steps is positive\n    if not isinstance(n_walks, int) or not isinstance(n_steps, int) or n_steps < 1:\n        raise ValueError(\"n_walks and n_steps must be positive integers.\")\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Initialize the figure and axis\n    fig, ax = plt.subplots()\n    \n    # Generate and plot each random walk\n    for _ in range(n_walks):\n        # Generate a random walk\n        walk = np.cumsum(np.random.randn(n_steps))\n        # Plot the walk with the next color in the cycle\n        ax.plot(walk, color=next(colors))\n    \n    # Return the axis with the plotted random walks\n    return ax"}
{"task_id": "BigCodeBench/474", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    \"\"\"\n    Generates a histogram and a probability density function (PDF) plot for a specified normal distribution.\n    \n    Parameters:\n    - n_samples (int): Number of samples to draw from the normal distribution.\n    - mu (float): Mean of the normal distribution.\n    - sigma (float): Standard deviation of the normal distribution.\n    - random_seed (int): Seed for the random number generator.\n    \n    Returns:\n    - ax (matplotlib.axes._axes.Axes): Axes object with the histogram and PDF plotted.\n    - samples (numpy.ndarray): Generated sample data.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate samples from the normal distribution\n    samples = np.random.normal(mu, sigma, n_samples)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram with normalized density\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Generate x values for the PDF\n    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)\n    \n    # Plot the PDF of the normal distribution\n    pdf = norm.pdf(x, mu, sigma)\n    ax.plot(x, pdf, 'r', linewidth=2)\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram and PDF of Normal Distribution')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax, samples"}
{"task_id": "BigCodeBench/475", "solution": "import pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\ndef task_func(data, date_format, country, country_codes=None):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pandas DataFrame\")\n    if not isinstance(date_format, str):\n        raise ValueError(\"date_format must be a string\")\n    if country_codes is not None and not isinstance(country_codes, dict):\n        raise ValueError(\"country_codes must be a dictionary\")\n    if country_codes is not None and country not in country_codes.values():\n        raise ValueError(\"country is not in country_codes\")\n    \n    # Assuming the DataFrame has a column named 'date' that needs to be converted\n    data['date'] = pd.to_datetime(data['date'], format=date_format)\n    \n    # Extract the year from the date\n    data['year'] = data['date'].dt.year\n    \n    # Filter the data for the specified country\n    if country_codes is not None:\n        country_code = [k for k, v in country_codes.items() if v == country][0]\n        data = data[data['country_code'] == country_code]\n    \n    # Plot the histogram\n    ax = data['year'].value_counts().sort_index().plot(kind='bar')\n    ax.set_title('Date Distribution')\n    ax.set_ylabel('Frequency')\n    \n    return ax"}
{"task_id": "BigCodeBench/476", "solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\ndef task_func(X, Y):\n    def quadratic_func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, _ = curve_fit(quadratic_func, X, Y)\n    a, b, c = popt\n\n    plt.scatter(X, Y, label='Data points')\n    plt.plot(X, quadratic_func(X, a, b, c), 'r', label='Quadratic fit')\n    plt.legend()\n    plt.show()\n\n    return (a, b, c), plt.gca()\nX = [1, 2, 3, 4, 5]\nY = [2, 4, 5, 7, 11]"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    if N < len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n    else:\n        categories = np.random.choice(CATEGORIES, size=N, replace=True)\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    df = pd.DataFrame({\"x\": x, \"y\": y, \"category\": categories})\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(x, y, c=categories, cmap='viridis')\n    return df, ax"}
{"task_id": "BigCodeBench/478", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    df = pd.DataFrame(data_list, columns=['Original String'])\n    df['Modified String'] = df['Original String'].apply(lambda x: re.sub(r',[^,]*$', '', x))\n    return df"}
{"task_id": "BigCodeBench/479", "solution": "import random\nimport string\nimport pandas as pd\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    result = []\n    for s in data_list:\n        if ',' in s:\n            parts = s.split(',')\n            if len(parts) > 1:\n                # Find the positions of commas\n                comma_positions = [i for i, char in enumerate(s) if char == ',']\n                # Select a random substring between two commas or at the beginning/end\n                if len(comma_positions) > 1:\n                    # Choose two random positions for the substring\n                    start_pos = random.choice(comma_positions)\n                    end_pos = random.choice(comma_positions)\n                    if start_pos > end_pos:\n                        start_pos, end_pos = end_pos, start_pos\n                elif len(comma_positions) == 1:\n                    # Choose between the beginning and the end\n                    if comma_positions[0] == 0:\n                        # Only the beginning\n                        start_pos = 0\n                        end_pos = comma_positions[0] + 1\n                    else:\n                        # Only the end\n                        start_pos = comma_positions[0]\n                        end_pos = len(s)\n                else:\n                    # No commas, so no substring to replace\n                    result.append((s, s))\n                    continue\n                # Extract the substring to replace\n                substring = s[start_pos:end_pos]\n                # Generate a random string of the same length\n                random_string = ''.join(random.choices(string.ascii_lowercase, k=len(substring)))\n                # Replace the substring\n                modified_s = s[:start_pos] + random_string + s[end_pos:]\n                result.append((s, modified_s))\n            else:\n                # No commas, so no substring to replace\n                result.append((s, s))\n        else:\n            # No commas, so no substring to replace\n            result.append((s, s))\n    # Create a DataFrame\n    df = pd.DataFrame(result, columns=['Original String', 'Modified String'])\n    return df"}
{"task_id": "BigCodeBench/480", "solution": "import re\nimport random\nimport pandas as pd\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    result = []\n    for string in data_list:\n        # Split the string by commas and strip whitespace\n        substrings = [s.strip() for s in re.split(r',\\s*', string) if s.strip()]\n        # Shuffle the substrings\n        random.shuffle(substrings)\n        # Join the shuffled substrings back into a string\n        shuffled_string = ', '.join(substrings)\n        # Append the original and shuffled strings to the result list\n        result.append({'Original String': string, 'Shuffled String': shuffled_string})\n    # Convert the result list to a pandas DataFrame\n    df = pd.DataFrame(result)\n    return df"}
{"task_id": "BigCodeBench/481", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    result = []\n    for string in data_list:\n        # Normalize spaces to ensure a single space follows each comma\n        normalized_string = re.sub(r'(\\s*,\\s*)', ', ', string)\n        # Split the string into substrings based on commas\n        substrings = normalized_string.split(', ')\n        # Randomize the order of substrings\n        random.shuffle(substrings)\n        # Join the randomized substrings back into a string\n        randomized_string = ', '.join(substrings)\n        result.append({'Original String': string, 'Randomized String': randomized_string})\n    return pd.DataFrame(result)\ndata_list = [\n    \"apple, banana, cherry\",\n    \"dog, cat, mouse\",\n    \"red, blue, green\"\n]"}
{"task_id": "BigCodeBench/482", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    operations = ['remove', 'replace', 'shuffle', 'randomize']\n    operation = random.choice(operations)\n    \n    df = pd.DataFrame({'Original String': data_list})\n    \n    for index, row in df.iterrows():\n        string = row['Original String']\n        substrings = re.split(r',\\s*', string)\n        \n        if operation == 'remove':\n            if len(substrings) > 1:\n                substrings.remove(random.choice(substrings))\n        elif operation == 'replace':\n            substrings[random.randint(0, len(substrings)-1)] = 'random_string'\n        elif operation == 'shuffle':\n            random.shuffle(substrings)\n        elif operation == 'randomize':\n            random.shuffle(substrings)\n        \n        modified_string = ', '.join(substrings)\n        df.at[index, 'Modified String'] = modified_string\n    \n    return df"}
{"task_id": "BigCodeBench/483", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    if pattern == \"\":\n        return df.copy()\n    \n    def reverse_words(match):\n        return ' '.join(reversed(match.group().split()))\n    \n    df = df.copy()\n    df[column_name] = df[column_name].astype(str)\n    df[column_name] = df[column_name].str.replace(pattern, reverse_words, regex=True)\n    return df"}
{"task_id": "BigCodeBench/484", "solution": "import math\nimport numpy as np\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=step)\n    \n    # Initialize a list to hold the sensor readings\n    data = []\n    \n    for timestamp in timestamps:\n        # Convert timestamp to seconds since the epoch\n        time_in_seconds = timestamp.timestamp()\n        \n        # Generate sensor readings\n        sensor1 = math.sin(time_in_seconds) + np.random.normal(0, 0.1)\n        sensor2 = math.cos(time_in_seconds) + np.random.normal(0, 0.1)\n        sensor3 = math.tan(time_in_seconds) + np.random.normal(0, 0.1)\n        \n        # Randomly select a sensor status\n        sensor_status = np.random.choice(sensor_statuses)\n        \n        # Append the data to the list\n        data.append({\n            \"Timestamp\": timestamp,\n            \"Sensor1\": sensor1,\n            \"Sensor2\": sensor2,\n            \"Sensor3\": sensor3,\n            \"SensorStatus\": sensor_status\n        })\n    \n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n    \n    # Reorder columns if necessary\n    df = df[columns]\n    \n    return df"}
{"task_id": "BigCodeBench/485", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time):\n    # Define the time zones\n    time_zones = {\n        'UTC': 'UTC',\n        'America/Los_Angeles': 'America/Los_Angeles',\n        'Europe/Paris': 'Europe/Paris',\n        'Asia/Kolkata': 'Asia/Kolkata',\n        'Australia/Sydney': 'Australia/Sydney'\n    }\n    \n    # Create a list to hold the time differences\n    time_diffs = []\n    \n    # Create a list to hold the dates\n    dates = []\n    \n    # Create a list to hold the time zone names\n    tz_names = []\n    \n    # Create a list to hold the colors for each time zone\n    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n    \n    # Loop through each day in the date range\n    current_date = start_time\n    while current_date <= end_time:\n        # Convert the current date to each time zone and calculate the difference with UTC\n        for tz_name, tz_str in time_zones.items():\n            tz = pytz.timezone(tz_str)\n            tz_time = tz.localize(current_date)\n            diff = tz_time.utcoffset().total_seconds() / 3600\n            time_diffs.append(diff)\n            dates.append(current_date)\n            tz_names.append(tz_name)\n        \n        # Move to the next day\n        current_date += timedelta(days=1)\n    \n    # Convert lists to numpy arrays for plotting\n    dates = np.array(dates)\n    time_diffs = np.array(time_diffs)\n    tz_names = np.array(tz_names)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot each time zone's time difference curve\n    for tz_name in np.unique(tz_names):\n        # Get the indices for this time zone\n        indices = np.where(tz_names == tz_name)\n        # Plot the time differences for this time zone\n        ax.plot(dates[indices], time_diffs[indices], label=tz_name, color=colors.pop(0))\n    \n    # Set the title and labels\n    ax.set_title('Hourly Difference Between UTC and Specified Time Zones')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Time Difference (hours)')\n    \n    # Set the x-axis to show dates\n    ax.xaxis.set_major_formatter(plt.DateFormatter('%Y-%m-%d'))\n    ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n    \n    # Add a legend\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate timestamps\n    start = datetime.fromisoformat(start_time)\n    end = datetime.fromisoformat(end_time)\n    timestamps = pd.date_range(start, end, freq=f'{step}S')\n    \n    # Generate random values from a normal distribution\n    values = np.random.normal(loc=0, scale=1, size=len(timestamps))\n    \n    # Add a linear trend\n    values += trend * np.arange(len(timestamps))\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n    \n    # Plot the time series\n    ax = df.plot(x='Time', y='Value', figsize=(10, 5))\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    \n    return ax"}
{"task_id": "BigCodeBench/487", "solution": "import os\nimport pandas as pd\nimport re\ndef task_func(file_path: str) -> pd.DataFrame:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{6}) - (\\w+) - (.*)')\n    data = []\n    \n    with open(file_path, 'r') as file:\n        for line in file:\n            match = pattern.match(line)\n            if match:\n                timestamp, level, message = match.groups()\n                data.append({'Timestamp': timestamp, 'Level': level, 'Message': message})\n    \n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/488", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    \"\"\"\n    Generate a time series with a given seasonality from the start UTC time to the end UTC time with a given step, and plot the time series with the seasonality.\n\n    Parameters:\n    - start_time (str): Start time in UTC, in the format 'YYYY-MM-DDTHH:MM:SS'.\n    - end_time (str): End time in UTC, in the format 'YYYY-MM-DDTHH:MM:SS'.\n    - step (int): Time step in seconds.\n    - amplitude (float): Amplitude of the seasonality.\n    - period (int): Period of the seasonality in days.\n    - seed (int, optional): Seed for the random number generator. Defaults to 0.\n\n    Returns:\n    - matplotlib.pyplot.Axes: A plot of the generated 'Time Series with Seasonality', with 'Timestamp' on x-axis and 'Value' on y-axis.\n    \"\"\"\n\n    # Convert start and end times to datetime objects\n    start = datetime.fromisoformat(start_time)\n    end = datetime.fromisoformat(end_time)\n\n    # Generate timestamps\n    timestamps = pd.date_range(start, end, freq=f'{step}S')\n\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random noise\n    noise = np.random.normal(0, 1, len(timestamps))\n\n    # Generate seasonality\n    seasonality = amplitude * np.sin(2 * np.pi * (timestamps - start).dt.total_seconds() / (period * 86400))\n\n    # Generate time series values\n    values = noise + seasonality\n\n    # Create a DataFrame\n    df = pd.DataFrame({'Timestamp': timestamps, 'Value': values})\n\n    # Plot the time series\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Timestamp'], df['Value'], label='Time Series with Seasonality')\n    plt.xlabel('Timestamp')\n    plt.ylabel('Value')\n    plt.title('Time Series with Seasonality')\n    plt.legend()\n    plt.grid(True)\n\n    # Return the plot\n    return plt.gca()"}
{"task_id": "BigCodeBench/489", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(epoch_milliseconds, seed=0):\n    # Set the seed for reproducibility\n    random.seed(seed)\n    \n    # Convert epoch time to datetime\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    \n    # Check if start time is after current time\n    if start_time > current_time:\n        raise ValueError(\"Start time is after the current system time.\")\n    \n    # Define lists of users and activities\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n    \n    # Initialize an empty list to store logs\n    logs = []\n    \n    # Start from the start time\n    current = start_time\n    \n    # Loop until current time is reached\n    while current <= current_time:\n        # Randomly select a user and an activity\n        user = random.choice(users)\n        activity = random.choice(activities)\n        \n        # Append the log entry\n        logs.append({'User': user, 'Activity': activity, 'Time': current})\n        \n        # Increment time by a random number of seconds between 1 and 10\n        current += timedelta(seconds=random.randint(1, 10))\n    \n    # Convert logs to DataFrame\n    df = pd.DataFrame(logs)\n    \n    return df"}
{"task_id": "BigCodeBench/490", "solution": "import xmltodict\nimport json\ndef task_func(s, file_path):\n    # Convert the XML string to a dictionary\n    dict_data = xmltodict.parse(s)\n    \n    # Save the dictionary as a JSON file\n    with open(file_path, 'w') as f:\n        json.dump(dict_data, f, indent=4)\n    \n    return dict_data"}
{"task_id": "BigCodeBench/491", "solution": "import random\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\ndef task_func(epoch_milliseconds, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    start_time = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.utcnow()\n    \n    if epoch_milliseconds < 0 or start_time > current_time:\n        raise ValueError(\"Start time is negative or after the current time.\")\n    \n    delta = current_time - start_time\n    days = delta.days\n    sales_data = {}\n    \n    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n    for category in categories:\n        sales_data[category] = [random.randint(10, 50) for _ in range(days)]\n    \n    x = range(days)\n    fig, ax = plt.subplots()\n    for category in categories:\n        ax.plot(x, sales_data[category], label=category)\n    \n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Sales units')\n    ax.set_title('Sales Trend Over Time')\n    ax.legend()\n    \n    return sales_data, ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(epoch_milliseconds, random_seed=0, products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"]):\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_date = datetime.now()\n    \n    # Check if start_date is in the future\n    if start_date > current_date:\n        raise ValueError(\"Start date is in the future.\")\n    \n    # Set random seed\n    random.seed(random_seed)\n    \n    # Initialize list to hold sales data\n    sales_data = []\n    \n    # Generate sales data for each day between start_date and current_date\n    current = start_date\n    while current <= current_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({'Product': product, 'Date': current, 'Sales': sales})\n        current += timedelta(days=1)\n    \n    # Create DataFrame from sales data\n    df = pd.DataFrame(sales_data)\n    \n    return df"}
{"task_id": "BigCodeBench/493", "solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Convert epoch milliseconds to datetime object\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_date = datetime.now()\n    \n    # Calculate the number of days between start_date and current_date\n    delta = current_date - start_date\n    num_days = delta.days + 1  # +1 to include the current day\n    \n    # Generate performance data for each team\n    performance_data = {}\n    for team in teams:\n        performance_data[team] = [round(random.uniform(0.1, 1), 2) for _ in range(num_days)]\n    \n    # Plot the performance trend for each team\n    fig, ax = plt.subplots()\n    for team, performances in performance_data.items():\n        ax.plot(range(num_days), performances, label=team)\n    \n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Performance')\n    ax.set_title('Performance Trend Over Time')\n    ax.legend()\n    \n    return performance_data, fig"}
{"task_id": "BigCodeBench/494", "solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    fake = Faker()\n    Faker.seed(seed)\n    event_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    event_name = fake.word()\n    valid_timezones = []\n    for tz in timezones:\n        if tz in pytz.all_timezones:\n            valid_timezones.append(tz)\n        else:\n            match = re.match(r'UTC([+-]\\d{2}:\\d{2})', tz)\n            if match:\n                valid_timezones.append(tz)\n    if not valid_timezones:\n        timezone = 'UTC'\n    else:\n        timezone = fake.random_element(valid_timezones)\n    tz = pytz.timezone(timezone)\n    event_time = tz.localize(event_time)\n    schedule = {\n        'date': event_time.strftime('%Y-%m-%d'),\n        'time': event_time.strftime('%H:%M:%S'),\n        'timezone': timezone\n    }\n    return {event_name: [schedule]}"}
{"task_id": "BigCodeBench/495", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(days, random_seed=0):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Create a date range starting from '2023-01-01' for the specified number of days\n    dates = pd.date_range(start='2023-01-01', periods=days)\n    \n    # Define the columns for the DataFrame\n    columns = ['Groceries', 'Entertainment', 'Rent', 'Utilities', 'Miscellaneous']\n    \n    # Generate random spending data for each category\n    data = {\n        'Groceries': np.random.randint(0, 101, days),\n        'Entertainment': np.random.randint(0, 101, days),\n        'Rent': np.random.randint(0, 101, days),\n        'Utilities': np.random.randint(0, 101, days),\n        'Miscellaneous': np.random.randint(0, 101, days)\n    }\n    \n    # Create the DataFrame\n    df = pd.DataFrame(data, index=dates)\n    \n    return df"}
{"task_id": "BigCodeBench/496", "solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be at least 1\")\n    \n    np.random.seed(random_seed)\n    dates = [datetime.now() - timedelta(days=i) for i in range(days_in_past)]\n    temperatures = np.random.randint(15, 36, days_in_past)\n    \n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures, marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Temperature (\u00b0C)')\n    ax.set_title('Temperature Trend')\n    \n    return ax"}
{"task_id": "BigCodeBench/497", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"days_in_past cannot be negative\")\n    today = datetime.now(pytz.utc)\n    target_date = today - timedelta(days=days_in_past)\n    weekday = target_date.weekday()\n    return calendar.day_name[weekday]"}
{"task_id": "BigCodeBench/498", "solution": "import xmltodict\nimport json\ndef task_func(s, save_json, json_file_path):\n    if not s or s.isspace():\n        raise ValueError(\"Input XML string is empty or contains only whitespace.\")\n    \n    # Convert XML string to dictionary\n    result = xmltodict.parse(s)\n    \n    # Save the dictionary as a JSON file if save_json is True\n    if save_json:\n        with open(json_file_path, 'w') as f:\n            json.dump(result, f, indent=4)\n    \n    return result"}
{"task_id": "BigCodeBench/499", "solution": "import xlwt\nimport os\nimport io\nimport csv\ndef task_func(csv_content, filename):\n    # Create a new workbook\n    workbook = xlwt.Workbook()\n    \n    # Add a sheet to the workbook\n    sheet = workbook.add_sheet('Sheet1')\n    \n    # Write the CSV content into the sheet\n    if isinstance(csv_content, str):\n        csv_content = [csv_content]\n    for i, row in enumerate(csv_content):\n        for j, cell in enumerate(row.split(',')):\n            sheet.write(i, j, cell.strip())\n    \n    # Save the workbook to a file\n    file_path = os.path.abspath(filename)\n    workbook.save(file_path)\n    \n    # Return the absolute path of the created Excel file\n    return file_path\ncsv_content = 'Hello'"}
{"task_id": "BigCodeBench/500", "solution": "import xlwt\nimport os\nFIELDS = ['ID', 'Name', 'Age']\ndef task_func(values, filename):\n    # Create a new workbook\n    workbook = xlwt.Workbook()\n    \n    # Add a sheet to the workbook\n    sheet = workbook.add_sheet('Sheet1')\n    \n    # Write the field names as the first row\n    for col_num, field in enumerate(FIELDS):\n        sheet.write(0, col_num, field)\n    \n    # Write the data rows\n    for row_num, data in enumerate(values, start=1):\n        for col_num, field in enumerate(FIELDS):\n            sheet.write(row_num, col_num, data.get(field, ''))\n    \n    # Save the workbook to the specified filename\n    workbook.save(filename)\n    \n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\nempty_data = []"}
{"task_id": "BigCodeBench/501", "solution": "import json\nimport pandas as pd\nimport os"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    df = pd.DataFrame(columns=[\"Date\", \"Activity\", \"Duration\"])\n    today = datetime.today()\n    for day in range(days_in_past):\n        date = today - timedelta(days=day)\n        for activity in activities:\n            duration = random.randint(0, 120)\n            df = df.append({\"Date\": date.strftime(\"%Y-%m-%d\"), \"Activity\": activity, \"Duration\": duration}, ignore_index=True)\n    df = df.sort_values(by=[\"Date\", \"Activity\"])\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n    plt.show()\n    return (ax, df)"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Create a list to hold the data for each stock\n    data = []\n    \n    # Get the current date\n    today = datetime.now()\n    \n    # Generate data for each stock\n    for stock in stock_names:\n        # Create a list to hold the prices for this stock\n        prices = []\n        \n        # Generate random prices for the specified number of days\n        for _ in range(days_in_past):\n            # Generate a random price between 0.0 and 1.0\n            price = np.random.rand()\n            prices.append(price)\n        \n        # Create a list of dates for the past days\n        dates = [today - timedelta(days=i) for i in range(days_in_past)]\n        \n        # Create a DataFrame for this stock\n        df = pd.DataFrame({\n            \"Date\": dates,\n            \"Price\": prices\n        })\n        \n        # Append the DataFrame to the data list\n        data.append(df)\n    \n    # Concatenate all the DataFrames into one\n    combined_df = pd.concat(data, keys=stock_names)\n    \n    # Reset the index to make it more readable\n    combined_df = combined_df.reset_index(level=1, drop=True)\n    \n    # Rename the 'level_0' column to 'Stock'\n    combined_df = combined_df.rename(columns={\"level_0\": \"Stock\"})\n    \n    # Reorder the columns\n    combined_df = combined_df[[\"Stock\", \"Date\", \"Price\"]]\n    \n    return combined_df"}
{"task_id": "BigCodeBench/504", "solution": "import hashlib\nimport rsa\nimport base64\ndef task_func(file_path):\n    with open(file_path, 'rb') as f:\n        file_contents = f.read()\n    file_hash = hashlib.sha256(file_contents).digest()\n    with open('private.pem', 'rb') as f:\n        private_key = rsa.PrivateKey.load_pkcs1(f.read())\n    signed_hash = rsa.sign(file_hash, private_key, 'SHA-256')\n    return base64.b64encode(signed_hash).decode('utf-8')"}
{"task_id": "BigCodeBench/505", "solution": "import hashlib\nimport hmac\ndef task_func(secret, message):\n    # Create a new HMAC object with the secret key and SHA-256 hash function\n    hmac_obj = hmac.new(secret.encode(), message.encode(), hashlib.sha256)\n    # Return the hexadecimal representation of the HMAC signature\n    return hmac_obj.hexdigest()"}
{"task_id": "BigCodeBench/506", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        return {\n            'sum': np.nan,\n            'mean': np.nan,\n            'min': np.inf,\n            'max': -np.inf,\n            'plot': None\n        }\n    \n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        return {\n            'sum': np.nan,\n            'mean': np.nan,\n            'min': np.inf,\n            'max': -np.inf,\n            'plot': None\n        }\n    \n    col_data = df[column]\n    result = {\n        'sum': col_data.sum(),\n        'mean': col_data.mean(),\n        'min': col_data.min(),\n        'max': col_data.max(),\n        'plot': None\n    }\n    \n    fig, ax = plt.subplots()\n    ax.hist(col_data, bins=10)\n    ax.set_title(f'Histogram of {column}')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Frequency')\n    plt.show()\n    \n    return result"}
{"task_id": "BigCodeBench/507", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(column, data):\n    if not data:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise ValueError(\"Invalid column name\")\n    sum_val = df[column].sum()\n    mean_val = df[column].mean()\n    min_val = df[column].min()\n    max_val = df[column].max()\n    return {'sum': sum_val, 'mean': mean_val, 'min': min_val, 'max': max_val}"}
{"task_id": "BigCodeBench/508", "solution": "import hashlib\nimport io\nimport os\ndef task_func(file_path1, file_path2):\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"File {file_path1} does not exist.\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"File {file_path2} does not exist.\")\n    \n    with open(file_path1, 'rb') as f1:\n        file1_content = f1.read()\n    with open(file_path2, 'rb') as f2:\n        file2_content = f2.read()\n    \n    file1_hash = hashlib.md5(file1_content).hexdigest()\n    file2_hash = hashlib.md5(file2_content).hexdigest()\n    \n    return file1_hash == file2_hash"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r', newline='', encoding='utf-8') as f1, open(file_path2, 'r', newline='', encoding='utf-8') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            \n            lines1 = list(reader1)\n            lines2 = list(reader2)\n            \n            if not lines1:\n                raise ValueError(\"First file is empty\")\n            if not lines2:\n                raise ValueError(\"Second file is empty\")\n            \n            df = pd.DataFrame(columns=['Line Number', 'Status', 'Content'])\n            for i in range(max(len(lines1), len(lines2))):\n                line1 = lines1[i] if i < len(lines1) else []\n                line2 = lines2[i] if i < len(lines2) else []\n                \n                if line1 == line2:\n                    status = ' '\n                    content = ' '.join(line1)\n                else:\n                    diff = ndiff(line1, line2)\n                    status = ''.join(['+' if c[0] == '+' else '-' if c[0] == '-' else ' ' for c in diff])\n                    content = ' '.join(line1 + line2)\n                \n                df = df.append({'Line Number': i+1, 'Status': status, 'Content': content}, ignore_index=True)\n            \n            return df\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both files not found\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/510", "solution": "import difflib\nimport gzip\ndef task_func(file_path1, file_path2):\n    with gzip.open(file_path1, 'rt') as f1:\n        content1 = f1.read()\n    with gzip.open(file_path2, 'rt') as f2:\n        content2 = f2.read()\n    differ = difflib.Differ()\n    diff = list(differ.compare(content1.splitlines(), content2.splitlines()))\n    if diff:\n        return '\\n'.join(diff)\n    else:\n        return ''"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, plt.gca())\n    \n    df = pd.DataFrame(data, columns=[column])\n    stats = {'sum': df[column].sum(), 'mean': df[column].mean(), 'min': df[column].min(), 'max': df[column].max()}\n    \n    fig, ax = plt.subplots()\n    ax.pie(df[column], labels=df[column], autopct='%1.1f%%')\n    ax.axis('equal')\n    \n    return (stats, ax)"}
{"task_id": "BigCodeBench/512", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(\"Column not found in the data.\")\n    \n    # Check if all values in the column are non-negative\n    if (df[column] < 0).any():\n        raise ValueError(\"Negative values found in the column.\")\n    \n    # Calculate sum, mean, min, and max of the column\n    sum_val = df[column].sum()\n    mean_val = df[column].mean()\n    min_val = df[column].min()\n    max_val = df[column].max()\n    \n    # Create a dictionary with the calculated values\n    stats = {\n        'sum': sum_val,\n        'mean': mean_val,\n        'min': min_val,\n        'max': max_val\n    }\n    \n    # Create a bar chart for the column\n    ax = df.plot(kind='bar', x='Product', y=column, title=f'Bar Chart of {column}')\n    \n    # Return the dictionary and the Axes object\n    return (stats, ax)\ndata = {\n    'Product': ['A', 'B', 'C', 'D'],\n    'Quantity Sold': [10, 20, 30, 40],\n    'Total Sales': [100, 200, 300, 400]\n}"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise KeyError(\"The specified column is not valid.\")\n    \n    if not df[column].dtype in [np.int64, np.float64]:\n        raise TypeError(\"The specified column is not numeric.\")\n    \n    if any(df[column] < 0):\n        raise ValueError(\"Negative values are not allowed in the specified column.\")\n    \n    summary = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    plt.plot(df['Date'], df[column])\n    plt.title(f'Line Chart of {column}')\n    plt.xlabel('Date')\n    plt.ylabel(column)\n    plt.show()\n    \n    return (summary, plt.gca())"}
{"task_id": "BigCodeBench/514", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    # Create a DataFrame from the 2D list\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n    \n    # Calculate the sum of each column\n    sums = df.sum()\n    \n    # Plot the sum of each column\n    ax = sums.plot(kind='bar')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/515", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    if not array or not all(len(sublist) == 5 for sublist in array):\n        raise ValueError(\"Input array must be non-empty and contain sublists of length 5.\")\n    \n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n    corr = df.corr()\n    heatmap = sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n    \n    return df, heatmap"}
{"task_id": "BigCodeBench/516", "solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import RegressionResultsWrapper\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, RegressionResultsWrapper):\n    # Validate input list\n    if not isinstance(array, list):\n        raise ValueError(\"Input must be a list.\")\n    if not all(isinstance(sublist, list) for sublist in array):\n        raise ValueError(\"All elements of the input list must be lists.\")\n    if not all(len(sublist) == len(array[0]) for sublist in array):\n        raise ValueError(\"All sublists must have the same length.\")\n    \n    # Create DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'Response'])\n    \n    # Separate independent and dependent variables\n    X = df[['A', 'B', 'C', 'D']]\n    y = df['Response']\n    \n    # Add a constant to the model\n    X = sm.add_constant(X)\n    \n    # Fit a linear regression using statsmodels\n    np.random.seed(random_seed)\n    model = sm.OLS(y, X)\n    results = model.fit()\n    \n    return df, results"}
{"task_id": "BigCodeBench/517", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    # Convert the 2D list into a pandas DataFrame\n    df = pd.DataFrame(array)\n    \n    # Apply PCA for dimensionality reduction\n    pca = PCA(n_components=2, random_state=random_seed)\n    pca.fit(df)\n    transformed_data = pca.transform(df)\n    \n    return df, transformed_data"}
{"task_id": "BigCodeBench/518", "solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\ndef task_func(array):\n    # Convert 2D list to DataFrame\n    df = pd.DataFrame(array)\n    # Dynamically assign alphabetical labels to columns\n    df.columns = [chr(65 + i) for i in range(df.shape[1])]\n    # Calculate Euclidean distance matrix\n    distance_matrix = pd.DataFrame(squareform(pdist(df, 'euclidean')))\n    # Return the DataFrame and distance matrix\n    return df, distance_matrix\narray = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/519", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Combine a list of dictionaries with the same keys (fruit names) into a single pandas dataframe\n    df = pd.DataFrame(data)\n    \n    # Fill NA/NaN values with 0\n    df.fillna(0, inplace=True)\n    \n    # Generate a line chart of sales\n    ax = df.plot(kind='line', x='Time', y='Sales Quantity', title='Fruit Sales over Time')\n    \n    # Set x-axis and y-axis labels\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Sales Quantity')\n    \n    return ax"}
{"task_id": "BigCodeBench/520", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return {}, None\n\n    # Combine the list of dictionaries with the same keys (fruit names)\n    combined = collections.defaultdict(int)\n    for item in data:\n        for fruit, quantity in item.items():\n            combined[fruit] += quantity\n\n    # Calculate the total turnover for each fruit\n    total_sales = {fruit: quantity for fruit, quantity in combined.items() if quantity >= 0}\n    if any(quantity < 0 for quantity in combined.values()):\n        raise ValueError(\"Sales quantity cannot be negative\")\n\n    # Create a bar chart's axes with colors representing different fruits\n    fruits = list(total_sales.keys())\n    quantities = list(total_sales.values())\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    color_mapping = {fruit: color for fruit, color in zip(fruits, colors)}\n\n    fig, ax = plt.subplots()\n    bars = ax.bar(fruits, quantities, color=[color_mapping[fruit] for fruit in fruits])\n\n    # Add some text for labels, title, and custom x-axis tick labels, etc.\n    ax.set_ylabel('Sales Quantity')\n    ax.set_title('Total Fruit Sales')\n    ax.set_xticks(fruits)\n    ax.set_xticklabels(fruits)\n\n    return total_sales, ax\ndata = [\n    {'apple': 10, 'banana': 20, 'cherry': 30},\n    {'apple': 15, 'banana': 25, 'cherry': 35},\n    {'apple': 5, 'banana': 15, 'cherry': 25}\n]"}
{"task_id": "BigCodeBench/521", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_list):\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data_list)\n    \n    # Set the student names as the index\n    df.set_index('name', inplace=True)\n    \n    # Plot the data\n    ax = df.plot(kind='line')\n    \n    # Set the labels and title\n    ax.set_xlabel('Test Number')\n    ax.set_ylabel('Score')\n    ax.set_title('Student Scores Over Tests')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/522", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n\n    # Combine the list of dictionaries into a single dictionary\n    combined = collections.defaultdict(list)\n    for student in data:\n        for key, value in student.items():\n            combined[key].append(value)\n\n    # Calculate the average score for each student\n    averages = {}\n    for key, values in combined.items():\n        if None in values:\n            values = [x for x in values if x is not None]\n        if any(x < 0 for x in values):\n            raise ValueError(\"Negative scores are not allowed\")\n        if values:\n            averages[key] = sum(values) / len(values)\n        else:\n            averages[key] = None\n\n    # Create a bar chart of average student scores\n    students = list(averages.keys())\n    scores = list(averages.values())\n\n    # Assign colors to the bars\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    color_cycle = cycle(colors)\n    bar_colors = [next(color_cycle) for _ in students]\n\n    # Create the bar chart\n    ax = plt.axes()\n    ax.bar(students, scores, color=bar_colors)\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Student Scores')\n\n    return ax"}
{"task_id": "BigCodeBench/523", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n    df = pd.DataFrame(data)\n    ax = df.plot(x='Time', y='Data Points', title='Data over Time')\n    return ax"}
{"task_id": "BigCodeBench/524", "solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty\")\n    if not all(isinstance(d, dict) for d in data):\n        raise TypeError(\"Input is not a list of dictionaries\")\n    if not all(all(isinstance(v, (int, float)) for v in d.values()) for d in data):\n        raise TypeError(\"Values in dictionaries are not numeric\")\n    \n    keys = set().union(*data)\n    key_data = defaultdict(list)\n    for d in data:\n        for k in keys:\n            key_data[k].append(d.get(k, None))\n    \n    results = {}\n    for k, v in key_data.items():\n        if None in v:\n            continue\n        mean = np.mean(v)\n        std = np.std(v)\n        results[k] = (mean, std)\n    \n    fig, axes = plt.subplots(len(results), 1, figsize=(10, 5*len(results)))\n    for i, (k, (mean, std)) in enumerate(results.items()):\n        axes[i].bar(['Mean', 'Std'], [mean, std])\n        axes[i].set_title(k)\n    \n    return (results, axes)"}
{"task_id": "BigCodeBench/525", "solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(input_file):\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize dictionaries to hold mean and median values\n    means = defaultdict(list)\n    medians = defaultdict(list)\n    \n    # Iterate through each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            means[key].append(value)\n            medians[key].append(value)\n    \n    # Calculate mean and median for each key\n    result = {}\n    plots = []\n    \n    for key in means:\n        mean_val = np.mean(means[key])\n        median_val = np.median(medians[key])\n        \n        result[key] = {'mean': mean_val, 'median': median_val}\n        \n        # Create bar chart for mean and median\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Median'], [mean_val, median_val])\n        ax.set_title(f'Statistics for {key}')\n        plots.append(ax)\n    \n    return result, plots"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    results = defaultdict(list)\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                results[key].append(value)\n    \n    means = {}\n    medians = {}\n    for key, values in results.items():\n        if values:\n            means[key] = np.mean(values)\n            medians[key] = np.median(values)\n        else:\n            means[key] = np.nan\n            medians[key] = np.nan\n    \n    df = pd.DataFrame({\n        'mean': means,\n        'median': medians\n    }).sort_index()\n    \n    return df"}
{"task_id": "BigCodeBench/527", "solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file: str) -> plt.Axes:\n    # Read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize a dictionary to hold the results\n    results = {}\n    \n    # Initialize a list to hold the data for the box plot\n    box_plot_data = []\n    \n    # Iterate over each dictionary in the list\n    for item in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in item.items():\n            # If the key is not in the results dictionary, add it\n            if key not in results:\n                results[key] = {'values': []}\n            # Append the value to the list of values for this key\n            results[key]['values'].append(value)\n            # Append the value to the box plot data\n            box_plot_data.append({'X': key, 'Y': value})\n    \n    # Calculate mean and median for each key\n    for key, value in results.items():\n        results[key]['mean'] = np.mean(value['values'])\n        results[key]['median'] = np.median(value['values'])\n    \n    # Convert the box plot data to a pandas DataFrame\n    df = pd.DataFrame(box_plot_data)\n    \n    # Create a box plot using seaborn\n    ax = sns.boxplot(x='X', y='Y', data=df)\n    \n    # Return the results and the box plot\n    return ax"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be a CSV file with .csv extension.\")\n    \n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        headers = next(reader)\n        data = [row for row in reader]\n    \n    df = pd.DataFrame(data, columns=headers)\n    duplicates = df.duplicated(keep=False)\n    duplicate_rows = df[duplicates]\n    \n    duplicate_counts = Counter(tuple(row) for row in duplicate_rows.values)\n    duplicate_dict = {k: v for k, v in duplicate_counts.items() if v > 1}\n    \n    fig, ax = plt.subplots()\n    ax.bar(duplicate_dict.keys(), duplicate_dict.values())\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Duplicate Rows in CSV File')\n    \n    return duplicate_dict, ax"}
{"task_id": "BigCodeBench/529", "solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    random.seed(random_seed)\n    sums = []\n    for _ in range(num_rolls):\n        roll = [random.randint(1, 6) for _ in range(num_dice)]\n        sums.append(sum(roll))\n    counter = Counter(sums)\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Sum of Dice Roll')\n    ax.set_ylabel('Count')\n    if plot_path:\n        plt.savefig(plot_path)\n    else:\n        plt.show()\n    return counter, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Ensure 'name' and 'age' columns are present\n    if 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns\")\n    \n    # Round down ages to nearest integer\n    df['age'] = df['age'].apply(lambda x: int(x) if isinstance(x, float) else x)\n    \n    # Check for negative ages\n    if df['age'].min() < 0:\n        raise ValueError(\"Age cannot be negative\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    \n    if duplicates.empty:\n        return Counter(), None\n    \n    # Record age distribution for duplicates\n    age_distribution = duplicates['age'].value_counts().sort_index()\n    \n    # Create histogram plot\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5)\n    plt.hist(duplicates['age'], bins=bins, align='left', rwidth=0.8)\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution of Duplicates')\n    plt.xticks(np.arange(min_age, max_age + 1))\n    plt.grid(axis='y', alpha=0.75)\n    \n    return age_distribution, plt.gca()"}
{"task_id": "BigCodeBench/531", "solution": "import pandas as pd\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    # Identify duplicate points\n    duplicates = df.duplicated(keep=False)\n    duplicate_points = df[duplicates]\n    duplicate_counts = Counter(duplicate_points.values.flatten())\n    \n    # Remove duplicates to perform KMeans on unique points\n    unique_df = df.drop_duplicates()\n    \n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    unique_df['cluster'] = kmeans.fit_predict(unique_df)\n    \n    # Create a scatter plot\n    plt.scatter(unique_df.iloc[:, 0], unique_df.iloc[:, 1], c=unique_df['cluster'], cmap='viridis')\n    plt.title('KMeans Clustering of Unique Points')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.colorbar(label='Cluster')\n    plt.show()\n    \n    return (duplicate_counts, unique_df, plt.gca())"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Count duplicates\n    value_counts = df['value'].value_counts()\n    duplicates = value_counts[value_counts > 1]\n    counter = Counter(duplicates.index)\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6, label='Data')\n    mu, std = norm.fit(df['value'])\n    y = norm.pdf(bins, mu, std)\n    ax.plot(bins, y, 'k-', linewidth=2, label='Normal Distribution')\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    return (counter, ax)"}
{"task_id": "BigCodeBench/533", "solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Bases must be at least 2.\")\n    \n    # Convert the number from from_base to decimal\n    try:\n        decimal_num = int(num, from_base)\n    except ValueError:\n        raise ValueError(\"Invalid number format for the specified base.\")\n    \n    # Convert the decimal number to to_base\n    if to_base == 10:\n        converted_num = str(decimal_num)\n    else:\n        converted_num = ''\n        while decimal_num > 0:\n            converted_num = alphabet[decimal_num % to_base] + converted_num\n            decimal_num = decimal_num // to_base\n        if converted_num == '':\n            converted_num = alphabet[0]\n    \n    # Generate a random salt\n    salt = secrets.token_bytes(16)\n    \n    # Hash the converted number with the salt using SHA-256\n    hash_object = hashlib.sha256()\n    hash_object.update(salt + converted_num.encode())\n    hashed_num = hash_object.digest()\n    \n    # Encode the hash in base64 using the custom alphabet\n    base64_encoded = base64.b64encode(hashed_num).decode()\n    base64_encoded = ''.join(alphabet[base64.b64encode(hashed_num).index(c)] for c in base64_encoded)\n    \n    return base64_encoded, salt\nalphabet = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'"}
{"task_id": "BigCodeBench/534", "solution": "import numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.backends import default_backend\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert the number from the source base to decimal\n    num_dec = int(num, from_base)\n    \n    # Convert the decimal number to the target base\n    num_to_base = ''\n    while num_dec > 0:\n        num_to_base = alphabet[num_dec % to_base] + num_to_base\n        num_dec = num_dec // to_base\n    if num_to_base == '':\n        num_to_base = alphabet[0]\n    \n    # Sign the converted number with the private RSA key\n    signer = padding.PSS(\n        mgf=padding.MGF1(hashes.SHA256()),\n        salt_length=padding.PSS.MAX_LENGTH\n    )\n    signature = private_key.sign(\n        num_to_base.encode(),\n        signer\n    )\n    \n    # Encode the signed number in base64 using the custom alphabet\n    encoded = base64.b64encode(signature).decode()\n    encoded = ''.join(alphabet[alphabet.index(c)] for c in encoded)\n    \n    return encoded"}
{"task_id": "BigCodeBench/535", "solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\nNAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\nAGES = range(18, 65)\nHEIGHTS = range(150, 200)\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n    \n    if random_seed is not None:\n        seed(random_seed)\n    \n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    \n    try:\n        c.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (name TEXT, age INTEGER, height INTEGER)\")\n    except sqlite3.OperationalError as e:\n        print(f\"An error occurred: {e}\")\n        return 0\n    \n    names = [choice(NAMES) for _ in range(num_entries)]\n    ages = np.random.choice(list(AGES), size=num_entries)\n    heights = np.random.choice(list(HEIGHTS), size=num_entries)\n    \n    for name, age, height in zip(names, ages, heights):\n        c.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n    \n    conn.commit()\n    conn.close()\n    \n    return num_entries"}
{"task_id": "BigCodeBench/536", "solution": "import sqlite3\nimport pandas as pd\nimport os\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Read the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the connection\n    conn.close()\n    \n    # Ensure the directory exists for the CSV file\n    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n    \n    # Export the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n    \n    # Return the absolute path of the CSV file\n    return os.path.abspath(csv_path)"}
{"task_id": "BigCodeBench/537", "solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Load data from the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Check for negative age values\n    if df['age'].min() < 0:\n        raise ValueError(\"Data contains negative age values.\")\n    \n    # Create a histogram of age distribution\n    ax = sns.histplot(df['age'], bins=30, kde=True)\n    \n    # Set x-axis label\n    ax.set_xlabel('Age')\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/538", "solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(db_name, table_name):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n    \n    # Read the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Close the connection\n    conn.close()\n    \n    # Check if the DataFrame has at least two numerical columns\n    numerical_cols = df.select_dtypes(include=['number']).columns\n    if len(numerical_cols) < 2:\n        raise ValueError(\"The table must have at least two numerical columns.\")\n    \n    # Select the first two numerical columns\n    col1 = numerical_cols[0]\n    col2 = numerical_cols[1]\n    \n    # Create a scatter plot\n    plt.scatter(df[col1], df[col2])\n    plt.xlabel(col1)\n    plt.ylabel(col2)\n    plt.title(f\"Scatter plot of {col1} vs {col2}\")\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the axes object\n    return plt.gca()"}
{"task_id": "BigCodeBench/539", "solution": "import sqlite3\nfrom random import choice, seed\nimport os\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n    \n    # Define constants\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 66)\n    HEIGHTS = range(150, 201)\n    \n    # Create the database file\n    db_path = os.path.abspath(db_name)\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    \n    # Create table\n    c.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (name TEXT, age INTEGER, height INTEGER)\")\n    \n    # Seed the random generator\n    if random_seed is not None:\n        seed(random_seed)\n    \n    # Populate the table with random data\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        c.execute(f\"INSERT INTO {table_name} (name, age, height) VALUES (?, ?, ?)\", (name, age, height))\n    \n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n    \n    return db_path"}
{"task_id": "BigCodeBench/540", "solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    # Flatten the nested list using itertools.chain\n    flat_list = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count the occurrences of each item\n    counts = Counter(flat_list)\n    \n    # Extract items and their frequencies\n    items = list(counts.keys())\n    frequencies = list(counts.values())\n    \n    # Sort items alphabetically\n    sorted_indices = np.argsort(items)\n    sorted_items = [items[i] for i in sorted_indices]\n    sorted_frequencies = [frequencies[i] for i in sorted_indices]\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram\n    ax.bar(sorted_items, sorted_frequencies, color=color, width=width)\n    \n    # Set the title and labels\n    ax.set_title(title)\n    ax.set_xlabel(\"Menu Items\")\n    ax.set_ylabel(\"Frequency\")\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/541", "solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\ndef task_func(package_name):\n    try:\n        package = importlib.import_module(package_name)\n    except ImportError as e:\n        raise ImportError(f\"Package '{package_name}' is not installed. Please install it using 'pip install {package_name}'\") from e\n\n    package_path = os.path.dirname(package.__file__)\n    sys.path.append(package_path)\n\n    added_modules = []\n    for module_info in iter_modules([package_path]):\n        module_name = module_info.name\n        if module_name not in sys.modules:\n            importlib.import_module(f\"{package_name}.{module_name}\")\n            added_modules.append(module_name)\n\n    return added_modules"}
{"task_id": "BigCodeBench/542", "solution": "import hashlib\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    selected_key = random.choice(hex_keys)\n    try:\n        # Convert hex string to bytes\n        key_bytes = bytes.fromhex(selected_key)\n        # Convert bytes to float\n        float_value = struct.unpack('!f', key_bytes)[0]\n        # Compute MD5 hash\n        md5_hash = hashlib.md5(struct.pack('!f', float_value)).hexdigest()\n        return md5_hash\n    except ValueError:\n        raise ValueError(\"Invalid hexadecimal string\")"}
{"task_id": "BigCodeBench/543", "solution": "import base64\nimport os\ndef task_func():\n    # Generate a random float number\n    random_float = os.urandom(4)\n    # Convert it to a hexadecimal string\n    hex_string = random_float.hex()\n    # Encode this hexadecimal representation in base64\n    base64_encoded = base64.b64encode(hex_string.encode()).decode()\n    # Return the base64 encoded string\n    return base64_encoded"}
{"task_id": "BigCodeBench/544", "solution": "import struct\nimport zlib\nKEY = '470FC614'\ndef task_func(hex_string=KEY):\n    # Convert hex string to float\n    float_number = struct.unpack('!f', bytes.fromhex(hex_string))[0]\n    # Compress the float number\n    compressed_data = zlib.compress(struct.pack('!f', float_number))\n    # Return the compressed data\n    return compressed_data"}
{"task_id": "BigCodeBench/545", "solution": "import codecs\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS):\n    # Convert hex strings to bytes\n    byte_keys = [bytes.fromhex(key) for key in hex_keys]\n    \n    # Select a random byte string from the list\n    random_byte = random.choice(byte_keys)\n    \n    # Convert the byte string to a float\n    float_number = struct.unpack('!f', random_byte)[0]\n    \n    # Encode the float number to utf-8\n    utf8_encoded = str(float_number).encode('utf-8')\n    \n    return utf8_encoded"}
{"task_id": "BigCodeBench/546", "solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\ndef task_func(my_dict):\n    # Sort the dictionary by keys in ascending order\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n    \n    # Create a PrettyTable object\n    pt = PrettyTable()\n    \n    # Add columns for 'Key' and 'Value'\n    pt.field_names = ['Key', 'Value']\n    \n    # Add rows to the table\n    for key, value in sorted_dict.items():\n        pt.add_row([key, value])\n    \n    # Return the PrettyTable object\n    return pt"}
{"task_id": "BigCodeBench/547", "solution": "import hashlib\nimport os\nimport base64\ndef task_func(password: str, salt_length: int = 8) -> str:\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n    \n    # Create a new SHA-256 hash object\n    sha256 = hashlib.sha256()\n    \n    # Update the hash object with the salt\n    sha256.update(salt)\n    \n    # Update the hash object with the password\n    sha256.update(password.encode('utf-8'))\n    \n    # Get the hexadecimal digest of the hash\n    hashed_password = sha256.hexdigest()\n    \n    # Encode the salt and hashed password in base64\n    salt_base64 = base64.b64encode(salt).decode('utf-8')\n    hashed_password_base64 = base64.b64encode(hashed_password.encode('utf-8')).decode('utf-8')\n    \n    # Combine the salt and hashed password\n    combined = salt_base64 + hashed_password_base64\n    \n    # Encode the combined string in base64\n    final_encoded = base64.b64encode(combined.encode('utf-8')).decode('utf-8')\n    \n    return final_encoded"}
{"task_id": "BigCodeBench/548", "solution": "import random\nimport string\nimport base64\nimport zlib\ndef task_func(string_length=100):\n    # Generate a random string of the specified length with uppercase letters and digits\n    random_string = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(string_length))\n    \n    # Compress the random string using zlib\n    compressed_data = zlib.compress(random_string.encode())\n    \n    # Encode the compressed data in base64\n    base64_encoded = base64.b64encode(compressed_data)\n    \n    # Return the base64 encoded string\n    return base64_encoded.decode()"}
{"task_id": "BigCodeBench/549", "solution": "import base64\nimport pandas as pd\ndef task_func(df):\n    # Convert the DataFrame to a CSV string\n    csv_string = df.to_csv(index=False)\n    \n    # Encode the CSV string to bytes\n    csv_bytes = csv_string.encode('utf-8')\n    \n    # Encode the bytes to a Base64 string\n    base64_bytes = base64.b64encode(csv_bytes)\n    base64_string = base64_bytes.decode('utf-8')\n    \n    return base64_string"}
{"task_id": "BigCodeBench/550", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(list_of_menuitems):\n    # Flatten the nested list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n    \n    # Count the occurrences of each menu item\n    count = Counter(flat_list)\n    \n    # Create a DataFrame from the counts\n    df = pd.DataFrame.from_dict(count, orient='index', columns=['Count'])\n    \n    # Set the index name\n    df.index.name = 'MenuItem'\n    \n    return df"}
{"task_id": "BigCodeBench/551", "solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\ndef task_func(list_of_menuitems):\n    if not list_of_menuitems:\n        return None\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n    counter = Counter(flat_list)\n    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n    df.columns = ['menu_item', 'frequency']\n    plt.figure(figsize=(10,6))\n    sns.barplot(x='menu_item', y='frequency', data=df)\n    plt.title('Frequency of Menu Items')\n    plt.xlabel('Menu Item')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=45)\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/552", "solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\nITEMS = ['apple', 'banana']\ndef task_func(a, b, items=ITEMS):\n    # Combine the two lists\n    combined = a + b\n\n    # Count the frequency of predefined items in the combined list\n    frequency = collections.Counter(item for item in combined if item in items)\n\n    # Create a bar chart\n    plt.bar(frequency.keys(), frequency.values())\n    plt.xlabel('Items')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Predefined Items')\n    plt.show()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Generate random values for the DataFrame\n    data = np.random.rand(len(a), len(b))\n    \n    # Create the DataFrame with the given row indices and column names\n    df = pd.DataFrame(data, index=a, columns=b)\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object of the plotted bar chart\n    return ax"}
{"task_id": "BigCodeBench/554", "solution": "import numpy as np\nimport random\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    # Generate a random number of words between MIN_WORDS and MAX_WORDS\n    num_words = random.randint(MIN_WORDS, MAX_WORDS)\n    \n    # Randomly select words from the pool\n    words = random.sample(WORDS_POOL, num_words)\n    \n    # Create a palindrome by concatenating the words and their reverse\n    sentence = ' '.join(words) + ' ' + ' '.join(words[::-1])\n    \n    # Ensure the sentence is a palindrome\n    if sentence == sentence[::-1]:\n        return sentence\n    else:\n        return \"Failed to generate a palindrome\"\nMIN_WORDS = 3\nMAX_WORDS = 5\nWORDS_POOL = ['apple', 'banana', 'cherry', 'date', 'elderberry']"}
{"task_id": "BigCodeBench/555", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(a, b):\n    # Calculate Pearson correlation coefficient\n    corr_coef, _ = stats.pearsonr(a, b)\n    \n    # Create a DataFrame from the lists\n    df = pd.DataFrame({'a': a, 'b': b})\n    \n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'])\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['a'], df['b'])\n    ax.plot(df['a'], intercept + slope*df['a'], color='red')\n    \n    return corr_coef, ax\na = [1, 2, 3, 4, 5]\nb = [2, 4, 5, 4, 5]"}
{"task_id": "BigCodeBench/556", "solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\ndef task_func(s, min_length, max_length, letters):\n    # Generate a random string of length between min_length and max_length, inclusive\n    length = random.randint(min_length, max_length)\n    random_string = ''.join(random.choice(letters) for _ in range(length))\n    \n    # Evaluate its similarity to the provided string s\n    similarity = SequenceMatcher(None, s, random_string).ratio()\n    \n    # Determine if it's similar based on the similarity threshold\n    is_similar = similarity >= 0.5\n    \n    # Return a tuple containing the generated string and a boolean indicating similarity\n    return (random_string, is_similar)"}
{"task_id": "BigCodeBench/557", "solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list) or not all(isinstance(s, str) for s in s_list):\n        raise ValueError(\"s_list must be a list of strings\")\n    if len(s_list) < 2:\n        return [np.nan]\n    \n    scores = []\n    for i in range(len(s_list)):\n        total_similarity = 0\n        for j in range(len(s_list)):\n            if i != j:\n                matcher = SequenceMatcher(None, s_list[i], s_list[j])\n                total_similarity += matcher.ratio()\n        avg_similarity = total_similarity / (len(s_list) - 1)\n        scores.append(avg_similarity)\n    \n    if plot_path:\n        plt.plot(scores)\n        plt.xlabel('String Index')\n        plt.ylabel('Average Similarity Score')\n        plt.title('Average Similarity Scores of Strings')\n        plt.savefig(plot_path)\n        plt.close()\n    \n    return scores"}
{"task_id": "BigCodeBench/558", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(a, b, columns=['A', 'B']):\n    # Create a DataFrame from the input lists\n    df = pd.DataFrame({'A': a, 'B': b})\n    \n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n    \n    # Fit and transform the DataFrame\n    scaled_df = scaler.fit_transform(df)\n    \n    # Create a DataFrame from the scaled data\n    scaled_df = pd.DataFrame(scaled_df, columns=columns)\n    \n    # Create a bar plot of the scaled values\n    ax = scaled_df.plot(kind='bar')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the DataFrame and the Axes object\n    return scaled_df, ax"}
{"task_id": "BigCodeBench/559", "solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Calculate the Euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n    \n    # Create a DataFrame from the lists with indices 'A' and 'B'\n    df = pd.DataFrame({'A': a, 'B': b}, index=['Value'])\n    \n    # Draw the values with a line displaying the Euclidean distance\n    fig, ax = plt.subplots()\n    ax.plot(a, label='List A')\n    ax.plot(b, label='List B')\n    ax.legend()\n    ax.set_title(f'Euclidean Distance: {euclidean_distance:.2f}')\n    \n    return euclidean_distance, df, ax\na = [1, 2, 3]\nb = [4, 5, 6]"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Convert the 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Extract year and month from the 'date' column\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    \n    # Filter the data for the specified year\n    year = df['year'].unique()[0]\n    monthly_data = df[df['year'] == year]\n    \n    # Plot the bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(monthly_data['month'], monthly_data['value'], color='skyblue')\n    plt.title(f'Monthly Data for {year}')\n    plt.xlabel('Month')\n    plt.ylabel('Value')\n    plt.xticks(monthly_data['month'], monthly_data['month'].map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}))\n    plt.grid(axis='y')\n    \n    # Return the axes object\n    return plt.gca()\ndata = [\n    {'date': '2021-01-01', 'value': 10},\n    {'date': '2021-02-01', 'value': 20},\n    {'date': '2021-03-01', 'value': 30},\n    {'date': '2021-04-01', 'value': 40},\n    {'date': '2021-05-01', 'value': 50},\n    {'date': '2021-06-01', 'value': 60},\n    {'date': '2021-07-01', 'value': 70},\n    {'date': '2021-08-01', 'value': 80},\n    {'date': '2021-09-01', 'value': 90},\n    {'date': '2021-10-01', 'value': 100},\n    {'date': '2021-11-01', 'value': 110},\n    {'date': '2021-12-01', 'value': 120}\n]"}
{"task_id": "BigCodeBench/561", "solution": "import pytz\nfrom dateutil import parser\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the input date string\n    dt = parser.parse(date_str)\n    \n    # Localize the datetime to the original timezone\n    from_tz_obj = pytz.timezone(from_tz)\n    dt = from_tz_obj.localize(dt)\n    \n    # Convert to the target timezone\n    to_tz_obj = pytz.timezone(to_tz)\n    dt = dt.astimezone(to_tz_obj)\n    \n    # Format the datetime as \"yyyy-mm-dd hh:mm:ss\"\n    formatted_dt = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    return formatted_dt"}
{"task_id": "BigCodeBench/562", "solution": "import os\nimport ctypes\nimport sys\nimport subprocess\ndef task_func(filepath):\n    if not isinstance(filepath, str):\n        raise TypeError(\"Input filepath must be a string\")\n    if not filepath:\n        raise OSError(\"Input filepath cannot be empty\")\n    try:\n        # Load the DLL file\n        dll = ctypes.CDLL(filepath)\n        # Retrieve system information\n        system_name = os.name\n        node_name = os.uname().nodename\n        release = os.uname().release\n        version = os.uname().version\n        machine = os.uname().machine\n        python_version = sys.version\n        pip_version = subprocess.check_output([sys.executable, '-m', 'pip', 'show', 'pip']).decode('utf-8').split('\\n')[1].split(': ')[1]\n        # Print the system information\n        print(f\"System: {system_name} Node Name: {node_name} Release: {release} Version: {version} Machine: {machine} Python Version: {python_version} PIP Version: {pip_version}\")\n        # Return the name of the loaded DLL file\n        return os.path.basename(filepath)\n    except OSError as e:\n        raise OSError(\"Failed to load DLL file\") from e"}
{"task_id": "BigCodeBench/563", "solution": "import ctypes\nimport os\nimport shutil\nimport glob\ndef task_func(filepath, destination_dir):\n    # Load the DLL file specified by the given filepath\n    loaded_dll = ctypes.CDLL(filepath)\n    print(f\"Loaded DLL: {filepath}\")\n\n    # Get the directory of the specified filepath\n    dir_path = os.path.dirname(filepath)\n\n    # Use glob to find all DLL files in the same directory\n    dll_files = glob.glob(os.path.join(dir_path, '*.dll'))\n\n    # Move each DLL file to the specified destination directory\n    for dll in dll_files:\n        shutil.move(dll, destination_dir)\n        print(f\"Moved {dll} to {destination_dir}\")\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)"}
{"task_id": "BigCodeBench/564", "solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\ndef task_func(filepath):\n    # Load the DLL file\n    try:\n        dll = ctypes.CDLL(filepath)\n    except OSError as e:\n        return f\"Error loading DLL: {e}\"\n    \n    # Get file metadata\n    try:\n        # Get file creation time\n        creation_time = os.path.getctime(filepath)\n        # Get file modification time\n        modification_time = os.path.getmtime(filepath)\n        # Get file size\n        size = os.path.getsize(filepath)\n    except OSError as e:\n        return f\"Error getting file metadata: {e}\"\n    \n    # Convert times to UTC\n    utc_tz = pytz.UTC\n    creation_time_utc = datetime.fromtimestamp(creation_time, utc_tz)\n    modification_time_utc = datetime.fromtimestamp(modification_time, utc_tz)\n    \n    # Prepare the metadata dictionary\n    metadata = {\n        'Creation Time': creation_time_utc.isoformat(),\n        'Modification Time': modification_time_utc.isoformat(),\n        'Size': size\n    }\n    \n    # Return the DLL name and metadata\n    return dll._name, metadata"}
{"task_id": "BigCodeBench/565", "solution": "import ctypes\nimport hashlib\nimport binascii\ndef task_func(filepath):\n    # Load the DLL file\n    try:\n        dll = ctypes.CDLL(filepath)\n    except OSError as e:\n        print(f\"Error loading DLL: {e}\")\n        return\n\n    # Calculate MD5 hash\n    with open(filepath, 'rb') as f:\n        data = f.read()\n        md5_hash = hashlib.md5(data).hexdigest()\n\n    # Calculate SHA256 hash\n    with open(filepath, 'rb') as f:\n        data = f.read()\n        sha256_hash = hashlib.sha256(data).hexdigest()\n\n    # Print the hashes\n    print(f\"MD5: {md5_hash}\")\n    print(f\"SHA256: {sha256_hash}\")\n\n    # Return the actual name of the loaded DLL file\n    return dll._name"}
{"task_id": "BigCodeBench/566", "solution": "import inspect\nimport types\ndef task_func(f):\n    # Create a dictionary to store the function's specifications\n    func_spec = {}\n    \n    # Get the function's name\n    func_spec['name'] = f.__name__\n    \n    # Check if the function is a lambda function\n    func_spec['is_lambda'] = isinstance(f, types.LambdaType)\n    \n    # Get the function's arguments\n    argspec = inspect.getfullargspec(f)\n    func_spec['args'] = argspec.args\n    \n    # Get the function's default values\n    func_spec['defaults'] = argspec.defaults\n    \n    # Get the function's annotations\n    func_spec['annotations'] = f.__annotations__\n    \n    return func_spec"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the input string to a list of floats\n    data_list = list(map(float, data.split()))\n    \n    # Create a pandas DataFrame from the list\n    df = pd.DataFrame(data_list, columns=['Value'])\n    \n    # Calculate the frequency distribution\n    frequency = df['Value'].value_counts().sort_index()\n    \n    # Create a histogram using matplotlib\n    ax = plt.gca()\n    plt.hist(data_list, bins=np.arange(data_list.min(), data_list.max()+2) - 0.5, edgecolor='black')\n    plt.title('Histogram of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    \n    return ax"}
{"task_id": "BigCodeBench/568", "solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(f_list):\n    if any(isinstance(f, type(lambda: 0)) for f in f_list):\n        raise ValueError(\"Input contains a lambda function\")\n    \n    func_data = []\n    for func in f_list:\n        if inspect.isfunction(func):\n            arg_count = len(inspect.signature(func).parameters)\n            func_data.append((func.__name__, arg_count))\n    \n    df = pd.DataFrame(func_data, columns=['Function Name', 'Number of Arguments'])\n    return df"}
{"task_id": "BigCodeBench/569", "solution": "import inspect\nimport types\nimport math\ndef task_func(f):\n    # Get the function's name\n    func_name = f.__name__\n    \n    # Get the number of arguments\n    argspec = inspect.getfullargspec(f)\n    num_args = len(argspec.args) + len(argspec.kwonlyargs)\n    \n    # Calculate the square root of the number of arguments\n    sqrt_num_args = math.sqrt(num_args)\n    \n    # Count the number of lambda functions in default values\n    count_lambdas = 0\n    if argspec.defaults:\n        for default in argspec.defaults:\n            if isinstance(default, types.LambdaType):\n                count_lambdas += 1\n    if argspec.kwonlydefaults:\n        for default in argspec.kwonlydefaults.values():\n            if isinstance(default, types.LambdaType):\n                count_lambdas += 1\n    \n    # Return the results in a dictionary\n    return {\n        'name': func_name,\n        'sqrt_num_args': sqrt_num_args,\n        'count_lambdas': count_lambdas\n    }"}
{"task_id": "BigCodeBench/570", "solution": "import inspect\nimport types\nimport json\ndef task_func(f):\n    # Get the function's name\n    name = f.__name__\n    \n    # Get the function's arguments\n    argspec = inspect.getfullargspec(f)\n    args = argspec.args\n    defaults = argspec.defaults\n    annotations = argspec.annotations\n    \n    # Convert annotations to a string format\n    annotations_str = {k: str(v) for k, v in annotations.items()}\n    \n    # Check if the function is a lambda function\n    is_lambda = isinstance(f, types.LambdaType)\n    \n    # Create a dictionary with the function's specifications\n    func_spec = {\n        \"name\": name,\n        \"args\": args,\n        \"defaults\": defaults,\n        \"annotations\": annotations_str,\n        \"is_lambda\": is_lambda\n    }\n    \n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(func_spec, indent=4)\n    \n    return json_str"}
{"task_id": "BigCodeBench/571", "solution": "import inspect\nimport pandas as pd\nimport os\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(inspect.isfunction(func) for func in f_list):\n        raise ValueError(\"f_list must be a list of functions\")\n    if not f_list:\n        raise ValueError(\"f_list cannot be empty\")\n    if not os.path.isdir(os.path.dirname(file_path)):\n        raise IOError(\"Invalid file path\")\n    \n    data = []\n    for func in f_list:\n        func_name = func.__name__\n        num_args = len(inspect.signature(func).parameters)\n        defaults = inspect.signature(func).parameters\n        annotations = inspect.getfullargspec(func).annotations\n        is_lambda = inspect.islambda(func)\n        \n        data.append({\n            'Function Name': func_name,\n            'Number of Arguments': num_args,\n            'Defaults': defaults,\n            'Annotations': annotations,\n            'Is Lambda': is_lambda\n        })\n    \n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)"}
{"task_id": "BigCodeBench/572", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100):\n    # Generate two arrays of random integers\n    array1 = [randint(0, 100) for _ in range(array_length)]\n    array2 = [randint(0, 100) for _ in range(array_length)]\n    \n    # Calculate the maximum values of the respective elements of the two arrays\n    max_values = [max(a, b) for a, b in zip(array1, array2)]\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Plot the maximum values\n    ax.plot(max_values)\n    \n    # Set the y-axis label to 'Maximum Values'\n    ax.set_ylabel('Maximum Values')\n    \n    # Return the Axes object with the plot\n    return ax"}
{"task_id": "BigCodeBench/573", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array_length=100):\n    # Generate two arrays of random numbers\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n    \n    # Calculate statistics for each array\n    stats1 = {\n        'Mean': np.mean(array1),\n        'Median': np.median(array1),\n        'Standard Deviation': np.std(array1)\n    }\n    stats2 = {\n        'Mean': np.mean(array2),\n        'Median': np.median(array2),\n        'Standard Deviation': np.std(array2)\n    }\n    \n    # Create a DataFrame to store the statistics\n    statistics = pd.DataFrame({\n        'Array1': [stats1['Mean'], stats1['Median'], stats1['Standard Deviation']],\n        'Array2': [stats2['Mean'], stats2['Median'], stats2['Standard Deviation']]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n    \n    # Draw a bar chart to compare the statistics\n    statistics.plot(kind='bar')\n    plt.title('Comparison of Statistics for Array1 and Array2')\n    plt.xlabel('Statistic')\n    plt.ylabel('Value')\n    plt.show()\n    \n    return statistics"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 10, array_length)\n    \n    # Generate y values based on a sine function\n    y = np.sin(x)\n    \n    # Add noise to the y values\n    noise = np.random.normal(0, noise_level, array_length)\n    y_noisy = y + noise\n    \n    # Define the function to fit\n    def func(x, a, b, c):\n        return a * np.sin(b * x + c)\n    \n    # Fit the function to the data\n    popt, pcov = curve_fit(func, x, y_noisy)\n    \n    # Generate y values for the fitted function\n    y_fit = func(x, *popt)\n    \n    # Plot the original noisy sine wave and the fitted curve\n    plt.figure()\n    plt.plot(x, y_noisy, 'b-', label='Noisy Sine Wave')\n    plt.plot(x, y_fit, 'r-', label='Fitted Curve')\n    plt.legend()\n    plt.show()\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/575", "solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\ndef task_func(l, n_groups=5):\n    # Shuffle the list\n    shuffle(l)\n    \n    # Construct a DataFrame from the shuffled list\n    df = pd.DataFrame(l, columns=['values'])\n    \n    # For each row in the DataFrame, move the first n_groups elements to the end\n    for index, row in df.iterrows():\n        if len(row['values']) > n_groups:\n            df.at[index, 'values'] = row['values'][n_groups:] + row['values'][:n_groups]\n    \n    return df"}
{"task_id": "BigCodeBench/576", "solution": "from random import shuffle, randint\nimport pandas as pd\ndef task_func(l, n_groups=5):\n    # Shuffle the list\n    shuffle(l)\n    \n    # Create a longer series by cycling through the shuffled list\n    series = pd.Series(l * 10)  # Assuming 10 is a sufficient multiple for cycling\n    \n    # For each element in the series, randomly select n_groups characters from the start and move them to the end\n    result = []\n    for element in series:\n        if isinstance(element, str):\n            n = randint(1, n_groups)\n            element = element[n:] + element[:n]\n        result.append(element)\n    \n    return pd.Series(result)\nl = ['apple', 'banana', 'cherry', 'date', 'elderberry']"}
{"task_id": "BigCodeBench/577", "solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\ndef task_func(directory):\n    result = {}\n    for entry in os.scandir(directory):\n        if entry.is_file():\n            normalized_name = unicodedata.normalize('NFKD', entry.name).encode('ASCII', 'ignore').decode('ASCII')\n            file_path = os.path.join(directory, entry.name)\n            file_size = os.path.getsize(file_path)\n            with open(file_path, 'rb') as f:\n                file_hash = md5(f.read()).hexdigest()\n            result[normalized_name] = {'Size': file_size, 'MD5 Hash': file_hash}\n    return result"}
{"task_id": "BigCodeBench/578", "solution": "import unicodedata\nimport requests\nURL = 'https://api.github.com/users/'\ndef task_func(username):\n    try:\n        response = requests.get(URL + username)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        raise e\n    else:\n        user_data = response.json()\n        normalized_data = {}\n        for key, value in user_data.items():\n            if isinstance(value, str):\n                normalized_data[key] = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n            else:\n                normalized_data[key] = value\n        return normalized_data"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text = ' '.join([row[0] for row in reader])\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The CSV file was not found.\")\n    except IOError:\n        raise IOError(\"An error occurred while reading the file.\")\n    \n    normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n    words = normalized_text.lower().split()\n    word_counts = Counter(words)\n    most_common = word_counts.most_common(10)\n    \n    words, counts = zip(*most_common)\n    plt.bar(words, counts)\n    plt.xlabel('Words')\n    plt.ylabel('Frequencies')\n    plt.title('10 Most Common Words')\n    plt.show()\n    \n    return (plt.gca(), most_common)"}
{"task_id": "BigCodeBench/580", "solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\nRANGE = 10000\nSIZE = 1000\nBIN_WIDTH = 100\ndef task_func():\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n    \n    # Calculate moving average over a window of 6 (current and previous 5)\n    moving_averages = []\n    for i in range(SIZE):\n        window = random_numbers[max(0, i-5):i+1]\n        moving_avg = statistics.mean(window)\n        moving_averages.append(moving_avg)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Random Numbers\": random_numbers,\n        \"Moving Average\": moving_averages\n    })\n    \n    # Plot histogram\n    plt.hist(random_numbers, bins=np.arange(0, RANGE+BIN_WIDTH, BIN_WIDTH), edgecolor='black')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return df"}
{"task_id": "BigCodeBench/581", "solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nRANGE = 10000\nSIZE = 1000\ndef task_func(size=SIZE, frequency=1):\n    # Generate random x values\n    x = np.random.rand(size) * RANGE\n    # Generate sinusoidal values\n    y = np.sin(x * frequency)\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    # Plot the sinusoidal wave\n    ax.plot(x, y)\n    # Set the title and labels\n    ax.set_title('Sinusoidal Wave')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    # Show the plot\n    plt.show()\n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n    \n    # Plot the histogram\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    \n    # Plot the probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, loc=0, scale=1)\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    \n    # Add labels and legend\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n    \n    # Generate a random password for AES encryption\n    password = get_random_bytes(16)\n    \n    # Generate a random nonce for AES encryption\n    nonce = get_random_bytes(16)\n    \n    # Encrypt the private key with AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, _ = cipher.encrypt_and_digest(privkey.save_pkcs1())\n    \n    # Generate a filename based on 8 random bytes\n    random_bytes = os.urandom(8)\n    filename = f\"private_key_{b64encode(random_bytes).decode()}.txt\"\n    \n    # Save the encrypted private key to a file\n    with open(filename, 'wb') as f:\n        f.write(b64encode(nonce + cipher.nonce + ciphertext))\n    \n    # Return the public key, filename, password, and nonce\n    return pubkey, filename, password, nonce"}
{"task_id": "BigCodeBench/584", "solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\ndef task_func(url):\n    try:\n        # Generate RSA key pair\n        (pubkey, privkey) = rsa.newkeys(1024)\n        \n        # Retrieve content from the specified URL\n        with urllib.request.urlopen(url) as response:\n            content = response.read()\n        \n        # Calculate SHA256 hash of the content\n        content_hash = sha256(content).digest()\n        \n        # Sign the hash with the private key\n        signed_hash = rsa.sign(content_hash, privkey, 'SHA-256')\n        \n        # Return the public key and the signed hash as a hexadecimal string\n        return pubkey, signed_hash.hex(), content_hash\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Error reaching the server: {e.reason}\")\n    except rsa.pkcs1.VerificationError as e:\n        raise ValueError(f\"Error signing the hash: {e}\")"}
{"task_id": "BigCodeBench/585", "solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\ndef task_func(directory):\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(1024)\n    \n    # Encrypt all files in the specified directory using the public key\n    zip_filename = 'encrypted_files.zip'\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                file_path = os.path.join(root, file)\n                with open(file_path, 'rb') as f:\n                    data = f.read()\n                encrypted_data = rsa.encrypt(data, pubkey)\n                zipf.writestr(file, b64encode(encrypted_data))\n    \n    return pubkey, zip_filename"}
{"task_id": "BigCodeBench/586", "solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\nimport os\ndef task_func(file_path):\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n    public_key = pubkey.save_pkcs1().decode('utf-8')\n    \n    # Generate Fernet key\n    fernet_key = Fernet.generate_key()\n    fernet = Fernet(fernet_key)\n    \n    # Read the file to be encrypted\n    with open(file_path, 'rb') as file:\n        file_data = file.read()\n    \n    # Encrypt the file data using Fernet symmetric encryption\n    encrypted_data = fernet.encrypt(file_data)\n    \n    # Encrypt the Fernet key with the RSA public key\n    encrypted_fernet_key = rsa.encrypt(fernet_key, pubkey)\n    encrypted_fernet_key = b64encode(encrypted_fernet_key).decode('utf-8')\n    \n    # Save the encrypted file and the encrypted Fernet key to separate files\n    encrypted_file_path = file_path + '.encrypted'\n    with open(encrypted_file_path, 'wb') as encrypted_file:\n        encrypted_file.write(encrypted_data)\n    \n    encrypted_fernet_key_path = file_path + '.key.encrypted'\n    with open(encrypted_fernet_key_path, 'w') as key_file:\n        key_file.write(encrypted_fernet_key)\n    \n    return public_key, encrypted_file_path, encrypted_fernet_key_path"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n    \n    # Generate AES key\n    aes_key = os.urandom(32)\n    \n    # Encrypt the file using AES\n    with open(file_path, 'rb') as f:\n        data = f.read()\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(os.urandom(16)), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_data)\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n    \n    # Save the encrypted AES key\n    encrypted_aes_key_path = file_path + '.key.enc'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(encrypted_aes_key)\n    \n    return pubkey, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/588", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nRANGE = 100\nSIZE = 1000\ndef task_func():\n    # Generate random integers for 'X' and 'Y' columns\n    X = np.random.randint(0, RANGE, SIZE)\n    Y = np.random.randint(0, RANGE, SIZE)\n    \n    # Create a DataFrame with 'X' and 'Y' columns\n    df = pd.DataFrame({'X': X, 'Y': Y})\n    \n    # Create a scatter plot using Seaborn\n    sns.scatterplot(x='X', y='Y', data=df)\n    \n    # Display the plot\n    plt.show()\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/589", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\ndef task_func():\n    # Generate random 2D points\n    data = np.random.rand(SIZE, 2) * RANGE\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS)\n    kmeans.fit(data)\n    # Plot the results\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n    # Return the data points and the fitted KMeans model\n    return (data, kmeans)"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL cannot be empty\")\n    try:\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Error fetching URL: \" + str(e.reason))\n    doc = pq(html)\n    anchors = doc('a')\n    data = []\n    for anchor in anchors.items():\n        text = anchor.text()\n        href = anchor.attr('href')\n        data.append({'text': text, 'href': href})\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    return df"}
{"task_id": "BigCodeBench/591", "solution": "from datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\ndef task_func(hours, file_path=FILE_PATH):\n    # Generate temperature data for the specified number of hours\n    data = []\n    current_time = datetime.now()\n    for hour in range(hours):\n        time = current_time + timedelta(hours=hour)\n        temperature = randint(10, 35)\n        category = TEMP_CATEGORIES[randint(0, 2)]\n        data.append({'Time': time, 'Temperature': temperature, 'Category': category})\n    \n    # Save the data in a CSV file\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n    \n    # Plot the data using matplotlib\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Temperature'], label='Temperature')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Temperature')\n    ax.set_title('Temperature Over Time')\n    ax.legend()\n    \n    return (file_path, ax)\nhours = 24"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Calculate the end time\n    end_time = datetime.now()\n    start_time = end_time - timedelta(hours=hours)\n    \n    # Generate data for each sensor\n    data = []\n    current_time = start_time\n    while current_time < end_time:\n        row = {'Time': current_time.strftime('%Y-%m-%d %H:%M:%S')}\n        for sensor in SENSORS:\n            # Generate random values for each sensor\n            if sensor == 'Temperature':\n                value = randint(20, 30)\n            elif sensor == 'Humidity':\n                value = randint(30, 70)\n            elif sensor == 'Pressure':\n                value = randint(1000, 1020)\n            row[sensor] = value\n        data.append(row)\n        current_time += timedelta(minutes=1)\n    \n    # Write data to CSV file\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time'] + SENSORS)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    print(f\"Data has been saved to {filename}\")"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for hour in range(hours):\n        row = {'Time': current_time + timedelta(hours=hour)}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(0, 100)\n        data.append(row)\n    \n    # Save to CSV\n    csv_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time'] + VEHICLE_TYPES)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Plot data\n    df = pd.DataFrame(data)\n    df['Time'] = pd.to_datetime(df['Time'])\n    df.set_index('Time', inplace=True)\n    \n    ax = df.plot(kind='line', figsize=(10,5))\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    \n    return csv_path, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Create backup directory if it doesn't exist\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n    \n    # Generate current time\n    current_time = datetime.now()\n    \n    # Generate weather data\n    weather_data = []\n    for _ in range(hours):\n        time = current_time + timedelta(hours=_)\n        condition = choice(WEATHER_CONDITIONS)\n        weather_data.append({'Time': time.strftime('%Y-%m-%d %H:%M:%S'), 'Condition': condition})\n    \n    # Create CSV file\n    file_name = f'weather_{current_time.strftime(\"%Y%m%d%H%M%S\")}.csv'\n    file_path = os.path.join(output_dir, file_name)\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Condition'])\n        writer.writeheader()\n        for data in weather_data:\n            writer.writerow(data)\n    \n    # Backup the file\n    backup_file_path = os.path.join(BACKUP_DIR, file_name)\n    shutil.copy2(file_path, backup_file_path)\n    \n    return file_path"}
{"task_id": "BigCodeBench/595", "solution": "import random\nimport bisect\nfrom array import array\ndef task_func(n=10, total=100):\n    while True:\n        numbers = [random.randint(1, total // n) for _ in range(n)]\n        if sum(numbers) == total:\n            break\n    numbers.sort()\n    new_number = random.randint(1, total // n)\n    insertion_position = bisect.bisect_left(numbers, new_number)\n    return (array('i', numbers), insertion_position)"}
{"task_id": "BigCodeBench/596", "solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func(duration):\n    timestamps = []\n    values = []\n    start_time = time.time()\n    while time.time() - start_time < duration:\n        current_time = datetime.now().strftime('%H:%M:%S.%f')\n        random_value = randint(0, 100)\n        timestamps.append(current_time)\n        values.append(random_value)\n        time.sleep(1)  # Wait for 1 second before generating the next data point\n    return (timestamps, values)"}
{"task_id": "BigCodeBench/597", "solution": "import pandas as pd\nimport time\ndef task_func(data, letter):\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n    # Filter rows where 'Name' starts with the specified letter\n    filtered_df = df[df['Name'].str.startswith(letter)]\n    # Return the 'Name' column as a Series\n    return filtered_df['Name']"}
{"task_id": "BigCodeBench/598", "solution": "import pandas as pd\nimport time\ndef task_func(df, letter):\n    # Filter rows where 'Word' column starts with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the length of the words in the filtered column\n    word_lengths = filtered_df['Word'].str.len()\n    \n    # Create a dictionary of word lengths and their counts\n    length_counts = word_lengths.value_counts().to_dict()\n    \n    return length_counts"}
{"task_id": "BigCodeBench/599", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the lengths of these words\n    filtered_df['Length'] = filtered_df['Word'].str.len()\n    \n    # Create a histogram plot of the word lengths\n    plt.hist(filtered_df['Length'], bins=range(min(filtered_df['Length']), max(filtered_df['Length']) + 2) - 0.5, align='mid')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Word Lengths Starting with ' + letter)\n    plt.show()"}
{"task_id": "BigCodeBench/600", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the lengths of these words\n    word_lengths = filtered_df['Word'].str.len()\n    \n    # Calculate mean, median, and mode of the word lengths\n    mean_length = np.mean(word_lengths)\n    median_length = np.median(word_lengths)\n    mode_length = stats.mode(word_lengths)\n    \n    # Return the results in a dictionary\n    return {\n        'mean': mean_length,\n        'median': median_length,\n        'mode': mode_length\n    }\ndata = {\n    'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape'],\n    'Length': [5, 6, 6, 4, 12, 3, 5]\n}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/601", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, letter):\n    if 'Word' not in df.columns:\n        return None\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    if filtered_df.empty:\n        return None\n    word_lengths = filtered_df['Word'].str.len()\n    plt.figure()\n    sns.boxplot(x=word_lengths)\n    plt.title(f'Box plot of word lengths starting with \"{letter}\"')\n    plt.xlabel('Word Lengths')\n    plt.show()\n    return plt"}
{"task_id": "BigCodeBench/602", "solution": "import numpy as np\nimport pandas as pd\nimport os\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nOUTPUT_DIR = './output'\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate a 2D matrix with random lowercase letters\n    matrix = np.random.choice(LETTERS, size=(10, 10))\n    \n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix)\n    \n    # Write the DataFrame to a CSV file\n    file_path = os.path.join(output_dir, file_path)\n    df.to_csv(file_path, header=False, index=False)"}
{"task_id": "BigCodeBench/603", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(matrix1, matrix2):\n    # Connect two 2D numeric arrays (matrices) along the second axis (columns)\n    combined_matrix = np.hstack((matrix1, matrix2))\n    \n    # Convert the combined matrix into a Pandas DataFrame\n    df = pd.DataFrame(combined_matrix)\n    \n    # Return a string representation of the DataFrame without index and header\n    return df.to_string(index=False, header=False)\nmatrix1 = np.array([[1, 2], [3, 4]])\nmatrix2 = np.array([[5, 6], [7, 8]])"}
{"task_id": "BigCodeBench/604", "solution": "import subprocess\nimport logging\ndef task_func(filepath):\n    try:\n        # Define the compiler and the file to compile\n        compiler = 'g++'  # You can change this to your preferred compiler\n        compile_command = [compiler, filepath, '-o', 'output']\n        \n        # Run the compilation process\n        result = subprocess.run(compile_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        \n        # Log the compilation outcome\n        if result.returncode == 0:\n            logging.info('Compilation successful.')\n        else:\n            logging.error('Compilation failed.')\n            logging.error(result.stderr.decode())\n    except subprocess.CalledProcessError as e:\n        logging.error('Compilation failed with error code: %s', e.returncode)\n        logging.error(e.stderr.decode())\n        raise\n    except FileNotFoundError as e:\n        logging.error('File not found or compiler not found: %s', e)\n        raise"}
{"task_id": "BigCodeBench/605", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    \"\"\"\n    Visualize a 2D numeric array (matrix) as a heatmap using matplotlib, specifying a cmap for the color mapping and interpolation to control the pixel rendering.\n    \n    Parameters:\n    matrix (list of lists): A 2D numeric array to be visualized as a heatmap.\n    \n    Returns:\n    ax (matplotlib.axes._axes.Axes): The Axes object with the heatmap of the 'hot' colormap.\n    \"\"\"\n    # Convert the matrix to a pandas DataFrame for easier manipulation\n    df = pd.DataFrame(matrix)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Generate the heatmap\n    heatmap = ax.imshow(df, cmap='hot', interpolation='bilinear')\n    \n    # Add a color bar to the side\n    plt.colorbar(heatmap)\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/606", "solution": "import pandas as pd\nfrom scipy import stats\ndef task_func(matrix):\n    df = pd.DataFrame(matrix)\n    normalized_df = df.apply(stats.zscore)\n    return normalized_df"}
{"task_id": "BigCodeBench/607", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    # Remove rows based on column values\n    for tup in tuples:\n        df = df[~df[tup[0]].isin(tup[1])]\n    # Generate random scatter plots\n    fig, axes = plt.subplots(nrows=1, ncols=n_plots, figsize=(15, 5))\n    axes = axes.flatten()\n    for i in range(n_plots):\n        cols = sample(COLUMNS, 2)\n        ax = axes[i]\n        ax.scatter(df[cols[0]], df[cols[1]])\n        ax.set_title(f'Plot {i+1}')\n    plt.tight_layout()\n    return df, axes.tolist()"}
{"task_id": "BigCodeBench/608", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on values of multiple columns\n    for col, val in tuples:\n        df = df[df[col] != val]\n    \n    # Create n random pairs of two columns against each other to generate pairplots\n    pairs = sample(list(combinations(COLUMNS, 2)), n_plots)\n    pairplots = []\n    for pair in pairs:\n        sns.pairplot(df, vars=pair)\n        plt.show()\n        pairplots.append(plt.gcf())\n    \n    return df, pairplots"}
{"task_id": "BigCodeBench/609", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nfrom random import sample\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on the list of tuples\n    for tup in tuples:\n        df = df[~df.eq(tup).all(axis=1)]\n    \n    # Generate up to 'n_plots' scatter plots for random combinations of two columns\n    columns = df.columns\n    plot_combinations = list(combinations(columns, 2))\n    if n_plots > len(plot_combinations):\n        n_plots = len(plot_combinations)\n    plot_list = []\n    for _ in range(n_plots):\n        col_pair = sample(plot_combinations, 1)[0]\n        plot_combinations.remove(col_pair)\n        plt.scatter(df[col_pair[0]], df[col_pair[1]])\n        plot_list.append((col_pair, plt))\n    \n    return df, plot_list"}
{"task_id": "BigCodeBench/610", "solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    # Remove rows based on values of multiple columns\n    for tup in tuples:\n        df = df[~df[tup[0]].isin(tup[1])]\n    \n    # Create n random joint plots of two columns against each other if the DataFrame is not empty\n    plots = []\n    if not df.empty:\n        cols = sample(COLUMNS, 2)\n        plot = sns.jointplot(x=cols[0], y=cols[1], data=df)\n        plots.append(plot)\n    \n    return df, plots"}
{"task_id": "BigCodeBench/611", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on values of multiple columns\n    for col, value in tuples:\n        df = df[df[col] != value]\n    \n    # Create n random line plots of two columns against each other\n    plot_details = []\n    for _ in range(n_plots):\n        cols = sample(COLUMNS, 2)\n        plt.plot(df[cols[0]], df[cols[1]])\n        plt.title(f'Plot of {cols[0]} vs {cols[1]}')\n        plt.xlabel(cols[0])\n        plt.ylabel(cols[1])\n        plt.show()\n        plot_details.append((cols[0], cols[1]))\n    \n    return df, plot_details"}
{"task_id": "BigCodeBench/612", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    # Create a DataFrame with teams, goals, and penalties\n    data = {'Team': teams, 'Goals': goals, 'Penalties': penalties}\n    df = pd.DataFrame(data)\n    \n    # Calculate Penalties Cost\n    df['Penalties Cost'] = df['Penalties'].apply(lambda x: choice(penalties_costs) * x)\n    \n    # Calculate Performance Score\n    df['Performance Score'] = df['Goals'] - df['Penalties Cost']\n    df['Performance Score'] = df['Performance Score'].abs()\n    \n    return df[['Team', 'Goals', 'Penalties', 'Penalties Cost', 'Performance Score']]\ngoals = [10, 15, 20, 25, 30]\npenalties = [2, 3, 1, 4, 0]"}
{"task_id": "BigCodeBench/613", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\ndef task_func(goals, penalties):\n    # Calculate net scores by subtracting penalties from goals\n    net_scores = [goals[i] - penalties[i] for i in range(len(goals))]\n    \n    # Clip scores to stay within the specified range\n    clipped_scores = [max(GOALS_RANGE[0], min(GOALS_RANGE[1], score)) for score in net_scores]\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Team': TEAMS, 'Score': clipped_scores})\n    \n    # Visualize the results with a bar chart\n    plt.bar(df['Team'], df['Score'])\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Team Scores')\n    plt.show()\n    \n    return df"}
{"task_id": "BigCodeBench/614", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(goals, penalties):\n    # Create a DataFrame with the given goals and penalties\n    data = {'Team': ['Team A', 'Team B', 'Team C', 'Team D'],\n            'Goals': goals,\n            'Penalties': penalties}\n    df = pd.DataFrame(data)\n    \n    # Create a pairplot visualization\n    pairplot = sns.pairplot(df, vars=['Goals', 'Penalties'])\n    \n    return df, pairplot\ngoals = [10, 15, 20, 25]\npenalties = [5, 10, 15, 20]"}
{"task_id": "BigCodeBench/615", "solution": "from random import randint, seed\nimport pandas as pd\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n    team_names = ['Team A', 'Team B', 'Team C', 'Team D']\n    team_data = []\n    for team in team_names:\n        goals_scored = sum(goals[team])\n        penalties_scored = sum(penalties[team])\n        total_fines = penalties_scored * 1000  # Assuming each penalty is worth 1000 fines\n        team_data.append({'Team': team, 'Match Result': goals_scored, 'Accumulated Fines': total_fines})\n    df = pd.DataFrame(team_data)\n    return df"}
{"task_id": "BigCodeBench/616", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    # Generate random goals and penalties for each team\n    team_data = []\n    for team in teams:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        team_data.append({'Team': team, 'Goals': team_goals, 'Penalty Cost': team_penalties * penalty_cost})\n    \n    # Create DataFrame\n    df = pd.DataFrame(team_data)\n    \n    # Create bar plot\n    fig, ax = plt.subplots()\n    ax.bar(df['Team'], df['Goals'], label='Goals')\n    ax.bar(df['Team'], df['Penalty Cost'], bottom=df['Goals'], label='Penalty Fines')\n    ax.set_ylabel('Points')\n    ax.set_title('Football Match Results')\n    ax.legend()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/617", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    # Set the random seed if provided\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    # Initialize a dictionary to hold team results\n    team_results = {team: {'Goals': 0, 'Penalty Cost': 0} for team in teams}\n    \n    # Simulate match results\n    for _ in range(goals):\n        # Randomly select two teams for the match\n        team1, team2 = sorted(sample(teams, 2))\n        \n        # Simulate goals for each team\n        goals_team1 = randint(0, 5)\n        goals_team2 = randint(0, 5)\n        \n        # Update team results\n        team_results[team1]['Goals'] += goals_team1\n        team_results[team2]['Goals'] += goals_team2\n        \n        # Simulate penalties\n        for _ in range(penalties):\n            # Randomly select a team to take a penalty\n            penalty_taker = sample(teams, 1)[0]\n            \n            # Simulate penalty outcome\n            penalty_result = randint(0, 1)\n            if penalty_result:\n                # Penalty scored\n                if penalty_taker == team1:\n                    team_results[team1]['Goals'] += 1\n                else:\n                    team_results[team2]['Goals'] += 1\n            else:\n                # Penalty missed\n                team_results[penalty_taker]['Penalty Cost'] += PENALTY_COST\n    \n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame.from_dict(team_results, orient='index')\n    \n    # Reset index to have 'Team' as a column\n    df.reset_index(inplace=True)\n    df.rename(columns={'index': 'Team'}, inplace=True)\n    \n    # Sort the DataFrame by 'Goals' in descending order\n    df.sort_values(by='Goals', ascending=False, inplace=True)\n    \n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Team'], df['Goals'], color='skyblue')\n    plt.xlabel('Team')\n    plt.ylabel('Total Goals')\n    plt.title('Total Goals per Team')\n    plt.show()\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Generate random goals and penalties for each team\n    team_goals = {team: randint(0, goals) for team in TEAMS}\n    team_penalties = {team: randint(0, penalties) for team in TEAMS}\n    \n    # Convert penalties to fines\n    team_fines = {team: penalty * PENALTY_COST for team, penalty in team_penalties.items()}\n    \n    # Create a DataFrame for match results\n    data = {\n        'Team': list(team_goals.keys()),\n        'Goals': list(team_goals.values()),\n        'Penalty Cost': list(team_fines.values())\n    }\n    df = pd.DataFrame(data)\n    \n    # Create plots\n    goals_plot = sns.barplot(x='Team', y='Goals', data=df)\n    fines_plot = sns.barplot(x='Team', y='Penalty Cost', data=df)\n    \n    # Return the DataFrame and the list of plots\n    return df, [goals_plot, fines_plot]"}
{"task_id": "BigCodeBench/619", "solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None):\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    # Simulate match results\n    team_goals = {team: randint(0, goals) for team in TEAMS}\n    team_penalties = {team: randint(0, penalties) for team in TEAMS}\n    \n    # Calculate penalty costs\n    team_penalty_costs = {team: team_penalties[team] * PENALTY_COST for team in TEAMS}\n    \n    # Create DataFrame\n    data = {\n        'Team': list(team_goals.keys()),\n        'Goals': list(team_goals.values()),\n        'Penalty Cost': list(team_penalty_costs.values())\n    }\n    df = pd.DataFrame(data)\n    \n    # Prepare data for linear regression\n    X = df[['Goals']]\n    y = df['Penalty Cost']\n    \n    # Train linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    return (df, model)"}
{"task_id": "BigCodeBench/620", "solution": "import numpy as np\nimport pandas as pd\nRANGE = (1, 100)\ndef task_func(L):\n    # Initialize an empty list to store the dimensions\n    dimensions = []\n    \n    # Iterate through each pair in the input list of lists\n    for pair in L:\n        # Multiply the two integers in the pair to get the dimension\n        dim = pair[0] * pair[1]\n        # Append the dimension to the list\n        dimensions.append(dim)\n    \n    # Create a DataFrame with random integers\n    # The number of rows is the first dimension, and the number of columns is the second dimension\n    df = pd.DataFrame(np.random.randint(RANGE[0], RANGE[1], size=(dimensions[0], dimensions[1])))\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/621", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists into a single list\n    flat_list = list(chain.from_iterable(L))\n    \n    # Convert the list of integers to a numpy array\n    data = np.array(flat_list)\n    \n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data.reshape(-1, 1))\n    \n    # Plot the standardized values\n    plt.plot(standardized_data)\n    plt.show()\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/622", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain.from_iterable(L))\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(flat_list)\n    \n    # Plot the histogram\n    plt.hist(flat_list, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the normal distribution overlay\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the axes object\n    return plt.gca()\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/623", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists into a single list of integers\n    flat_list = list(chain.from_iterable(L))\n    \n    # Reshape the list into a 2D array for KMeans\n    data = np.array(flat_list).reshape(-1, 1)\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=3)  # You can adjust the number of clusters as needed\n    kmeans.fit(data)\n    \n    # Get the cluster labels\n    labels = kmeans.labels_\n    \n    # Create a scatter plot\n    plt.scatter(range(len(flat_list)), flat_list, c=labels)\n    \n    # Return the Axes object\n    return plt.gca()\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/624", "solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\nN_COMPONENTS = 2\ndef task_func(L):\n    # Convert list of lists to a 2D numpy array\n    data = np.array(L)\n    \n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(data)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    \n    # Return the PCA result and the scatter plot\n    return (pca_result, ax)"}
{"task_id": "BigCodeBench/625", "solution": "import math\nfrom random import randint\nimport pandas as pd\ndef task_func(cities_list):\n    data = []\n    for city in cities_list:\n        population = math.ceil(randint(100000, 1000000) / 1000) * 1000\n        data.append({'City': city, 'Population': population})\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/626", "solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\ndef task_func(date_str, from_tz):\n    # Parse the input datetime string\n    dt = parse(date_str)\n    \n    # Localize the datetime to the specified timezone\n    from_tz = pytz.timezone(from_tz)\n    dt = from_tz.localize(dt)\n    \n    # Convert to a randomly chosen timezone\n    to_tz = choice(TIMEZONES)\n    to_tz = pytz.timezone(to_tz)\n    dt = dt.astimezone(to_tz)\n    \n    # Format the datetime string\n    converted_date_str = dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n    \n    return (converted_date_str, to_tz.zone)"}
{"task_id": "BigCodeBench/627", "solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\ndef task_func(products_list):\n    # Initialize a list to hold the sales data for each product\n    sales_data = []\n    \n    # Loop through each product in the products_list\n    for product in products_list:\n        # Initialize a list to hold the sales data for the current product\n        product_sales = []\n        \n        # Generate random sales data for each month (12 months)\n        for month in range(1, 13):\n            # Generate a random sales figure between 100 and 1000\n            sales = randint(100, 1000)\n            product_sales.append(sales)\n        \n        # Calculate the average sales for the product\n        avg_sales = mean(product_sales)\n        \n        # Append the product name and its sales data to the sales_data list\n        sales_data.append([product] + product_sales + [avg_sales])\n    \n    # Define the column names for the DataFrame\n    columns = ['Product'] + [f'Month {month}' for month in range(1, 13)] + ['Average Sales']\n    \n    # Create a pandas DataFrame from the sales_data list\n    df = pd.DataFrame(sales_data, columns=columns)\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/628", "solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate random parameters for the sine wave\n    frequency = randint(1, 10)\n    amplitude = randint(1, 10)\n    phase_shift = randint(0, 360)\n    \n    # Generate time values\n    time = [i for i in range(0, 100)]\n    \n    # Generate sine wave values\n    sine_wave = [amplitude * math.sin(2 * math.pi * frequency * t / 100 + phase_shift * math.pi / 180) for t in time]\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot the sine wave\n    ax.plot(time, sine_wave)\n    \n    # Set the title and labels\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the axis object\n    return ax"}
{"task_id": "BigCodeBench/629", "solution": "import os\nimport time\nimport pandas as pd\nOUTPUT_DIR = './output'\ndef task_func(dataset, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Writes multiple Pandas DataFrames to a single CSV file, separating each DataFrame by a line of hyphens (\"------\").\n    The function does not return any value.\n\n    Parameters:\n    - dataset: A list of Pandas DataFrames to be written to the CSV file.\n    - filename: The name of the output CSV file.\n    - output_dir: The directory where the output CSV file will be saved. Default is './output'.\n\n    Returns:\n    - None\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n\n    with open(filepath, 'w', newline='') as f:\n        for idx, df in enumerate(dataset):\n            if idx > 0:\n                f.write('------\\n')\n            df.to_csv(f, index=False)"}
{"task_id": "BigCodeBench/630", "solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Saves a Pandas DataFrame to a JSON file in a specified directory.\n\n    Parameters:\n    - df: pandas DataFrame to be saved.\n    - filename: Name of the file to save the DataFrame as.\n    - output_dir: Directory where the file will be saved. Defaults to OUTPUT_DIR.\n\n    Returns:\n    - str: The full file path where the DataFrame is saved.\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Construct the full file path\n    file_path = os.path.join(output_dir, filename)\n\n    # Save the DataFrame to JSON\n    df.to_json(file_path, orient='records')\n\n    return file_path"}
{"task_id": "BigCodeBench/631", "solution": "import csv\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Saves a Pandas DataFrame to a CSV file in a specified directory.\n\n    Parameters:\n    - df: Pandas DataFrame to be saved.\n    - filename: Name of the CSV file to be created.\n    - output_dir: Directory where the CSV file will be saved. Default is './output'.\n\n    Returns:\n    - str: The absolute path of the saved CSV file.\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Construct the full path for the CSV file\n    csv_path = os.path.join(output_dir, filename)\n\n    # Save the DataFrame to CSV\n    df.to_csv(csv_path, index=False)\n\n    # Return the absolute path of the saved CSV file\n    return os.path.abspath(csv_path)"}
{"task_id": "BigCodeBench/632", "solution": "import pandas as pd\nimport time\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Write a Pandas DataFrame into a JSON Lines file and save it in a specified directory.\n\n    Parameters:\n    - df (pd.DataFrame): The DataFrame to be saved.\n    - filename (str): The name of the file to save the DataFrame as.\n\n    Returns:\n    - str: The full path where the JSON Lines file was saved.\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Construct the full path for the file\n    filepath = os.path.join(OUTPUT_DIR, filename)\n\n    # Write the DataFrame to a JSON Lines file\n    df.to_json(filepath, orient='records', lines=True)\n\n    return filepath"}
{"task_id": "BigCodeBench/633", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter"}
{"task_id": "BigCodeBench/634", "solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\ndef task_func(input_list: list, repetitions: int) -> Any:\n    # Flatten the list with multiple repetitions\n    flattened_list = list(itertools.chain.from_iterable(itertools.repeat(input_list, repetitions)))\n    # Calculate the mode of the flattened list\n    mode_result = stats.mode(flattened_list)\n    return mode_result"}
{"task_id": "BigCodeBench/635", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\ndef task_func(text, n=2):\n    # Remove duplicate consecutive words\n    text = re.sub(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    text = ' '.join(filtered_words)\n    \n    # Generate co-occurrence matrix\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    matrix = vectorizer.fit_transform([text])\n    feature_names = vectorizer.get_feature_names_out()\n    co_occurrence_matrix = matrix.toarray()\n    \n    # Plot the matrix\n    plt.imshow(co_occurrence_matrix, cmap='hot', interpolation='nearest')\n    plt.colorbar()\n    plt.show()\n    \n    return (co_occurrence_matrix, plt.gca())\ntext = \"This is a test text with some words and some duplicate words and some stopwords\""}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Generate a DataFrame with random integers between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.replace(0, np.nan).count()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    \n    # Return the DataFrame and the Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Generate a list of courses\n    courses = ['Math', 'Science', 'History', 'English', 'Art']\n    \n    # Generate a list of students\n    students = [f'Student {i+1}' for i in range(num_students)]\n    \n    # Generate random grades for each student in each course\n    grades = {course: [np.random.randint(0, 101) for _ in range(num_students)] for course in courses}\n    \n    # Create a DataFrame from the grades\n    df = pd.DataFrame(grades, index=students)\n    \n    # Calculate the average grade in each course\n    avg_grades = df.mean()\n    \n    # Calculate the number of students with a passing grade (>= 60)\n    passing_counts = (df >= 60).sum()\n    \n    # Create a bar plot for average grades\n    avg_plot = avg_grades.plot(kind='bar', title='Course-wise Average Grades')\n    \n    # Create a bar plot for passing grade counts\n    passing_plot = passing_counts.plot(kind='bar', title='Course-wise Passing Grade Counts')\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    # Return the DataFrame and the bar plot's Axes object\n    return df, avg_plot, passing_plot\nnum_students = 10"}
{"task_id": "BigCodeBench/638", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(num_teams=5, num_games=100):\n    # Generate random scores for each game\n    scores = np.random.randint(0, 100, size=(num_teams, num_games))\n    \n    # Create a list of team names\n    teams = [f\"Team {i+1}\" for i in range(num_teams)]\n    \n    # Create a list of game names\n    games = [f\"Game {i+1}\" for i in range(num_games)]\n    \n    # Create a DataFrame with teams as index and games as columns\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    \n    return df"}
{"task_id": "BigCodeBench/639", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(num_samples=100, num_features=5):\n    # Generate a DataFrame with random values\n    np.random.seed(0)\n    data = np.random.rand(num_samples, num_features)\n    df = pd.DataFrame(data, columns=[f'Feature_{i}' for i in range(num_features)])\n    \n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n    \n    # Visualize the correlation matrix using a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Heatmap')\n    plt.show()\n    \n    return df, plt.gca()"}
{"task_id": "BigCodeBench/640", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\ndef task_func():\n    # Generate a DataFrame with random sales figures\n    sales_data = np.random.randint(100, 1000, size=(12, 5))\n    sales_df = pd.DataFrame(sales_data, columns=PRODUCTS, index=MONTHS)\n    \n    # Calculate total sales per product across all months\n    total_sales = sales_df.sum().reset_index()\n    total_sales.columns = ['Product', 'Total_Sales']\n    \n    # Line plot for total sales per product\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(x='Product', y='Total_Sales', data=total_sales)\n    plt.title('Total Sales per Product')\n    plt.show()\n    \n    # Heatmap for sales figures across products and months\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(sales_df, annot=True, fmt='d', cmap='YlGnBu')\n    plt.title('Sales Figures across Products and Months')\n    plt.show()\n    \n    return sales_df"}
{"task_id": "BigCodeBench/641", "solution": "import os\nimport re\nimport pandas as pd\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Searches for files in the specified directory that match a given regex pattern.\n    This function walks through the directory, matches filenames against the pattern,\n    and saves the matched file paths to a CSV file. It returns a DataFrame of these paths\n    with a single column 'File Path'.\n\n    Parameters:\n    - pattern (str): The regex pattern to match against filenames.\n    - directory (str): The directory to search within.\n    - output_csv (str): The path to the CSV file where matched file paths will be saved.\n\n    Returns:\n    - pd.DataFrame: DataFrame with a single column 'File Path' of matched paths.\n    \"\"\"\n    # Compile the regex pattern for efficiency\n    regex = re.compile(pattern)\n\n    # List to hold matched file paths\n    matched_files = []\n\n    # Walk through the directory tree\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            # Check if the filename matches the pattern\n            if regex.search(filename):\n                # Construct the full file path\n                file_path = os.path.join(dirpath, filename)\n                # Append the file path to the list\n                matched_files.append(file_path)\n\n    # Create a DataFrame from the list of matched file paths\n    df = pd.DataFrame(matched_files, columns=['File Path'])\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(output_csv, index=False)\n\n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/642", "solution": "import binascii\nimport hashlib\nimport re\nimport os\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Searches for files within the specified directory matching a given regex pattern and computes a SHA256 hash of each file's content.\n    \n    Args:\n    directory (str): The directory to search within.\n    pattern (str): The regex pattern to match file names. Defaults to r\"(?<!Distillr)\\\\AcroTray\\.exe\".\n    \n    Returns:\n    dict: A dictionary with file paths as keys and their SHA256 hashes as values.\n    \"\"\"\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n    \n    # Initialize an empty dictionary to store file paths and their hashes\n    file_hashes = {}\n    \n    # Walk through the directory tree\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            # Check if the filename matches the pattern\n            if regex.search(filename):\n                # Construct the full file path\n                file_path = os.path.join(dirpath, filename)\n                # Open the file in binary mode\n                with open(file_path, 'rb') as file:\n                    # Read the file content\n                    content = file.read()\n                    # Compute the SHA256 hash\n                    sha256_hash = hashlib.sha256(content).hexdigest()\n                    # Store the hash in the dictionary\n                    file_hashes[file_path] = sha256_hash\n    \n    return file_hashes"}
{"task_id": "BigCodeBench/643", "solution": "import re\nimport pandas as pd\nimport numpy as np\nDATA_PATTERN = r'>\\d+\\.\\d+<'\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Extracts numeric data from a Pandas DataFrame based on a specific pattern.\n    The function searches each cell for occurrences of the regex pattern '>number<number>' (e.g., '>1.23<') and replaces the cell content with the extracted numeric value.\n    If no match is found, the cell is replaced with NaN.\n\n    Parameters:\n    - dataframe: pd.DataFrame\n    - data_pattern: str (default is DATA_PATTERN)\n\n    Returns:\n    - pd.DataFrame: A modified DataFrame with cells containing the extracted numeric values or NaN.\n    \"\"\"\n    # Define the regex pattern\n    pattern = re.compile(data_pattern)\n\n    # Function to extract the number from the matched string\n    def extract_number(match):\n        return float(match.group(0)[1:-1])\n\n    # Apply the pattern to each cell in the DataFrame\n    for col in dataframe.columns:\n        for idx, value in dataframe[col].items():\n            if isinstance(value, str):\n                match = pattern.search(value)\n                if match:\n                    dataframe.at[idx, col] = extract_number(match)\n                else:\n                    dataframe.at[idx, col] = np.nan\n            else:\n                dataframe.at[idx, col] = np.nan\n\n    return dataframe"}
{"task_id": "BigCodeBench/644", "solution": "import hashlib\nimport base64\nimport os\ndef task_func(filename, data, password):\n    # Generate a key from the password using SHA-256\n    key = hashlib.sha256(password.encode()).digest()\n    \n    # Create a new file or overwrite an existing one\n    with open(filename, 'wb') as file:\n        # Encrypt the data using the key\n        encrypted_data = encrypt_data(data, key)\n        # Write the encrypted data to the file\n        file.write(encrypted_data)\n    \n    # Return the encrypted string\n    return encrypted_data.decode()\ndef encrypt_data(data, key):\n    # Convert the data to bytes\n    data_bytes = data.encode()\n    # Use the key to encrypt the data\n    encrypted_bytes = bytearray()\n    for i in range(len(data_bytes)):\n        encrypted_bytes.append(data_bytes[i] ^ key[i % len(key)])\n    # Encode the encrypted bytes to base64\n    encrypted_data = base64.b64encode(bytes(encrypted_bytes))\n    return encrypted_data"}
{"task_id": "BigCodeBench/645", "solution": "import os\nimport pandas as pd\ndef task_func(filename: str) -> pd.DataFrame:\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n    df = pd.read_csv(filename)\n    os.remove(filename)\n    return df"}
{"task_id": "BigCodeBench/646", "solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\nOUTPUT_DIR = './output'\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n    \n    # Convert the date strings to datetime objects\n    df[date_column] = df[date_column].apply(lambda x: parse(x).date())\n    \n    # Extract the year from the datetime objects\n    df['year'] = df[date_column].dt.year\n    \n    # Draw a histogram of the year distribution\n    plt.hist(df['year'], bins=range(df['year'].min(), df['year'].max()+2), align='left', rwidth=0.8)\n    plt.xlabel('Year')\n    plt.ylabel('Frequency')\n    plt.title('Year Distribution')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the plot object\n    return plt.gca()"}
{"task_id": "BigCodeBench/647", "solution": "import pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string into a datetime object\n    dt = parse(date_str)\n    \n    # Localize the datetime object to the source time zone\n    from_tz_obj = pytz.timezone(from_tz)\n    dt = from_tz_obj.localize(dt)\n    \n    # Convert the datetime object to the destination time zone\n    to_tz_obj = pytz.timezone(to_tz)\n    dt = dt.astimezone(to_tz_obj)\n    \n    # Get the current time in the destination time zone\n    current_dt = pytz.datetime.datetime.now(to_tz_obj)\n    \n    # Calculate the time difference in seconds\n    time_diff = (current_dt - dt).total_seconds()\n    \n    return int(time_diff)"}
{"task_id": "BigCodeBench/648", "solution": "from dateutil.parser import parse\nfrom datetime import timedelta\ndef task_func(date_str):\n    date = parse(date_str)\n    while date.weekday() >= 5:  # 5 and 6 are Saturday and Sunday\n        date += timedelta(days=1)\n    return date"}
{"task_id": "BigCodeBench/649", "solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\ndef task_func(dates_str_list):\n    # Create an empty dictionary to store the weekday counts\n    weekday_counts = {day: 0 for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']}\n    \n    # Iterate through each date string in the list\n    for date_str in dates_str_list:\n        # Parse the date string into a datetime object\n        date = parse(date_str)\n        \n        # Get the weekday name from the datetime object\n        weekday = date.strftime('%A')\n        \n        # Increment the count for that weekday in the dictionary\n        weekday_counts[weekday] += 1\n    \n    # Create a pandas Series from the dictionary\n    series = pd.Series(weekday_counts)\n    \n    # Return the Series\n    return series"}
{"task_id": "BigCodeBench/650", "solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, tz_str):\n    # Parse the input date string\n    date = parse(date_str)\n    \n    # Get the timezone object\n    tz = pytz.timezone(tz_str)\n    \n    # Localize the date to the specified timezone\n    date = tz.localize(date)\n    \n    # Get the current time in the specified timezone\n    now = datetime.now(tz)\n    \n    # Calculate the difference between the current time and the next New Year\n    next_new_year = date.replace(year=date.year + 1, month=1, day=1, hour=0, minute=0, second=0, microsecond=0)\n    if next_new_year < now:\n        next_new_year = next_new_year.replace(year=next_new_year.year + 1)\n    \n    # Calculate the time difference in seconds\n    time_diff = (next_new_year - now).total_seconds()\n    \n    return int(time_diff)"}
{"task_id": "BigCodeBench/651", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, target_value):\n    if df.empty:\n        return pd.Series(), None\n    # Convert the input dic of list to DataFrame\n    df = pd.DataFrame(df)\n    # Searcher in this DataFrame for rows with cells equal to the provided target_value\n    matching_rows = df[df.eq(target_value).any(axis=1)]\n    # Count of such rows per column\n    counts = matching_rows.count()\n    # Plot the count of such rows per column\n    if not counts.empty:\n        counts.plot(kind='bar')\n        plt.title('Count of rows with target value per column')\n        plt.xlabel('Columns')\n        plt.ylabel('Count')\n        plt.show()\n    else:\n        plt.close()\n    return counts, plt.gca() if not counts.empty else None"}
{"task_id": "BigCodeBench/652", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    # Find the row indices where the first cell matches the target value\n    matching_rows = np.where(array[:, 0] == target_value)[0]\n    \n    if len(matching_rows) == 0:\n        return 'N/A'\n    \n    # Extract the indices for statistical analysis\n    indices = array[matching_rows, 0]\n    \n    # Perform statistical analysis\n    mean = np.mean(indices)\n    variance = np.var(indices)\n    skewness = stats.skew(indices)\n    kurtosis = stats.kurtosis(indices)\n    \n    # Plot the distribution of the indices\n    plt.hist(indices, bins=10, edgecolor='black')\n    plt.title('Distribution of Indices')\n    plt.xlabel('Indices')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return (mean, variance, skewness, kurtosis)"}
{"task_id": "BigCodeBench/653", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(dataframe, target_value='332'):\n    # Create a DataFrame with Boolean values indicating the presence of the target value\n    result_df = dataframe == target_value\n    \n    # Create a heatmap using seaborn\n    plt.figure(figsize=(10, 6))\n    heatmap = sns.heatmap(result_df, cmap='coolwarm', center=0)\n    \n    # Return the result DataFrame and the Axes object of the heatmap\n    return (result_df, heatmap)"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Filter the array to get indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n    # Extract the second column for fitting\n    y = array[indices, 1]\n    # Generate x values (indices)\n    x = np.arange(len(y))\n    # Define the exponential decay function\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    # Fit the function to the data\n    popt, pcov = optimize.curve_fit(func, x, y)\n    # Plot the original data and the fitted function\n    plt.plot(x, y, 'o', label='original data')\n    plt.plot(x, func(x, *popt), 'r', label='fitted function')\n    plt.legend()\n    plt.show()\n    return popt, plt.gca()"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    def preprocess(text):\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n    processed_texts = [preprocess(text) for text in texts]\n    \n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=1000)\n    X = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    W = nmf.fit_transform(X)\n    H = nmf.components_\n    \n    # Get the top words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(H):\n        print(\"Topic %d:\" % (topic_idx + 1))\n        top_features = topic.argsort()[:-11:-1]\n        topic_words = [feature_names[i] for i in top_features]\n        topics.append(topic_words)\n        print(\" \".join(topic_words))\n    return topics"}
{"task_id": "BigCodeBench/656", "solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    # Cleaning the text\n    # Remove non-alphanumeric characters except spaces\n    text = ALPHANUMERIC.sub(' ', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', PUNCTUATIONS))\n    \n    # Analyze sentiment\n    sentiment_scores = sia.polarity_scores(text)\n    \n    return sentiment_scores"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # Download necessary NLTK data files\n    nltk.download('stopwords')\n    \n    # If stopwords are not provided, use the default English stopwords\n    if stopwords is None:\n        stopwords = set(stopwords.words('english'))\n    \n    # Clean and preprocess the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase the text\n        cleaned_text = cleaned_text.lower()\n        # Tokenize the text\n        words = nltk.word_tokenize(cleaned_text)\n        # Remove stop words\n        words = [word for word in words if word not in stopwords]\n        # Join the words back into a string\n        cleaned_text = ' '.join(words)\n        cleaned_texts.append(cleaned_text)\n    \n    # Tokenize the cleaned texts into words\n    tokenized_texts = [nltk.word_tokenize(text) for text in cleaned_texts]\n    \n    # Train the Word2Vec model\n    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model"}
{"task_id": "BigCodeBench/658", "solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts):\n    # Preprocess the text data\n    preprocessed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        lower_text = cleaned_text.lower()\n        # Remove stop words\n        words = lower_text.split()\n        filtered_words = [word for word in words if word not in STOPWORDS]\n        # Reconstruct the text\n        preprocessed_text = ' '.join(filtered_words)\n        preprocessed_texts.append(preprocessed_text)\n    # Create a CountVectorizer instance\n    vectorizer = CountVectorizer()\n    # Fit and transform the preprocessed texts\n    X = vectorizer.fit_transform(preprocessed_texts)\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    return df"}
{"task_id": "BigCodeBench/659", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots()\n    for i in range(len(x)):\n        ax.plot(x[i], y[i], label=labels[i])\n    ax.legend()\n    return fig"}
{"task_id": "BigCodeBench/660", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(x, y, labels):\n    \"\"\"\n    Scale the \"x\" and \"y\" arrays using the standard scaler of sklearn and plot them with given labels.\n    Each pair of x and y arrays are scaled independently and plotted as a separate series with a label.\n    The function should output with:\n        matplotlib.figure.Figure: The figure object containing the plot.\n    \"\"\"\n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n    \n    # Scale the x and y arrays independently\n    x_scaled = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n    y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n    \n    # Plot the scaled x and y arrays with their respective labels\n    ax.plot(x_scaled, y_scaled, label=labels)\n    \n    # Add a legend\n    ax.legend()\n    \n    # Return the figure object\n    return fig"}
{"task_id": "BigCodeBench/661", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(x, y, labels):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({'x': x, 'y': y, 'labels': labels})\n    \n    # Pivot the DataFrame to create a matrix for the heatmap\n    df_pivot = df.pivot(index='x', columns='y', values='labels')\n    \n    # Create the heatmap using seaborn\n    ax = sns.heatmap(df_pivot, annot=True, fmt='s')\n    \n    return ax, df_pivot"}
{"task_id": "BigCodeBench/662", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(x, y, labels):\n    # Combine x and y into a single array\n    data = np.column_stack((x, y))\n    \n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(data)\n    transformed = pca.transform(data)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot the transformed data\n    for label in np.unique(labels):\n        mask = labels == label\n        ax.scatter(transformed[mask, 0], transformed[mask, 1], label=label)\n    \n    # Add labels and title\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA of x and y')\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return fig"}
{"task_id": "BigCodeBench/663", "solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\ndef task_func(x, y, labels):\n    \"\"\"\n    Fits an exponential curve to given data points and plots the curves with labels.\n\n    Parameters:\n    x (list of lists): A list of lists, where each sublist contains x-values for a set of data points.\n    y (list of lists): A list of lists, where each sublist contains y-values for a set of data points.\n    labels (list of str): A list of strings, where each string is a label for the corresponding data set.\n\n    Returns:\n    matplotlib.figure.Figure: The figure object that contains the plotted curves.\n    \"\"\"\n    # Define the exponential function to fit\n    def exp_func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Initialize a figure\n    fig, ax = plt.subplots()\n\n    # Fit the exponential function to each set of data points and plot\n    for xi, yi, label in zip(x, y, labels):\n        # Fit the exponential function\n        params, _ = curve_fit(exp_func, xi, yi)\n        a, b, c = params\n\n        # Generate x values for the fitted curve\n        x_fit = np.linspace(min(xi), max(xi), 100)\n        y_fit = exp_func(x_fit, a, b, c)\n\n        # Plot the fitted curve\n        ax.plot(x_fit, y_fit, label=label)\n\n    # Add labels and legend\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    # Show the plot\n    plt.show()\n\n    return fig\nx = [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]]\ny = [[1, 2, 4, 8, 16, 32], [1, 3, 9, 27, 81, 243]]\nlabels = ['Data Set 1', 'Data Set 2']"}
{"task_id": "BigCodeBench/664", "solution": "import statistics\nimport matplotlib.pyplot as plt\ndef task_func(sales_data):\n    # Extracting data\n    months = sales_data['Month']\n    product_a = sales_data['Product A']\n    product_b = sales_data['Product B']\n    product_c = sales_data['Product C']\n    product_d = sales_data['Product D']\n    product_e = sales_data['Product E']\n\n    # Calculating standard deviations\n    std_a = statistics.stdev(product_a)\n    std_b = statistics.stdev(product_b)\n    std_c = statistics.stdev(product_c)\n    std_d = statistics.stdev(product_d)\n    std_e = statistics.stdev(product_e)\n\n    # Creating the plot\n    fig, ax = plt.subplots()\n\n    # Plotting each product with standard deviation shading\n    ax.plot(months, product_a, label='Product A')\n    ax.fill_between(months, [x - std_a for x in product_a], [x + std_a for x in product_a], color='blue', alpha=0.2)\n\n    ax.plot(months, product_b, label='Product B')\n    ax.fill_between(months, [x - std_b for x in product_b], [x + std_b for x in product_b], color='green', alpha=0.2)\n\n    ax.plot(months, product_c, label='Product C')\n    ax.fill_between(months, [x - std_c for x in product_c], [x + std_c for x in product_c], color='red', alpha=0.2)\n\n    ax.plot(months, product_d, label='Product D')\n    ax.fill_between(months, [x - std_d for x in product_d], [x + std_d for x in product_d], color='purple', alpha=0.2)\n\n    ax.plot(months, product_e, label='Product E')\n    ax.fill_between(months, [x - std_e for x in product_e], [x + std_e for x in product_e], color='orange', alpha=0.2)\n\n    # Adding labels and legend\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/665", "solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\ndef task_func(src_dir, dst_dir):\n    # List all files in the source directory\n    files = os.listdir(src_dir)\n    \n    # Define the patterns to match\n    patterns = ['*.txt', '*.docx']\n    \n    # Find files that match any of the patterns\n    matching_files = []\n    for pattern in patterns:\n        matching_files.extend(fnmatch.filter(files, pattern))\n    \n    # Copy each matching file to the destination directory\n    for file in matching_files:\n        src_file = os.path.join(src_dir, file)\n        dst_file = os.path.join(dst_dir, file)\n        shutil.copy2(src_file, dst_file)\n    \n    # Return the destination directory\n    return dst_dir"}
{"task_id": "BigCodeBench/666", "solution": "from itertools import combinations\nimport math\ndef task_func(seq, letter_weight_dict):\n    # Generate all possible subsequences\n    subsequences = []\n    for r in range(1, len(seq) + 1):\n        subsequences.extend(combinations(seq, r))\n    \n    # Calculate the total weight for each subsequence\n    subseq_weights = []\n    for subseq in subsequences:\n        weight = sum(letter_weight_dict[char] for char in subseq)\n        subseq_weights.append((subseq, weight))\n    \n    # Find the subsequence with the maximum weight\n    max_weight = -math.inf\n    max_subseq = ()\n    for subseq, weight in subseq_weights:\n        if weight > max_weight:\n            max_weight = weight\n            max_subseq = subseq\n    \n    # Convert the subsequence tuple to a string\n    max_str = ''.join(max_subseq)\n    \n    return max_str"}
{"task_id": "BigCodeBench/667", "solution": "import heapq\nimport collections\ndef task_func(x, n):\n    # Create a frequency dictionary\n    freq = collections.Counter(x)\n    \n    # Use a heap to find the n most common letters\n    heap = [(-freq[letter], letter) for letter in freq]\n    heapq.heapify(heap)\n    \n    # Extract the n most common letters\n    most_common = [heapq.heappop(heap)[1] for _ in range(n)]\n    \n    return most_common"}
{"task_id": "BigCodeBench/668", "solution": "import itertools\nimport math\ndef task_func(x):\n    # Initialize variables\n    min_length = math.inf\n    min_subseq = []\n    \n    # Generate all possible subsequences\n    for r in range(1, len(x)+1):\n        for subseq in itertools.combinations(x.keys(), r):\n            # Calculate the total length of the sub-sequence\n            total_length = sum(x[key] for key in subseq)\n            # Update the minimum sub-sequence if the current one is smaller\n            if total_length < min_length:\n                min_length = total_length\n                min_subseq = list(subseq)\n    \n    return min_subseq"}
{"task_id": "BigCodeBench/669", "solution": "import itertools\nimport math\ndef task_func(x):\n    # Initialize variables to store the maximum sum and the corresponding key pair\n    max_sum = float('-inf')\n    max_pair = None\n\n    # Iterate over all possible pairs of keys in the dictionary\n    for pair in itertools.combinations(x.keys(), 2):\n        # Calculate the sum of the cosine of the values for the current pair\n        current_sum = math.cos(x[pair[0]]) + math.cos(x[pair[1]])\n\n        # If this sum is greater than the current maximum, update the maximum sum and the key pair\n        if current_sum > max_sum:\n            max_sum = current_sum\n            max_pair = pair\n\n    # Return the key pair with the highest sum of cosine values\n    return max_pair"}
{"task_id": "BigCodeBench/670", "solution": "from itertools import combinations\nimport math\ndef task_func(x, w):\n    # Initialize variables to store the maximum weight and the corresponding substring\n    max_weight = -math.inf\n    max_substr = \"\"\n    \n    # Iterate over all possible substrings of x\n    for i in range(len(x)):\n        for j in range(i+1, len(x)+1):\n            substr = x[i:j]\n            # Calculate the total weight of the substring\n            total_weight = sum(w[char] for char in substr)\n            # Update the maximum weight and substring if the current one is greater\n            if total_weight > max_weight:\n                max_weight = total_weight\n                max_substr = substr\n                \n    return max_substr"}
{"task_id": "BigCodeBench/671", "solution": "import os\nimport random\nimport json\ndef task_func(directory, n):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    for i in range(n):\n        file_name = os.path.join(directory, f\"file_{i}.json\")\n        with open(file_name, 'w') as f:\n            json.dump({'number': random.randint(1, 100)}, f)\n    return directory"}
{"task_id": "BigCodeBench/672", "solution": "import csv\nimport sys\ndef task_func(filename):\n    with open(filename, 'r') as file:\n        lines = file.readlines()\n    lines = lines[::-1]\n    with open(filename, 'w') as file:\n        file.writelines(lines)\n    with open(filename, 'r') as file:\n        print(file.read())"}
{"task_id": "BigCodeBench/673", "solution": "import os\nimport random\ndef task_func(directory, n_files):\n    for i in range(1, n_files + 1):\n        file_name = f\"file_{i}.txt\"\n        file_path = os.path.join(directory, file_name)\n        with open(file_path, 'w') as file:\n            digit = str(random.randint(0, 9))\n            file.write(digit)\n        with open(file_path, 'r+') as file:\n            file.seek(0)\n    return n_files"}
{"task_id": "BigCodeBench/674", "solution": "import pandas as pd\nimport os\ndef task_func(filename):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(filename)\n    \n    # Reverse the order of the lines\n    df = df.iloc[::-1]\n    \n    # Write the inverted lines back into the file\n    df.to_csv(filename, index=False)\n    \n    # Move the cursor back to the beginning of the file\n    os.system(f\"seek {filename} 0\")\n    \n    # Output the filename\n    print(filename)"}
{"task_id": "BigCodeBench/675", "solution": "import os\nimport random\nimport string\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    for i in range(n_files):\n        file_name = os.path.join(directory, f\"file_{i}.txt\")\n        with open(file_name, 'w') as file:\n            random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n            file.write(random_string)\n    return directory"}
{"task_id": "BigCodeBench/676", "solution": "import pandas as pd\nimport random\ndef task_func(df):\n    df['winner'] = df.apply(lambda row: random.choice([row['team1'], row['team2']]) if row['score1'] == row['score2'] else (row['team1'] if row['score1'] > row['score2'] else row['team2']), axis=1)\n    return df"}
{"task_id": "BigCodeBench/677", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\ndef task_func(df):\n    \"\"\"\n    Perform a linear regression on two variables in a DataFrame and add a 'predicted' column.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame containing the two variables to be analyzed.\n\n    Returns:\n    df (pandas.DataFrame): The DataFrame with an additional 'predicted' column containing the predicted values based on the linear regression.\n    \"\"\"\n    # Extract the two variables for regression\n    x = df.iloc[:, 0]\n    y = df.iloc[:, 1]\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n\n    # Calculate predicted values\n    predicted = slope * x + intercept\n\n    # Add the predicted column to the DataFrame\n    df['predicted'] = predicted\n\n    return df"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Create a list to hold all the dataframes\n    dfs = []\n    \n    # List all files in the directory\n    files = os.listdir(path)\n    \n    # Filter only JSON files\n    json_files = [f for f in files if f.endswith('.json')]\n    \n    # Sort the JSON files alphabetically\n    json_files.sort()\n    \n    # Process each JSON file\n    for file in json_files:\n        # Read the JSON file\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n        \n        # Convert the JSON data to a DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add a \"Source\" column with the filename\n        df['Source'] = file\n        \n        # Append the DataFrame to the list\n        dfs.append(df)\n    \n    # Concatenate all DataFrames into one\n    df = pd.concat(dfs, ignore_index=True)\n    \n    # Create a \"processed\" subdirectory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n    \n    # Move each JSON file to the \"processed\" subdirectory\n    for file in json_files:\n        shutil.move(os.path.join(path, file), os.path.join(processed_path, file))\n    \n    return df"}
{"task_id": "BigCodeBench/679", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(df):\n    # Create a list of combinations for each row\n    df['combination'] = df.apply(lambda row: tuple(row), axis=1)\n    \n    # Count the frequency of each combination\n    combination_counts = Counter(df['combination'])\n    \n    # Return the frequency dictionary\n    return dict(combination_counts)\ndata = {\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/680", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, features):\n    \"\"\"\n    Standardize the specified features in a DataFrame using StandardScaler from scikit-learn.\n\n    Parameters:\n    - df (pandas.DataFrame): The input DataFrame containing the data to be standardized.\n    - features (list of str): The list of column names to be standardized.\n\n    Returns:\n    - pandas.DataFrame: The DataFrame with the specified features standardized.\n    \"\"\"\n    scaler = StandardScaler()\n    df[features] = scaler.fit_transform(df[features])\n    return df"}
{"task_id": "BigCodeBench/681", "solution": "import pandas as pd\nimport json\ndef task_func(file_path, key):\n    # Load the JSON file into a list of dictionaries\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    # Remove the specified key from each dictionary in the list\n    for item in data:\n        if key in item:\n            del item[key]\n    \n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Write the DataFrame back into a JSON file oriented by records\n    df.to_json('processed_data.json', orient='records')\n    \n    return df"}
{"task_id": "BigCodeBench/682", "solution": "from collections import Counter\nimport math\ndef task_func(nested_dict):\n    # Initialize a Counter to aggregate values\n    aggregated = Counter()\n    \n    # Iterate through the nested dictionary\n    for key, value in nested_dict.items():\n        # Aggregate the values of the same keys\n        aggregated[key] += value\n    \n    # Remove the \"ele\" key if it exists\n    if \"ele\" in aggregated:\n        del aggregated[\"ele\"]\n    \n    # Take the sine of each remaining key\n    result = {key: math.sin(value) for key, value in aggregated.items()}\n    \n    return result"}
{"task_id": "BigCodeBench/683", "solution": "import math\nimport yaml\ndef task_func(yaml_path, key):\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n    \n    if key in data:\n        data[key] = math.cos(data[key])\n    \n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n    \n    return data"}
{"task_id": "BigCodeBench/684", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, col):\n    # Remove the specified column\n    df = df.drop(columns=col)\n    \n    # Add 'IsEvenIndex' column\n    df['IsEvenIndex'] = df.index % 2 == 0\n    \n    return df"}
{"task_id": "BigCodeBench/685", "solution": "from collections import Counter\nfrom itertools import chain\ndef task_func(list_of_lists):\n    merged_list = list(chain.from_iterable(list_of_lists))\n    counter = Counter(merged_list)\n    return counter"}
{"task_id": "BigCodeBench/686", "solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\ndef task_func(list_of_lists):\n    # Merge the list of lists into a single list\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Initialize the OneHotEncoder\n    encoder = OneHotEncoder(sparse=False)\n    \n    # Reshape the merged list to fit the encoder\n    merged_list_reshaped = np.array(merged_list).reshape(-1, 1)\n    \n    # Fit and transform the merged list\n    one_hot = encoder.fit_transform(merged_list_reshaped)\n    \n    return one_hot"}
{"task_id": "BigCodeBench/687", "solution": "import numpy as np\nfrom scipy.stats import mode\ndef task_func(list_of_lists):\n    # Merge the lists into a single list\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Convert the merged list to a numpy array\n    merged_array = np.array(merged_list)\n    \n    # Find the mode using scipy's mode function\n    mode_result = mode(merged_array)\n    \n    # Extract the mode value and its count\n    mode_value = mode_result.mode[0]\n    mode_count = mode_result.count[0]\n    \n    # Return the results as specified\n    return (mode_value, mode_count), mode_value, mode_count"}
{"task_id": "BigCodeBench/688", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized"}
{"task_id": "BigCodeBench/689", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(df):\n    p_values = {}\n    for column in df.columns:\n        p_value = stats.shapiro(df[column])[1]\n        p_values[column] = p_value\n    return p_values"}
{"task_id": "BigCodeBench/690", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    model = LinearRegression()\n    model.fit(df[['X']], df['Y'])\n    return model"}
{"task_id": "BigCodeBench/691", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Standardize the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n    \n    # Run KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(scaled_data)\n    \n    return labels"}
{"task_id": "BigCodeBench/692", "solution": "import math\nimport pandas as pd\ndef task_func(tuples_list):\n    # Create a list of dictionaries, each dictionary representing a row in the DataFrame\n    data = []\n    for tup in tuples_list:\n        row = {}\n        for idx, val in enumerate(tup):\n            row[f'col{idx}'] = math.sin(val)\n        data.append(row)\n    \n    # Create the DataFrame\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/693", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(tuples_list, columns):\n    df = pd.DataFrame(tuples_list, columns=columns)\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n    return df_scaled"}
{"task_id": "BigCodeBench/694", "solution": "import itertools\nimport random\ndef task_func(t, n):\n    # Generate all combinations from the tuple with length n\n    combinations = list(itertools.combinations(t, n))\n    \n    # Return a random combination of length n\n    if combinations:\n        return random.choice(combinations)\n    else:\n        return None  # or raise an exception, depending on requirements"}
{"task_id": "BigCodeBench/695", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(tuples_list, n_components):\n    # Convert the list of tuples to a numpy array\n    data = np.array(tuples_list)\n    \n    # Initialize PCA with the specified number of components\n    pca = PCA(n_components=n_components)\n    \n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n    \n    return transformed_data"}
{"task_id": "BigCodeBench/696", "solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\ndef task_func(radius, num_points):\n    points = []\n    for _ in range(num_points):\n        angle = uniform(0, 2 * math.pi)\n        distance = uniform(0, radius)\n        x = distance * math.cos(angle)\n        y = distance * math.sin(angle)\n        points.append((x, y))\n    return points"}
{"task_id": "BigCodeBench/697", "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Extract the feature and target variables\n    X = df[['feature']]\n    y = df['value']\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    \n    # Fit the model to the data\n    model.fit(X, y)\n    \n    # Extract the coefficients and intercept\n    coefficients = model.coef_[0]\n    intercept = model.intercept_\n    \n    # Return the results in a dictionary\n    result = {\n        'coefficients': coefficients,\n        'intercept': intercept\n    }\n    \n    return result"}
{"task_id": "BigCodeBench/698", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df):\n    # Separate the target column\n    y = df['target']\n    X = df.drop('target', axis=1)\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    return X_train, X_test, y_train, y_test"}
{"task_id": "BigCodeBench/699", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    # Create a DataFrame from the given lists\n    data = pd.DataFrame({'x': x_list, 'y': y_list})\n    \n    # Initialize and fit the KMeans model\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(data)\n    \n    # Extract labels and centroids\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n    \n    return labels, centroids"}
{"task_id": "BigCodeBench/700", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, cols):\n    # Convert the provided data into a DataFrame\n    df = pd.DataFrame(data, columns=cols)\n    \n    # Calculate the correlation matrix of numeric columns\n    correlation_matrix = df.corr()\n    \n    # Return the correlation matrix\n    return correlation_matrix"}
{"task_id": "BigCodeBench/701", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\ndef task_func(df, target):\n    # Split the data into features and target\n    X = df.drop(target, axis=1)\n    y = df[target]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Create a Linear Regression model\n    model = LinearRegression()\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate the R-squared score\n    score = r2_score(y_test, y_pred)\n    \n    return score"}
{"task_id": "BigCodeBench/702", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(df):\n    # Perform PCA on the DataFrame\n    pca = PCA(n_components=2)\n    pca.fit(df)\n    # Transform the DataFrame to get the first two principal components\n    df_pca = pd.DataFrame(pca.transform(df), columns=['PC1', 'PC2'])\n    return df_pca"}
{"task_id": "BigCodeBench/703", "solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\ndef task_func(data, cols):\n    df = pd.DataFrame(data, columns=cols)\n    X = df[cols]\n    db = DBSCAN(eps=3, min_samples=2).fit(X)\n    df['Cluster'] = db.labels_\n    return df"}
{"task_id": "BigCodeBench/704", "solution": "import pandas as pd\nfrom itertools import combinations\ndef task_func(data, cols, percentage):\n    \"\"\"\n    Find all combinations of columns from a given DataFrame so that the absolute correlation between them is greater than a certain threshold.\n\n    Parameters:\n    - data (pd.DataFrame): The input DataFrame.\n    - cols (list): List of column names to consider for correlation.\n    - percentage (float): The minimum percentage threshold for correlation.\n\n    Returns:\n    - corr_combinations (list): A list of tuples where each tuple contains two column names with correlation above the threshold.\n    \"\"\"\n    corr_combinations = []\n    # Calculate the correlation matrix for the specified columns\n    corr_matrix = data[cols].corr()\n    # Iterate over all unique pairs of columns\n    for col1, col2 in combinations(cols, 2):\n        # Calculate the absolute correlation between the pair\n        corr = abs(corr_matrix.loc[col1, col2])\n        # If the correlation is greater than the threshold, add the pair to the list\n        if corr > percentage:\n            corr_combinations.append((col1, col2))\n    return corr_combinations"}
{"task_id": "BigCodeBench/705", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(df, column, alpha):\n    \"\"\"\n    Test the normality of a particular numeric column from a DataFrame using the Shapiro-Wilk test.\n\n    Parameters:\n    - df: pandas DataFrame containing the data.\n    - column: string, the name of the numeric column to test for normality.\n    - alpha: float, the significance level to use for the test.\n\n    Returns:\n    - bool: True if the column passes the normality test, False otherwise.\n    \"\"\"\n    # Extract the column data\n    data = df[column]\n    \n    # Perform the Shapiro-Wilk test\n    stat, p_value = stats.shapiro(data)\n    \n    # Determine if the column passes the normality test\n    if p_value > alpha:\n        return True\n    else:\n        return False"}
{"task_id": "BigCodeBench/706", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\ndef task_func(data, columns, target_column):\n    # Split the data into features and target\n    X = data[columns]\n    y = data[target_column]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy"}
{"task_id": "BigCodeBench/707", "solution": "import json\nimport numpy as np\nimport pandas as pd\ndef task_func(df):\n    # Transform the 'IntCol' column by a logarithm (base 10)\n    df['IntCol'] = np.log10(df['IntCol'])\n    \n    # Write the transformed 'IntCol' to a 'IntCol.json' file as a list\n    with open('IntCol.json', 'w') as f:\n        json.dump(df['IntCol'].tolist(), f)\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/708", "solution": "import json\nimport csv\nimport os\nimport base64\ndef task_func(raw_string, filename, output_dir):\n    # Decode the base64-encoded JSON string\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_string = decoded_bytes.decode('utf-8')\n    data = json.loads(decoded_string)\n    \n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Construct the file path\n    file_path = os.path.join(output_dir, filename)\n    \n    # Write the data to a CSV file\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    return file_path"}
{"task_id": "BigCodeBench/709", "solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\ndef task_func(raw_string, line_length):\n    # Decode the raw string from base64\n    decoded_bytes = base64.b64decode(raw_string)\n    decoded_string = decoded_bytes.decode('utf-8')\n    \n    # Decouple HTML entities\n    cleaned_string = unescape(decoded_string)\n    \n    # Replace multiple spaces with a single space\n    single_space_string = re.sub(r'\\s+', ' ', cleaned_string)\n    \n    # Strip leading and trailing spaces\n    stripped_string = single_space_string.strip()\n    \n    # Wrap text to a certain line length\n    wrapped_text = textwrap.fill(stripped_string, width=line_length)\n    \n    return wrapped_text"}
{"task_id": "BigCodeBench/710", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_path):\n    df = pd.read_csv(data_path)\n    scaler = MinMaxScaler()\n    df_scaled = scaler.fit_transform(df)\n    df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n    return df_scaled"}
{"task_id": "BigCodeBench/711", "solution": "import json\nimport csv\nimport os\ndef task_func(json_file, csv_file):\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n    \n    return csv_file"}
{"task_id": "BigCodeBench/712", "solution": "import os\nimport shutil\nimport glob\ndef task_func(source_dir, dest_dir, extension):\n    # Create the destination directory if it doesn't exist\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # Find all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(source_dir, f'*.{extension}'))\n    \n    # Move each file to the destination directory\n    moved_count = 0\n    for file in files:\n        try:\n            shutil.move(file, dest_dir)\n            moved_count += 1\n        except Exception as e:\n            print(f\"Error moving {file}: {e}\")\n    \n    return moved_count"}
{"task_id": "BigCodeBench/713", "solution": "import os\nimport re\ndef task_func(log_file_path: str, keywords: list):\n    formatted_lines = []\n    with open(log_file_path, 'r') as file:\n        for line in file:\n            for keyword in keywords:\n                if keyword in line:\n                    # Extract timestamp and message\n                    match = re.search(r'\\[(.*?)\\]', line)\n                    if match:\n                        timestamp = match.group(1)\n                        message = line.replace(match.group(0), '').strip()\n                        formatted_line = f\"{keyword:20}{timestamp:20}{message}\"\n                        formatted_lines.append(formatted_line)\n                    break  # No need to check other keywords if one is found\n    return formatted_lines"}
{"task_id": "BigCodeBench/714", "solution": "import sys\nfrom pathlib import Path\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(path_to_append=PATH_TO_APPEND):\n    # Add the specific path to sys.path\n    sys.path.append(path_to_append)\n    \n    # Create the directory if it does not exist\n    path = Path(path_to_append)\n    if not path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n    \n    # Output the path that was appended and where the directory was created\n    print(f\"Path appended: {path_to_append}\")\n    print(f\"Directory created: {path_to_append}\")"}
{"task_id": "BigCodeBench/715", "solution": "import sys\nimport subprocess\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    # Switch to the specified Python version\n    try:\n        subprocess.run(['py', '-{}'.format(python_version), '-c', 'import sys; print(sys.version)'], check=True)\n    except subprocess.CalledProcessError as e:\n        print(\"Error switching Python version:\", e)\n        return\n    # Add the specified path to sys.path\n    sys.path.append(path_to_append)\n    # Output the Python version that was switched to\n    print(\"Python version switched to:\", python_version)\n    # Output the updated sys.path\n    print(\"Updated sys.path:\", sys.path)"}
{"task_id": "BigCodeBench/716", "solution": "import sys\nimport json\nfrom datetime import datetime\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n    \n    # Get the current date and time\n    current_datetime = datetime.now().isoformat()\n    \n    # Read the existing JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n    \n    # Update the 'last_updated' key with the current date and time\n    json_data['last_updated'] = current_datetime\n    \n    # Write the updated JSON data back to the file\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file, indent=4)\n    \n    # Return the updated JSON data\n    return json_data"}
{"task_id": "BigCodeBench/717", "solution": "import sys\nfrom configparser import ConfigParser\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    # Add the specific path to sys.path\n    sys.path.append(path_to_append)\n    \n    # Read the existing configuration file\n    config = ConfigParser()\n    config.read(config_file)\n    \n    # Update the configuration with the new path\n    if 'paths' not in config:\n        config['paths'] = {}\n    config['paths']['new_path'] = path_to_append\n    \n    # Write the updated configuration back to the file\n    with open(config_file, 'w') as f:\n        config.write(f)\n    \n    # Return the updated configuration and the path to the config file\n    return config, config_file"}
{"task_id": "BigCodeBench/718", "solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\ndef task_func(text1, text2):\n    # Split the strings into words using regular expressions\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n    \n    # Check if the number of words in both strings is the same\n    if len(words1) != len(words2):\n        return np.nan, np.nan\n    \n    # Perform the paired t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n    \n    return t_statistic, p_value"}
{"task_id": "BigCodeBench/719", "solution": "import re\nimport os\nimport glob\ndef task_func(directory, word):\n    count = 0\n    for file in glob.glob(os.path.join(directory, '*')):\n        if os.path.isfile(file):\n            with open(file, 'r') as f:\n                content = f.read()\n                if re.search(word, content):\n                    count += 1\n    return count"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the file path\n    file_path = \"task_func_data/Output.txt\"\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    \n    # Open the file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Generate random temperature and humidity data\n        temperature = random.uniform(20, 30)\n        humidity = random.uniform(30, 60)\n        \n        # Get current date and time\n        now = datetime.now()\n        date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n        \n        # Write the data to the CSV file\n        writer.writerow([date_time, temperature, humidity])\n    \n    # Return the path to the CSV file\n    return file_path"}
{"task_id": "BigCodeBench/721", "solution": "import os\nimport csv\nfrom collections import Counter\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = []\n        for row in reader:\n            words.extend(row)\n        if not words:\n            return None\n        word_counts = Counter(words)\n        most_common = word_counts.most_common(1)\n        if most_common:\n            return most_common[0]\n        else:\n            return None"}
{"task_id": "BigCodeBench/722", "solution": "import urllib.request\nimport os\nimport re\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\ndef task_func(url):\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n    \n    # Read the file\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n    \n    # Search for the pattern\n    matches = re.findall(SEARCH_PATTERN, content)\n    \n    # Return the number of occurrences\n    return len(matches)"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n    # Read the HTML content\n    html_content = response.read()\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html_content, 'html.parser')\n    # Find all the paragraphs in the HTML\n    paragraphs = soup.find_all('p')\n    # Open a CSV file to write the scraped data\n    with open(CSV_FILE_PATH, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Paragraph'])\n        # Write each paragraph to the CSV file\n        for paragraph in paragraphs:\n            writer.writerow([paragraph.get_text()])\n    # Return the path of the CSV file\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/724", "solution": "import os\nimport json\ndef task_func(config_path: str) -> dict:\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n    with open(config_path, 'r') as file:\n        config = json.load(file)\n    return config"}
{"task_id": "BigCodeBench/725", "solution": "import codecs\nimport os\nimport glob\nDIRECTORY_PATH = './files/'\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    # List all files in the directory\n    files = glob.glob(os.path.join(directory, '*'))\n    \n    # Iterate over each file\n    for file in files:\n        # Check if the file is a text file\n        if os.path.isfile(file):\n            # Open the file in read mode with the original encoding\n            with codecs.open(file, 'r', encoding=from_encoding) as f:\n                content = f.read()\n            \n            # Open the file in write mode with the new encoding\n            with codecs.open(file, 'w', encoding=to_encoding) as f:\n                f.write(content)"}
{"task_id": "BigCodeBench/726", "solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\nimport nltk\nSAMPLE_ENGLISH_WORDS = set(words.words())\ndef task_func(s, n):\n    # Split the string into words, ignoring case\n    words_list = re.findall(r'\\b\\w+\\b', s.lower())\n    # Retain only English words\n    english_words = [word for word in words_list if word in SAMPLE_ENGLISH_WORDS]\n    # Get unique words\n    unique_words = list(set(english_words))\n    # If there are fewer than n unique words, return all of them\n    if len(unique_words) < n:\n        return unique_words\n    else:\n        # Otherwise, return a random sample of n unique words\n        return sample(unique_words, n)"}
{"task_id": "BigCodeBench/727", "solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\ndef task_func(s: str) -> np.ndarray:\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the sentences\n    X = vectorizer.fit_transform(SENTENCES)\n    \n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Split the input string into words\n    words = re.findall(r'\\b\\w+\\b', s)\n    \n    # Initialize a list to hold the counts\n    counts = []\n    \n    # For each word in the input string\n    for word in words:\n        # If the word is in the feature names, get its count\n        if word in feature_names:\n            # Get the index of the word\n            idx = np.where(feature_names == word)[0][0]\n            # Get the count from the vectorized data\n            count = X.toarray()[:, idx].sum()\n            counts.append(count)\n        else:\n            counts.append(0)\n    \n    # Convert counts to a numpy array\n    counts_array = np.array(counts)\n    \n    return counts_array"}
{"task_id": "BigCodeBench/728", "solution": "import csv\nimport io\ndef task_func(filename='sample.csv', from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    with open(filename, 'r', encoding=from_encoding) as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        data = [row for row in reader]\n    with io.StringIO() as output:\n        writer = csv.DictWriter(output, fieldnames=reader.fieldnames, delimiter=delimiter)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n        converted_csv = output.getvalue()\n    return (data, converted_csv)"}
{"task_id": "BigCodeBench/729", "solution": "import pickle\nimport os\nimport random\nimport string\ndef task_func(strings, filename=None):\n    if filename is None:\n        filename = ''.join(random.choices(string.ascii_letters + string.digits, k=10)) + '.pkl'\n    with open(filename, 'wb') as f:\n        pickle.dump(strings, f)\n    with open(filename, 'rb') as f:\n        loaded_strings = pickle.load(f)\n    return loaded_strings"}
{"task_id": "BigCodeBench/730", "solution": "import pickle\nimport os\nfrom datetime import datetime\nFILE_NAME = 'save.pkl'\ndef task_func(dt):\n    # Save the datetime object to a pickle file\n    with open(FILE_NAME, 'wb') as f:\n        pickle.dump(dt, f)\n    \n    # Read the datetime object back from the pickle file\n    with open(FILE_NAME, 'rb') as f:\n        loaded_dt = pickle.load(f)\n    \n    # Remove the pickle file\n    os.remove(FILE_NAME)\n    \n    # Return the loaded datetime object\n    return loaded_dt"}
{"task_id": "BigCodeBench/731", "solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\nFILE_NAME = 'save.pkl'\ndef task_func(data, target):\n    # Save the data and target to a pickle file\n    with open(FILE_NAME, 'wb') as f:\n        pickle.dump((data, target), f)\n    \n    # Read the data and target from the pickle file\n    with open(FILE_NAME, 'rb') as f:\n        loaded_data, loaded_target = pickle.load(f)\n    \n    # Return the loaded data and target\n    return (loaded_data, loaded_target)"}
{"task_id": "BigCodeBench/732", "solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nSTEMMER = PorterStemmer()\ndef task_func(content):\n    # Remove punctuation and split the sentence into words\n    translator = str.maketrans('', '', string.punctuation)\n    words = re.sub(r'\\s+', ' ', content.translate(translator)).split()\n    \n    # Stem all words except the last one\n    stemmed_words = [STEMMER.stem(word) for word in words[:-1]]\n    last_word = words[-1]\n    \n    # Count the frequency of each stem\n    stem_freq = Counter(stemmed_words)\n    \n    # Add the last word as is\n    stem_freq[last_word] = stem_freq.get(last_word, 0) + 1\n    \n    return dict(stem_freq)"}
{"task_id": "BigCodeBench/733", "solution": "import re\nimport string\ndef task_func(content):\n    # Define a list of stop words\n    stop_words = set([\n        'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'of',\n        'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n        'through', 'during', 'before', 'after', 'above', 'below', 'to',\n        'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n        'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n        'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n        'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so',\n        'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should',\n        'now'\n    ])\n\n    # Remove punctuation and split the sentence into words\n    translator = str.maketrans('', '', string.punctuation)\n    words = re.sub(r'\\s+', ' ', content.translate(translator)).split()\n\n    # Count the non-stop words excluding the last word\n    count = 0\n    for word in words[:-1]:\n        if word.lower() not in stop_words:\n            count += 1\n\n    return count"}
{"task_id": "BigCodeBench/734", "solution": "import nltk\nfrom collections import Counter\ndef task_func(content):\n    # Tokenize the sentence into words\n    words = nltk.word_tokenize(content)\n    # Remove the last word\n    words = words[:-1]\n    # Tag the POS of each word\n    pos_tags = nltk.pos_tag(words)\n    # Extract the POS tags and count them\n    pos_counts = Counter(tag for word, tag in pos_tags)\n    return dict(pos_counts)"}
{"task_id": "BigCodeBench/735", "solution": "import numpy as np\nfrom itertools import chain\ndef task_func(L):\n    # Flatten the nested list\n    flat_list = list(chain.from_iterable(L))\n    \n    # Calculate mean and variance\n    mean = np.mean(flat_list)\n    variance = np.var(flat_list)\n    \n    # Return the results in a dictionary\n    return {'mean': mean, 'variance': variance}"}
{"task_id": "BigCodeBench/736", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(L):\n    # Flatten the nested list into a single list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the mode using scipy.stats\n    mode = stats.mode(flat_list)\n    \n    # Return the mode\n    return mode"}
{"task_id": "BigCodeBench/737", "solution": "import numpy as np\nimport math\ndef task_func(L):\n    # Flatten the nested list into a single list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Sort the flattened list\n    sorted_list = sorted(flat_list)\n    \n    # Calculate the median\n    n = len(sorted_list)\n    if n % 2 == 1:\n        # If odd, return the middle element\n        median = sorted_list[n // 2]\n    else:\n        # If even, return the average of the two middle elements\n        mid1 = sorted_list[(n // 2) - 1]\n        mid2 = sorted_list[n // 2]\n        median = (mid1 + mid2) / 2\n    \n    return median"}
{"task_id": "BigCodeBench/738", "solution": "import numpy as np\nfrom scipy.stats import iqr\ndef task_func(L):\n    # Flatten the nested list into a single list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n    \n    return iqr_value"}
{"task_id": "BigCodeBench/739", "solution": "import struct\nimport random\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_key=None):\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n    # Convert hex to float\n    float_num = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n    # Round the float to 2 decimal places\n    rounded_float = round(float_num, 2)\n    return rounded_float"}
{"task_id": "BigCodeBench/740", "solution": "from collections import Counter\nimport heapq\ndef task_func(my_dict):\n    # Create a counter object from the dictionary\n    counter = Counter(my_dict)\n    # Use heapq.nlargest to find the 3 most common letters\n    most_common = heapq.nlargest(3, counter.items(), key=lambda x: x[1])\n    # Extract the letters from the tuples\n    most_common_letters = [letter for letter, count in most_common]\n    return most_common_letters"}
{"task_id": "BigCodeBench/741", "solution": "from itertools import groupby\nfrom operator import itemgetter\nKEY_FUNC = itemgetter(0)\ndef task_func(my_dict):\n    # Sort the dictionary items by the first character of the key\n    sorted_items = sorted(my_dict.items(), key=KEY_FUNC)\n    # Group the sorted items by the first character of the key\n    grouped = groupby(sorted_items, key=KEY_FUNC)\n    # Aggregate the values for each group\n    aggregated_dict = {}\n    for key, group in grouped:\n        # Extract the values from the group\n        values = [item[1] for item in group]\n        # Sum the values if they are numbers, otherwise concatenate them\n        if all(isinstance(val, (int, float)) for val in values):\n            aggregated_dict[key] = sum(values)\n        else:\n            aggregated_dict[key] = ''.join(values)\n    return aggregated_dict"}
{"task_id": "BigCodeBench/742", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input array is empty\")\n    for pair in list_of_pairs:\n        if not all(isinstance(x, (int, float)) for x in pair):\n            raise ValueError(\"Values are not numeric\")\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df[['Value']])\n    return df"}
{"task_id": "BigCodeBench/743", "solution": "import json\nimport os\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\ndef task_func(directory):\n    # Initialize a dictionary to store the counts\n    counts = {prefix: 0 for prefix in PREFIXES}\n    \n    # List all files in the directory\n    files = os.listdir(directory)\n    \n    # Iterate over each file\n    for file in files:\n        # Check if the file is a JSON file\n        if file.endswith('.json'):\n            # Open and read the JSON file\n            with open(os.path.join(directory, file), 'r') as f:\n                data = json.load(f)\n            \n            # Iterate over each key in the JSON data\n            for key in data.keys():\n                # Check if the key starts with any of the prefixes\n                for prefix in PREFIXES:\n                    if key.startswith(prefix):\n                        # Increment the count for that prefix\n                        counts[prefix] += 1\n                        # No need to check other prefixes once a match is found\n                        break\n    \n    return counts"}
{"task_id": "BigCodeBench/744", "solution": "import nltk\nfrom string import punctuation\nimport pandas as pd\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input must be a string\")\n    \n    # Tokenize the text into words\n    words = nltk.word_tokenize(text)\n    \n    # Filter words that start with '$' and are not entirely made up of punctuation\n    filtered_words = [word for word in words if word.startswith('$') and not all(char in punctuation for char in word)]\n    \n    # Count the frequency of each filtered word\n    frequency = {}\n    for word in filtered_words:\n        if word in frequency:\n            frequency[word] += 1\n        else:\n            frequency[word] = 1\n    \n    # Create a DataFrame from the frequency dictionary\n    df = pd.DataFrame(list(frequency.items()), columns=[\"Word\", \"Frequency\"])\n    \n    return df"}
{"task_id": "BigCodeBench/745", "solution": "import subprocess\nimport random\nimport os\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'\ndef task_func():\n    # Select a random script from the list\n    script_name = random.choice(SCRIPTS)\n    script_path = os.path.join(SCRIPTS_DIR, script_name)\n    \n    # Check if the script exists\n    if not os.path.exists(script_path):\n        print(f\"Error: The script {script_name} does not exist in {SCRIPTS_DIR}\")\n        return\n    \n    # Run the script using subprocess\n    try:\n        subprocess.run([script_path], check=True)\n        print(f\"Script executed successfully: {script_path}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error executing script {script_path}: {e}\")\n    \n    # Return the full path of the executed script\n    return script_path"}
{"task_id": "BigCodeBench/746", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df must be a DataFrame\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column must be a string\")\n    if target_values is not None and not hasattr(target_values, '__iter__'):\n        raise ValueError(\"target_values must be an array-like object\")\n    \n    if target_values is not None:\n        for col in df.columns:\n            if col != target_column:\n                df[col] = df[col].replace(set(df[col]) - set(target_values), 0)\n    \n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    model = LinearRegression()\n    model.fit(X, y)\n    \n    return model"}
{"task_id": "BigCodeBench/747", "solution": "import re\nimport math\ndef task_func(s):\n    # Split the string by commas\n    elements = s.split(',')\n    \n    # Initialize counters and sum\n    count = 0\n    sqrt_sum = 0.0\n    \n    # Regular expression to match integers and floats\n    pattern = re.compile(r'^-?\\d+$|^-?\\d*\\.\\d+$')\n    \n    for element in elements:\n        # Strip whitespace\n        element = element.strip()\n        \n        # Check if the element is an integer or float\n        if pattern.match(element):\n            # Convert to float\n            num = float(element)\n            # Increment count\n            count += 1\n            # Add square root to sum\n            sqrt_sum += math.sqrt(num)\n    \n    return {\n        'count': count,\n        'sqrt_sum': sqrt_sum\n    }"}
{"task_id": "BigCodeBench/748", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, age, weight):\n    # Check if 'Age' and 'Weight' columns are present in the DataFrame\n    required_columns = ['Age', 'Weight']\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise KeyError(f\"The DataFrame is missing the required columns: {', '.join(missing_columns)}\")\n    \n    # Filter the DataFrame based on the age and weight criteria\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n    \n    # Standardize the numerical values in the filtered DataFrame\n    scaler = StandardScaler()\n    numerical_cols = filtered_df.select_dtypes(include=['float64', 'int64']).columns\n    filtered_df[numerical_cols] = scaler.fit_transform(filtered_df[numerical_cols])\n    \n    return filtered_df"}
{"task_id": "BigCodeBench/749", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\ndef task_func(myList):\n    scaler = MinMaxScaler()\n    normalized = scaler.fit_transform(np.array(myList).reshape(-1,1))\n    return normalized.flatten()"}
{"task_id": "BigCodeBench/750", "solution": "import pandas as pd\nimport statsmodels.api as sm\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty:\n        return None\n    subset = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n    if subset.empty:\n        return None\n    y = subset[columns[0]]\n    X = subset[columns[1:]]\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X).fit()\n    return model"}
{"task_id": "BigCodeBench/751", "solution": "import random\nfrom collections import Counter\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    Sample random numbers based on a given weighted distribution and return a histogram of the samples.\n\n    Parameters:\n    values (list): A list of values to sample from.\n    weights (list): A list of weights corresponding to the values.\n    n_samples (int): The number of samples to draw.\n\n    Returns:\n    dict: A histogram as a dictionary with the values as keys and counts as values.\n    \"\"\"\n    # Ensure that the lengths of values and weights match\n    if len(values) != len(weights):\n        raise ValueError(\"The length of values and weights must be the same.\")\n    \n    # Create a list of tuples (value, weight)\n    weighted_values = list(zip(values, weights))\n    \n    # Sample n_samples from the weighted distribution\n    samples = random.choices(values, weights=weights, k=n_samples)\n    \n    # Create a histogram using Counter\n    histogram = Counter(samples)\n    \n    return dict(histogram)"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a DataFrame\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"data is empty\")\n    \n    # Check if target_column is in data\n    if target_column not in data.columns:\n        raise ValueError(f\"target_column '{target_column}' is not a column of data\")\n    \n    # Check if data contains only numeric values\n    if not data.select_dtypes(include=[np.number]).columns.equals(data.columns):\n        raise ValueError(\"data contains non-numeric values\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state must be an integer\")\n    \n    # Check if test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"test_size must be between 0 and 1\")\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(data.drop(target_column, axis=1), data[target_column], test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model score on the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/753", "solution": "import math\nimport random\nimport statistics\nRADIUS = 5\ndef task_func(n):\n    points = []\n    for _ in range(n):\n        x = random.uniform(-RADIUS, RADIUS)\n        y = random.uniform(-RADIUS, RADIUS)\n        distance = math.sqrt(x**2 + y**2)\n        if distance <= RADIUS:\n            points.append(distance)\n    average_distance = statistics.mean(points)\n    return average_distance"}
{"task_id": "BigCodeBench/754", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    # Check if 'from_user' key exists in the result dictionary\n    if 'from_user' not in result:\n        raise ValueError(\"The 'from_user' key is missing in the result dictionary.\")\n    \n    # Extract 'from_user' values\n    from_user_values = result.get('from_user', [])\n    \n    # Check if 'from_user' values are numeric\n    if not all(isinstance(val, (int, float)) for val in from_user_values):\n        raise ValueError(\"All 'from_user' values must be numeric.\")\n    \n    # Calculate statistics\n    mean = np.mean(from_user_values)\n    median = np.median(from_user_values)\n    min_val = np.min(from_user_values)\n    max_val = np.max(from_user_values)\n    std = np.std(from_user_values)\n    \n    # Get current date and time\n    current_time = datetime.now().strftime(DATE_FORMAT)\n    \n    # Create a pandas Series with the summary\n    summary = pd.Series({\n        'mean': mean,\n        'median': median,\n        'min': min_val,\n        'max': max_val,\n        'std': std,\n        'current_time': current_time\n    })\n    \n    return summary"}
{"task_id": "BigCodeBench/755", "solution": "import os\nimport glob\ndef task_func(directory_path):\n    # Get all files in the directory\n    files = glob.glob(os.path.join(directory_path, '*'))\n    \n    new_filenames = []\n    for file in files:\n        # Extract the filename without the path\n        filename = os.path.basename(file)\n        \n        # Split the filename by periods\n        parts = filename.split('.')\n        \n        # Reverse the order of words\n        reversed_parts = parts[::-1]\n        \n        # Join the reversed parts back into a filename\n        new_filename = '.'.join(reversed_parts)\n        \n        # Construct the new file path\n        new_file_path = os.path.join(directory_path, new_filename)\n        \n        # Rename the file\n        os.rename(file, new_file_path)\n        \n        # Append the new filename to the list\n        new_filenames.append(new_filename)\n    \n    return new_filenames"}
{"task_id": "BigCodeBench/756", "solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    source = Path(source_dir)\n    target = Path(target_dir)\n    \n    if not source.exists():\n        raise ValueError(\"Source directory does not exist.\")\n    if not target.exists():\n        raise ValueError(\"Target directory does not exist.\")\n    \n    moved_files = 0\n    for file in source.iterdir():\n        if file.suffix in extensions:\n            shutil.move(file, target)\n            moved_files += 1\n    return moved_files"}
{"task_id": "BigCodeBench/757", "solution": "import numpy as np\nimport datetime\ndef task_func(arr):\n    # Convert the numpy array to a list of strings\n    str_list = arr.tolist()\n    \n    # Reverse each string in the list\n    reversed_list = [s[::-1] for s in str_list]\n    \n    # Convert the list back to a numpy array\n    reversed_arr = np.array(reversed_list)\n    \n    return reversed_arr"}
{"task_id": "BigCodeBench/758", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n              ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples must be an integer\")\n    \n    rng = np.random.default_rng(rng_seed)\n    \n    country = rng.choice(countries, size=num_samples)\n    age = rng.choice(ages, size=num_samples)\n    gender = rng.choice(genders, size=num_samples)\n    \n    le = LabelEncoder()\n    gender_encoded = le.fit_transform(gender)\n    \n    data = {'Country': country, 'Age': age, 'Gender': gender_encoded}\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/759", "solution": "import os\nimport shutil\nimport fnmatch\ndef task_func(source_directory, destination_directory, file_pattern):\n    moved_files = []\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            if fnmatch.fnmatch(file, file_pattern):\n                source_file = os.path.join(root, file)\n                destination_file = os.path.join(destination_directory, file)\n                shutil.move(source_file, destination_file)\n                moved_files.append(file)\n    return moved_files"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nfrom datetime import datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n              rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Generate IDs\n    ids = np.arange(1, 101)\n    \n    # Generate Names\n    names = []\n    for _ in range(100):\n        if np.random.rand() < 0.5:\n            name = np.random.choice(latin_names)\n        else:\n            name = np.random.choice(other_names)\n        # Correct improper encoding\n        name = name.encode('utf-8', 'replace').decode('utf-8')\n        names.append(name)\n    \n    # Generate Date of Birth\n    dates = []\n    for _ in range(100):\n        year = np.random.randint(start_year, end_year + 1)\n        month = np.random.randint(1, 13)\n        day = np.random.randint(1, 28)  # Assuming all months have 28 days for simplicity\n        date = datetime(year, month, day)\n        dates.append(date.strftime('%y-%m-%d %H:%M:%S'))\n    \n    # Generate Email\n    emails = []\n    for name, date in zip(names, dates):\n        # Extract year from date\n        year = date.split('-')[0]\n        # Construct email\n        email = re.sub(r'[^a-zA-Z0-9]', '', name.lower()) + year + '@' + email_domain\n        emails.append(email)\n    \n    # Create DataFrame\n    data = {\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates,\n        'Email': emails\n    }\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/761", "solution": "import json\nimport re\nfrom collections import Counter\nREPLACE_NONE = \"None\"\ndef task_func(json_str):\n    # Load the JSON string into a Python dictionary\n    data = json.loads(json_str)\n    \n    # Function to process the data recursively\n    def process_data(d):\n        # Remove None values\n        if isinstance(d, dict):\n            return {k: process_data(v) for k, v in d.items() if v is not None}\n        elif isinstance(d, list):\n            return [process_data(item) for item in d if item is not None]\n        else:\n            return d\n    \n    # Process the data\n    processed_data = process_data(data)\n    \n    # Count the frequency of each unique value\n    value_counts = Counter(processed_data)\n    \n    # Replace all email addresses with \"None\"\n    if isinstance(processed_data, str):\n        processed_data = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', REPLACE_NONE, processed_data)\n    elif isinstance(processed_data, list):\n        processed_data = [re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', REPLACE_NONE, item) if isinstance(item, str) else item for item in processed_data]\n    elif isinstance(processed_data, dict):\n        for key, value in processed_data.items():\n            if isinstance(value, str):\n                processed_data[key] = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', REPLACE_NONE, value)\n    \n    # Return the result\n    return {\n        \"data\": processed_data,\n        \"value_counts\": value_counts\n    }"}
{"task_id": "BigCodeBench/762", "solution": "import codecs\nimport os\nimport zipfile\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    # Create the directory\n    os.makedirs(directory_name, exist_ok=True)\n    \n    # Create and write to each .txt file\n    for file_name in file_names:\n        file_path = os.path.join(directory_name, file_name)\n        with codecs.open(file_path, 'w', encoding=encoding) as file:\n            file.write(content)\n    \n    # Zip the directory\n    zip_file_name = directory_name + \".zip\"\n    with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_name):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, os.path.relpath(file_path, directory_name))\n    \n    # Remove the directory\n    os.rmdir(directory_name)\n    \n    return zip_file_name"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a dictionary to hold the means and medians\n    results = defaultdict(lambda: {'mean': None, 'median': None})\n    \n    # Iterate over each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            # Convert the value to a float\n            value = float(value)\n            # Calculate mean and median\n            if results[key]['mean'] is None:\n                results[key]['mean'] = []\n                results[key]['median'] = []\n            results[key]['mean'].append(value)\n            results[key]['median'].append(value)\n    \n    # Calculate the mean and median for each key\n    for key in results:\n        results[key]['mean'] = np.mean(results[key]['mean'])\n        results[key]['median'] = np.median(results[key]['median'])\n    \n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=['key', 'mean', 'median'])\n        writer.writeheader()\n        for key in results:\n            writer.writerow({'key': key, 'mean': results[key]['mean'], 'median': results[key]['median']})"}
{"task_id": "BigCodeBench/764", "solution": "import csv\nimport random\ndef task_func(csv_file='names.csv', \n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'],\n              encoding='latin-1', rng_seed=None):\n    if not isinstance(csv_file, str):\n        raise TypeError(\"csv_file must be a string\")\n    if not isinstance(latin_names, list):\n        raise TypeError(\"latin_names must be a list\")\n    if not isinstance(names, list):\n        raise TypeError(\"names must be a list\")\n    \n    if rng_seed is not None:\n        random.seed(rng_seed)\n    \n    with open(csv_file, 'w', newline='', encoding=encoding) as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Name', 'Age'])\n        for _ in range(50):\n            name = random.choice(latin_names)\n            age = random.randint(20, 50)\n            writer.writerow([name, age])\n        for _ in range(50):\n            name = random.choice(names)\n            age = random.randint(20, 50)\n            writer.writerow([name, age])\n    \n    return csv_file"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    for file_path in kwargs.values():\n        if os.path.exists(file_path):\n            if os.path.getsize(file_path) > 0:\n                target_path = Path(target_dir) / Path(file_path).name\n                shutil.copy(file_path, target_path)\n                copied_files.append(str(target_path))\n    return copied_files"}
{"task_id": "BigCodeBench/766", "solution": "import re\nimport collections\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"string must be a str\")\n    if not isinstance(patterns, list) or not all(isinstance(pattern, str) for pattern in patterns):\n        raise TypeError(\"patterns must be a list of str\")\n    counts = collections.defaultdict(int)\n    for pattern in patterns:\n        counts[pattern] = len(re.findall(pattern, string))\n    return dict(counts)"}
{"task_id": "BigCodeBench/767", "solution": "from collections import Counter\nimport random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(list_of_lists):\n    # Replace each sublist with a random letter\n    random_letters = [random.choice(LETTERS) for _ in list_of_lists]\n    \n    # Count the occurrences of each letter\n    letter_counts = Counter(random_letters)\n    \n    return dict(letter_counts)"}
{"task_id": "BigCodeBench/768", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path):\n    if not os.path.exists(dir_path):\n        raise ValueError(\"Directory does not exist\")\n    error_count = {}\n    for file in glob.glob(dir_path + '/**/*.txt', recursive=True):\n        with open(file, 'r') as f:\n            content = f.read()\n            count = len(re.findall(r'\\berror\\b', content, re.IGNORECASE))\n            error_count[os.path.relpath(file, dir_path)] = count\n    return error_count"}
{"task_id": "BigCodeBench/769", "solution": "from collections import Counter\nimport itertools\nimport operator\ndef task_func(list_of_menuitems):\n    # Flatten the nested list\n    flat_list = list(itertools.chain.from_iterable(list_of_menuitems))\n    \n    # Count the frequency of each menu item\n    counter = Counter(flat_list)\n    \n    # Find the most common menu item\n    most_common = counter.most_common(1)[0][0]\n    \n    return most_common"}
{"task_id": "BigCodeBench/770", "solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    if test_size < 0.0 or test_size >= 1.0:\n        raise ValueError(\"Test set size must be between 0.0 and 1.0\")\n    if test_size * num_samples < 2:\n        raise ValueError(\"Test set size is too small\")\n    \n    np.random.seed(random_seed)\n    X = np.random.rand(num_samples, 1)\n    y = 2 * X.squeeze() + 1 + noise_strength * np.random.randn(num_samples)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    r2 = r2_score(y_test, y_pred)\n    \n    return r2, model"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    directory = Path(directory)\n    for file in directory.iterdir():\n        if file.is_file() and file.suffix == '.csv':\n            match = re.match(pattern, file.stem)\n            if match:\n                new_name = f\"{match.group(1)}-new.csv\"\n                new_file = directory / new_name\n                with open(file, 'r') as f_in, open(new_file, 'w', newline='') as f_out:\n                    reader = csv.reader(f_in)\n                    writer = csv.writer(f_out)\n                    for row in reader:\n                        writer.writerow(row)\n                new_files.append(new_name)\n    return new_files"}
{"task_id": "BigCodeBench/772", "solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    original_data = np.random.normal(loc=0, scale=1, size=(num_samples, d))\n    linear_transformed_data = original_data @ np.random.normal(loc=0, scale=1, size=(d, k))\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(linear_transformed_data)\n    mse = mean_squared_error(original_data, standardized_data)\n    return mse"}
{"task_id": "BigCodeBench/773", "solution": "import os\nimport re\nimport shutil\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\ndef task_func():\n    # List all files in the source directory\n    files = os.listdir(SOURCE_DIR)\n    \n    for file in files:\n        # Check if the file is a JSON file\n        if file.endswith('.json'):\n            # Use the regex to split the filename\n            match = FILE_PATTERN.match(file)\n            if match:\n                # Extract the prefix part of the filename\n                prefix = match.group(1)\n                # Construct the new filename\n                new_filename = f\"{prefix}.json\"\n                # Construct the full paths\n                source_path = os.path.join(SOURCE_DIR, file)\n                target_path = os.path.join(TARGET_DIR, new_filename)\n                # Move the file to the target directory with the new filename\n                shutil.move(source_path, target_path)\n                print(f\"Moved {file} to {new_filename} in {TARGET_DIR}\")\n            else:\n                print(f\"Skipping {file}: does not match the expected pattern.\")\n        else:\n            print(f\"Skipping {file}: not a JSON file.\")"}
{"task_id": "BigCodeBench/774", "solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv < 2\")\n    np.random.seed(random_seed)\n    X = np.random.randn(num_samples, 5)\n    y = np.sum(X, axis=1) + np.random.randn(num_samples)\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error')\n    return -np.mean(scores), model"}
{"task_id": "BigCodeBench/775", "solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\ndef task_func(string):\n    match = LETTERS_PATTERN.match(string)\n    if match:\n        prefix = match.group(1)\n        frequency = Counter(prefix)\n        return dict(frequency)\n    else:\n        return {}"}
{"task_id": "BigCodeBench/776", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error reading the CSV file: {e}\")\n\n    # Sort the DataFrame by the specified column key\n    try:\n        df_sorted = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error sorting the data: {e}\")\n\n    if linear_regression:\n        if x_column is None or y_column is None:\n            raise ValueError(\"x_column and y_column must be specified for linear regression.\")\n        if x_column not in df_sorted.columns or y_column not in df_sorted.columns:\n            raise ValueError(\"Specified columns for linear regression do not exist in the dataframe.\")\n        \n        # Fit a linear regression model\n        model = LinearRegression()\n        try:\n            model.fit(df_sorted[[x_column]], df_sorted[y_column])\n        except Exception as e:\n            raise Exception(f\"Error fitting the linear regression model: {e}\")\n        \n        return model\n    else:\n        if output_path is None:\n            return df_sorted\n        else:\n            try:\n                df_sorted.to_csv(output_path, index=False)\n            except Exception as e:\n                raise Exception(f\"Error writing to the output file: {e}\")\n            return output_path"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    # Compile the regular expression pattern\n    regex = re.compile(pattern)\n    \n    # List to store the directories where files were extracted\n    extracted_dirs = []\n    \n    # Walk through all files in the specified directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file matches the pattern\n            match = regex.match(file)\n            if match:\n                # Extract the prefix part of the filename\n                prefix = match.group(1)\n                \n                # Create the directory name using the prefix\n                extract_dir = os.path.join(root, prefix)\n                \n                # Create the directory if it doesn't exist\n                if not os.path.exists(extract_dir):\n                    os.makedirs(extract_dir)\n                \n                # Construct the full path of the zip file\n                zip_path = os.path.join(root, file)\n                \n                # Extract the zip file to the created directory\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(extract_dir)\n                \n                # Add the extracted directory to the list\n                extracted_dirs.append(extract_dir)\n    \n    return extracted_dirs"}
{"task_id": "BigCodeBench/778", "solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\ndef task_func(news_articles):\n    # Check if all dictionaries in the list have the required keys\n    required_keys = {'title', 'title_url', 'id', 'category'}\n    for article in news_articles:\n        if required_keys.issubset(article.keys()):\n            continue\n        else:\n            raise ValueError(\"Dictionary keys do not match the requirements.\")\n    \n    # Sort the articles by 'category' and then by 'title'\n    sorted_articles = sorted(news_articles, key=itemgetter('category', 'title'))\n    \n    # Group the sorted articles by 'category'\n    grouped_articles = defaultdict(list)\n    for article in sorted_articles:\n        grouped_articles[article['category']].append(article)\n    \n    return grouped_articles"}
{"task_id": "BigCodeBench/779", "solution": "import os\nimport shutil\nimport tempfile\nBACKUP_DIR = '/tmp/backup'\ndef get_unique_backup_dir():\n    return tempfile.mkdtemp(dir=BACKUP_DIR)\ndef task_func(directory):\n    errors = []\n    backup_dir = get_unique_backup_dir()\n    try:\n        shutil.copytree(directory, backup_dir)\n    except Exception as e:\n        errors.append(str(e))\n    try:\n        shutil.rmtree(directory)\n    except Exception as e:\n        errors.append(str(e))\n    return backup_dir, errors"}
{"task_id": "BigCodeBench/780", "solution": "import pandas as pd\nimport pytz\ndef task_func(articles, timezone):\n    if not isinstance(articles, list):\n        raise TypeError(\"Articles must be a list of dictionaries.\")\n    if not articles:\n        raise ValueError(\"Articles list cannot be empty.\")\n    for article in articles:\n        if not isinstance(article, dict):\n            raise ValueError(\"Each article must be a dictionary.\")\n        if 'published_time' not in article or 'category' not in article:\n            raise ValueError(\"Each article must contain 'published_time' and 'category' keys.\")\n    \n    # Convert 'published_time' to the specified timezone\n    for article in articles:\n        article['published_time'] = pytz.utc.localize(article['published_time']).astimezone(pytz.timezone(timezone))\n    \n    # Group articles by 'category'\n    articles_df = pd.DataFrame(articles)\n    grouped = articles_df.groupby('category')\n    \n    # Calculate count, mean, min, max publication times only considering the hour\n    result = grouped['published_time'].agg(['count', 'mean', 'min', 'max'])\n    result['mean'] = result['mean'].dt.hour\n    result['min'] = result['min'].dt.hour\n    result['max'] = result['max'].dt.hour\n    \n    return result"}
{"task_id": "BigCodeBench/781", "solution": "import os\nfrom datetime import datetime\ndef task_func(filepath: str) -> dict:\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n    \n    file_stats = os.stat(filepath)\n    size = file_stats.st_size\n    modification_time = datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')\n    \n    return {'size': size, 'last_modified': modification_time}"}
{"task_id": "BigCodeBench/782", "solution": "import random\nimport pandas as pd\nimport numpy as np\ndef task_func(n,\n              domain=\"samplewebsite.com\",\n              categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n              random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n        random.seed(random_seed)\n    \n    titles = [f\"Article {i}\" for i in range(n)]\n    ids = list(range(n))\n    categories = random.choices(categories, k=n)\n    views = np.random.poisson(lam=1000, size=n)\n    \n    title_urls = [f\"{domain}/Article_{i}\" for i in range(n)]\n    \n    data = {\n        'title': titles,\n        'title_url': title_urls,\n        'id': ids,\n        'category': categories,\n        'views': views\n    }\n    \n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/783", "solution": "import os\nimport shutil\ndef task_func(src_dir, dest_dir, extension):\n    files_moved = 0\n    for filename in os.listdir(src_dir):\n        if filename.endswith(extension):\n            src_file = os.path.join(src_dir, filename)\n            dest_file = os.path.join(dest_dir, filename)\n            shutil.move(src_file, dest_file)\n            files_moved += 1\n    return files_moved"}
{"task_id": "BigCodeBench/784", "solution": "import pandas as pd\nimport random\nimport csv\ndef task_func(n, \n              categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n              news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n              likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n              file_path='news_survey_data.csv',\n              random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    \n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1  # Assigning numerical value starting from 1\n        data.append([site, category, response, value])\n    \n    # Write to CSV\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Site', 'Category', 'Response', 'Value'])\n        writer.writerows(data)\n    \n    # Read from CSV into DataFrame\n    df = pd.read_csv(file_path)\n    return df"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n    # Find files matching the pattern\n    files = glob.glob(pattern)\n    if not files:\n        print(f\"No files found matching the pattern: {pattern}\")\n        return None\n    # Create archive file name\n    archive_name = os.path.join(ARCHIVE_DIR, f\"archive_{os.path.basename(pattern)}.tar.gz\")\n    # Create archive\n    with open(archive_name, 'wb') as f:\n        subprocess.run(['tar', '-czf', '-', *files], stdout=f)\n    # Delete original files\n    for file in files:\n        os.remove(file)\n    return archive_name"}
{"task_id": "BigCodeBench/786", "solution": "import pandas as pd\nimport csv\nimport random\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    data = []\n    for _ in range(n):\n        country = random.choice(countries)\n        product = random.choice(products)\n        sales = random.randint(1, 100)\n        data.append([country, product, sales])\n    df = pd.DataFrame(data, columns=['Country', 'Product', 'Sales'])\n    if output_path is not None:\n        df.to_csv(output_path, index=False)\n    return df"}
{"task_id": "BigCodeBench/787", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(array1, array2):\n    if len(array1) != len(array2):\n        raise ValueError(\"Input arrays must have the same length.\")\n    \n    if len(array1) == 0:\n        return 0.0\n    \n    points1 = np.array(array1)\n    points2 = np.array(array2)\n    \n    max_distance = 0.0\n    for p1, p2 in combinations(zip(points1, points2), 2):\n        distance = np.linalg.norm(np.array(p1) - np.array(p2))\n        if distance > max_distance:\n            max_distance = distance\n    \n    return max_distance"}
{"task_id": "BigCodeBench/788", "solution": "import heapq\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n    if N <= 1:\n        raise ValueError(\"N must be greater than 1.\")\n    \n    # Calculate absolute differences\n    differences = abs(df[col1] - df[col2])\n    \n    # Find the N largest differences\n    largest_diffs = heapq.nlargest(N, differences)\n    \n    # Perform t-Test on the elements with these differences\n    # Assuming that the t-Test is performed on the original values, not the differences\n    # But since t-Test is typically performed on means, we need to clarify this\n    # For now, I'll assume that we are to perform a t-Test on the original values\n    # But the problem says \"perform a t-Test on the elements with these differences\"\n    # This is ambiguous, so I'll assume that we are to perform a t-Test on the differences themselves\n    # But t-Test is usually performed on means, not individual differences\n    # Maybe it's a t-Test on the means of the top N differences\n    # Or perhaps it's a t-Test on the top N differences as a sample\n    # I need to clarify this\n    # For now, I'll assume that we are to perform a t-Test on the top N differences as a sample\n    # But t-Test requires two independent samples, not a single sample\n    # Maybe it's a t-Test on the top N differences against a reference value, like 0\n    # Or perhaps it's a t-Test on the top N differences against the mean difference\n    # I need to make an assumption here\n    # Let's assume that we are to perform a t-Test on the top N differences against the mean difference\n    # This way, we have a sample (the top N differences) and a reference value (the mean difference)\n    # This seems reasonable\n    # So, first, calculate the mean difference\n    mean_diff = differences.mean()\n    # Then, perform a one-sample t-Test on the top N differences against this mean\n    t_stat, p_value = stats.ttest_1samp(largest_diffs, mean_diff)\n    return p_value\ndf = pd.DataFrame({\n    'col1': [1, 3, 4, 70],\n    'col2': [2, 3, 5, 1]\n})"}
{"task_id": "BigCodeBench/789", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nARRAY_LENGTH = 10\ndef task_func():\n    # Generate a random array\n    random_array = np.random.rand(ARRAY_LENGTH)\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Reshape the array to fit the scaler\n    # The scaler expects a 2D array, so we reshape it to (ARRAY_LENGTH, 1)\n    reshaped_array = random_array.reshape(-1, 1)\n    \n    # Fit and transform the array\n    scaled_array = scaler.fit_transform(reshaped_array)\n    \n    # Reshape back to 1D array\n    scaled_array = scaled_array.flatten()\n    \n    return scaled_array"}
{"task_id": "BigCodeBench/790", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport heapq\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n    \n    scaler = StandardScaler()\n    df_scaled = df.copy()\n    df_scaled[col1] = scaler.fit_transform(df[[col1]])\n    df_scaled[col2] = scaler.fit_transform(df[[col2]])\n    \n    differences = []\n    for index, row in df_scaled.iterrows():\n        diff = abs(row[col1] - row[col2])\n        differences.append((index, diff))\n    \n    differences.sort(key=lambda x: x[1], reverse=True)\n    \n    return [index for index, diff in differences[:N]]"}
{"task_id": "BigCodeBench/791", "solution": "from collections import Counter\nimport random\nfrom itertools import cycle\ndef task_func(l):\n    # Cycle the list indefinitely\n    cycled_list = cycle(l)\n    # Take the first 30 elements from the cycled list\n    first_30 = [next(cycled_list) for _ in range(30)]\n    # Shuffle the first 30 elements\n    random.shuffle(first_30)\n    # Move the first 3 elements to the end of the list\n    first_3 = first_30[:3]\n    remaining = first_30[3:]\n    shuffled_list = remaining + first_3\n    # Create a frequency counter\n    counter = Counter(shuffled_list)\n    return counter"}
{"task_id": "BigCodeBench/792", "solution": "import heapq\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n    \n    model = LinearRegression()\n    model.fit(df[[feature]], df[target])\n    \n    residuals = model.predict(df[[feature]]) - df[target]\n    abs_residuals = abs(residuals)\n    indices = heapq.nsmallest(n, range(len(abs_residuals)), key=abs_residuals.__getitem__)\n    \n    return indices, model"}
{"task_id": "BigCodeBench/793", "solution": "import numpy as np\nimport random\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\ndef task_func(l=None):\n    if l is None:\n        l = random.sample(ELEMENTS, 10)\n    arr = np.array(l)\n    if len(arr) < 3:\n        return arr\n    else:\n        return np.concatenate((arr[3:], arr[:3]))"}
{"task_id": "BigCodeBench/794", "solution": "import string\nimport random\ndef task_func(length, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    BRACKETS = '(){}[]'\n    all_chars = string.ascii_lowercase + BRACKETS\n    return ''.join(random.choice(all_chars) for _ in range(length))"}
{"task_id": "BigCodeBench/795", "solution": "from collections import deque\nimport math\ndef task_func(l):\n    dq = deque(l)\n    dq.rotate(3)\n    print(f\"deque: {dq}\")\n    if dq:\n        sum_deq = sum(dq)\n        sqrt_sum = math.sqrt(sum_deq)\n        print(f\"The square root of the sum of numeric elements: {sqrt_sum}\")\n    return dq"}
{"task_id": "BigCodeBench/796", "solution": "import os\nimport re\ndef task_func(directory):\n    BRACKET_PATTERN = '[(){}\\\\[\\\\]]'\n    pattern = re.compile(BRACKET_PATTERN)\n    result = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if pattern.search(file):\n                result.append(os.path.join(root, file))\n    return result"}
{"task_id": "BigCodeBench/797", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    total_brackets = 0\n    for col in df.columns:\n        for val in df[col]:\n            if isinstance(val, str):\n                total_brackets += len(re.findall(r'[(){}[\\]]', val))\n    return total_brackets"}
{"task_id": "BigCodeBench/798", "solution": "import os\nimport shutil\nBACKUP_DIR = '/tmp/backup'\ndef task_func(directory):\n    \"\"\"\n    Rollback the update of a directory by restoring it from a backup.\n\n    Args:\n    directory (str): The directory to rollback.\n\n    Returns:\n    str: The restored directory path if successful, otherwise an error message.\n    \"\"\"\n    backup_path = os.path.join(BACKUP_DIR, directory)\n    if not os.path.exists(BACKUP_DIR):\n        return f\"Backup directory {BACKUP_DIR} does not exist. Cannot rollback update.\"\n    if not os.path.exists(backup_path):\n        return f\"Backup for directory {directory} does not exist. Cannot rollback update.\"\n    try:\n        shutil.rmtree(directory)\n        shutil.copytree(backup_path, directory)\n        return directory\n    except Exception as e:\n        return f\"An error occurred while rolling back the update: {e}\""}
{"task_id": "BigCodeBench/799", "solution": "import pandas as pd\nfrom random import seed, choices\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if not L:\n        return pd.DataFrame(), []\n    \n    if random_seed is not None:\n        seed(random_seed)\n    \n    column_names = choices('abcdefghijklmnopqrstuvwxyz', k=3)\n    df_list = []\n    \n    for _ in range(num_dataframes):\n        sample = choices(L, k=3)\n        df = pd.DataFrame(sample, columns=column_names)\n        df_list.append(df)\n    \n    common_rows = df_list[0]\n    for df in df_list[1:]:\n        common_rows = common_rows.merge(df, on=column_names, how='inner')\n    \n    return common_rows, df_list"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize a Counter to hold the total counts of goals and penalties\n    total_counts = Counter()\n    \n    # Check if the CSV file exists\n    if os.path.exists(csv_file_path):\n        with open(csv_file_path, mode='r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                # Convert goals and penalties to integers and add to the total counts\n                total_counts['goals'] += int(row['goals'])\n                total_counts['penalties'] += int(row['penalties'])\n    \n    # Add the new goals and penalties to the total counts\n    total_counts['goals'] += goals\n    total_counts['penalties'] += penalties\n    \n    return total_counts"}
{"task_id": "BigCodeBench/801", "solution": "import csv\nimport collections\nimport numpy as np\ndef task_func(file_name):\n    with open(file_name, 'r') as file:\n        reader = csv.reader(file)\n        header = next(reader)\n        data = list(reader)\n    if not data:\n        return {}\n    transposed_data = list(map(list, zip(*data)))\n    result = {}\n    for i, col in enumerate(transposed_data):\n        col = [x for x in col if x != '']\n        if not col:\n            continue\n        counter = collections.Counter(col)\n        most_common = counter.most_common()\n        if most_common:\n            most_common_value = most_common[0][0]\n            if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n                most_common_value = sorted([x[0] for x in most_common if x[1] == most_common[0][1]])[0]\n            result[header[i]] = most_common_value\n    return result"}
{"task_id": "BigCodeBench/802", "solution": "import numpy as np\nimport itertools\ndef task_func(dimension, seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Create a 2D numpy array of the given dimension with random integers between 1 and 100\n    matrix = np.random.randint(1, 101, size=dimension)\n    \n    # Flatten the matrix into a 1D list\n    flat_list = matrix.flatten().tolist()\n    \n    # Return a tuple containing the matrix and the flat list\n    return (matrix, flat_list)"}
{"task_id": "BigCodeBench/803", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(file_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_name)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"No numeric columns found in the input file.\")\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    return df"}
{"task_id": "BigCodeBench/804", "solution": "import os\nfrom datetime import datetime\nLOG_DIR = './logs'\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    Writes a dictionary of metrics to a specified log file, appending a timestamp to each entry.\n\n    Args:\n    metrics (dict): A dictionary containing the metrics to be logged.\n    filename (str): The name of the log file to write to.\n    log_dir (str, optional): The directory where the log file is located. Defaults to LOG_DIR.\n\n    Returns:\n    bool: True if the metrics were successfully written to the file, False otherwise.\n    \"\"\"\n    # Create the log directory if it doesn't exist\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Construct the full path to the log file\n    log_file_path = os.path.join(log_dir, filename)\n\n    try:\n        # Open the log file in append mode\n        with open(log_file_path, 'a') as file:\n            # Get the current timestamp\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            # Write the timestamp and metrics to the file\n            file.write(f\"Timestamp: {timestamp}\\n\")\n            for key, value in metrics.items():\n                file.write(f\"{key}: {value}\\n\")\n            file.write(\"\\n\")  # Add a newline for separation between entries\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False"}
{"task_id": "BigCodeBench/805", "solution": "import pandas as pd\nimport random\ndef task_func(dictionary, item, seed):\n    # Set the seed for random number generation\n    random.seed(seed)\n    \n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(dictionary)\n    \n    # Find the locations of the particular item in the DataFrame\n    locations = []\n    for col in df.columns:\n        for idx, val in enumerate(df[col]):\n            if val == item:\n                locations.append((idx, col))\n    \n    # Count the number of occurrences\n    count = len(locations)\n    \n    # Add a random integer x, where 0 <= x < 10, to the count\n    x = random.randint(0, 9)\n    total_count = count + x\n    \n    # Return the list of locations, the total count, and the DataFrame\n    return locations, total_count, df"}
{"task_id": "BigCodeBench/806", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text, n=2):\n    # Remove duplicate and stopwords from the string\n    words = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower().split()\n    words = [word for word in words if word not in STOPWORDS]\n    unique_words = list(set(words))\n    \n    # Generate n-grams\n    n_grams = []\n    for i in range(len(unique_words) - n + 1):\n        n_gram = ' '.join(unique_words[i:i + n])\n        n_grams.append(n_gram)\n    \n    # Count the n-grams\n    n_gram_counts = Counter(n_grams)\n    \n    return dict(n_gram_counts)"}
{"task_id": "BigCodeBench/807", "solution": "import numpy as np\nfrom scipy.stats import norm\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    \"\"\"\n    Determine the outlier indices in a 1D numpy array based on the Z score.\n    \n    Parameters:\n    - data: np.ndarray, the input data array.\n    - threshold: float, the threshold for Z score to determine outliers.\n    \n    Returns:\n    - list: The indices of outliers in the data where Z score > threshold. Empty if standard deviation is 0.\n    - float: The mean of the fitted normal distribution.\n    - float: The variance of the fitted normal distribution.\n    \"\"\"\n    if data.std() == 0:\n        return [], 0.0, 0.0\n    \n    mean = data.mean()\n    var = data.var()\n    z_scores = np.abs((data - mean) / np.sqrt(var))\n    outliers = np.where(z_scores > threshold)[0]\n    \n    return outliers.tolist(), mean, var"}
{"task_id": "BigCodeBench/808", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text):\n    # Remove duplicate and stopwords from the string \"text.\"\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    # Split the text into words\n    words = text.split()\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n    # Join the words back into a string\n    cleaned_text = ' '.join(words)\n    # Analyze the sentiment of the text using TextBlob\n    blob = TextBlob(cleaned_text)\n    sentiment = blob.sentiment.polarity\n    # Output the sentiment\n    print(f\"Sentiment: {sentiment}\")\n    return sentiment"}
{"task_id": "BigCodeBench/809", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    cluster_dict = {}\n    for i, label in enumerate(labels):\n        if label not in cluster_dict:\n            cluster_dict[label] = []\n        cluster_dict[label].append(i)\n    return cluster_dict\ndata = np.array([[1, 1], [2, 2]])"}
{"task_id": "BigCodeBench/810", "solution": "import re\nimport os\nimport subprocess\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    # Validate the directory path\n    if not os.path.isdir(dir_path):\n        raise ValueError(\"The provided directory path does not exist.\")\n    \n    # Compile the regular expression pattern for efficiency\n    pattern = re.compile(exe_pattern)\n    \n    # List to hold the results\n    results = []\n    \n    # Walk through the directory tree\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            # Check if the file matches the pattern\n            if pattern.search(file):\n                file_path = os.path.join(root, file)\n                if execute_files:\n                    # Execute the file and capture the output\n                    try:\n                        # Use subprocess to execute the file\n                        # Assuming the file is executable and can be run with subprocess\n                        # For example, if it's a Python script, you can use subprocess.run with the appropriate arguments\n                        # Here, we'll use subprocess.run with shell=True for simplicity\n                        # Note: Be cautious with shell=True as it can pose security risks\n                        result = subprocess.run([file_path], capture_output=True, text=True, shell=True)\n                        results.append(result.stdout)\n                    except Exception as e:\n                        # Handle any exceptions that occur during execution\n                        results.append(f\"Error executing {file_path}: {str(e)}\")\n                else:\n                    # Just append the file path\n                    results.append(file_path)\n    \n    return results"}
{"task_id": "BigCodeBench/811", "solution": "import pandas as pd\nfrom random import randint, seed\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(dictionary)\n    \n    # Find the positions of the particular item in the DataFrame\n    positions = [(row, col) for row, col in zip(df.index, df.columns) if df.at[row, col] == item]\n    \n    # Record the frequency distribution of the item\n    frequency = len(positions)\n    \n    # Optionally, return a random sample of these positions\n    if sample_size is not None:\n        if random_seed is not None:\n            seed(random_seed)\n        sample_positions = random.sample(positions, min(sample_size, len(positions)))\n        return sample_positions, df, frequency\n    else:\n        return positions, df, frequency\ndictionary = {1: ['road', 'car', 'traffic'], 2: ['car', 'light', 'candle']}"}
{"task_id": "BigCodeBench/812", "solution": "def task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    # Create a tar file\n    tar = tarfile.open(\"archive.tar\", \"w\")\n    \n    # Find files that match the pattern\n    for file in Path(directory).glob(\"*\"):\n        if re.search(file_pattern, str(file)):\n            tar.add(file)\n    \n    # Close the tar file\n    tar.close()\n    \n    # Return the path to the created tar file\n    return Path(\"archive.tar\")"}
{"task_id": "BigCodeBench/813", "solution": "from itertools import combinations\nimport pandas as pd\ndef task_func(number_list, element):\n    if not number_list:\n        return pd.DataFrame(columns=['Combinations'])\n    \n    # Find all unique combinations of 3 numbers\n    all_combinations = list(combinations(number_list, 3))\n    \n    # Filter combinations that sum up to the element\n    valid_combinations = [comb for comb in all_combinations if sum(comb) == element]\n    \n    # If no valid combinations, return an empty DataFrame\n    if not valid_combinations:\n        return pd.DataFrame(columns=['Combinations'])\n    \n    # Create a DataFrame with the valid combinations\n    result_df = pd.DataFrame({'Combinations': valid_combinations})\n    \n    return result_df"}
{"task_id": "BigCodeBench/814", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Create a configuration file path\n    config_file_path = os.path.join(target_dir, 'config.txt')\n    \n    # List to hold matching file paths\n    matching_files = []\n    \n    # Walk through the directory tree\n    for dirpath, dirnames, filenames in os.walk(source_dir):\n        for filename in filenames:\n            # Check if the filename matches the pattern\n            if re.match(file_pattern, filename):\n                # Construct the full file path\n                file_path = os.path.join(dirpath, filename)\n                # Append the file path to the list\n                matching_files.append(file_path)\n    \n    # Write the matching file paths to the configuration file\n    with open(config_file_path, 'w') as config_file:\n        for file_path in matching_files:\n            config_file.write(file_path + '\\n')\n    \n    # Return the path to the created configuration file\n    return config_file_path"}
{"task_id": "BigCodeBench/815", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    if student not in df['Student'].values:\n        raise ValueError('student is not present in the test_scores dataframe')\n    avg = df[df['Student'] == student]['Score'].mean()\n    std = df[df['Student'] == student]['Score'].std()\n    return np.array([avg, std]), df"}
{"task_id": "BigCodeBench/816", "solution": "from collections import Counter\nimport random\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\ndef task_func():\n    # Generate a random poker hand consisting of five cards\n    hand = [random.choice(HAND_RANKS) + random.choice(SUITS) for _ in range(5)]\n    \n    # Count the frequency of each card rank in the hand\n    rank_count = Counter(card[0] for card in hand)\n    \n    # Return the hand and the rank count as a tuple\n    return (hand, rank_count)"}
{"task_id": "BigCodeBench/817", "solution": "from collections import Counter\nimport logging\ndef task_func(letter_list, element, log_path):\n    # Configure logging\n    logging.basicConfig(filename=log_path + 'task_func.log', encoding='utf-8', level=logging.DEBUG)\n    logger = logging.getLogger()\n\n    # Log function call\n    logger.info(f\"Function called with list: {letter_list} and element: {element}\")\n\n    # Count frequency of each letter\n    frequency = Counter(letter_list)\n\n    # Check if element is in the list\n    if element in frequency:\n        # Log frequency\n        logger.info(f\"Frequency of '{element}' is {frequency[element]}\")\n        # Return frequency\n        return frequency[element]\n    else:\n        # Log error\n        logger.error(\"The element is not in the letter list.\")\n        # Raise ValueError\n        raise ValueError(\"The element is not in the letter list.\")"}
{"task_id": "BigCodeBench/818", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    # Remove punctuation using regular expression\n    text = re.sub(f'[{re.escape(PUNCTUATION)}]', '', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Split the text into words\n    words = text.split()\n    # Return the list of cleaned words\n    return words"}
{"task_id": "BigCodeBench/819", "solution": "import time\nimport random\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"iterations must be a positive integer\")\n    if not (isinstance(min_delay, float) and min_delay > 0) or not (isinstance(max_delay, float) and max_delay > 0):\n        raise ValueError(\"min_delay and max_delay must be positive floating point values\")\n    if min_delay >= max_delay:\n        raise ValueError(\"min_delay must be less than max_delay\")\n    if seed is not None:\n        random.seed(seed)\n    messages = []\n    total_delay = 0.0\n    for _ in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        messages.append(f\"{delay:.2f} seconds have passed\")\n        total_delay += delay\n    return messages, total_delay"}
{"task_id": "BigCodeBench/820", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(num_words, word_length):\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length must be non-negative integers.\")\n    \n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n    \n    return words"}
{"task_id": "BigCodeBench/821", "solution": "import time\nimport threading\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    threads = []\n    results = []\n    def worker(thread_id):\n        time.sleep(delay_time)\n        results.append(f\"Delay in thread {thread_id} completed\")\n    for i in range(num_threads):\n        thread = threading.Thread(target=worker, args=(i,))\n        threads.append(thread)\n        thread.start()\n    for thread in threads:\n        thread.join()\n    return results"}
{"task_id": "BigCodeBench/822", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\nDIGITS = string.digits\ndef task_func(length, num_digits):\n    if num_digits < 0 or num_digits > length:\n        raise ValueError(\"Number of digits must be between 0 and the specified length.\")\n    \n    # Generate the required number of digits\n    digits = ''.join(random.choice(DIGITS) for _ in range(num_digits))\n    \n    # Generate the remaining characters as letters\n    remaining_length = length - num_digits\n    letters = ''.join(random.choice(LETTERS) for _ in range(remaining_length))\n    \n    # Combine digits and letters\n    password = digits + letters\n    \n    # Shuffle the password to ensure variability\n    password_list = list(password)\n    random.shuffle(password_list)\n    password = ''.join(password_list)\n    \n    return password"}
{"task_id": "BigCodeBench/823", "solution": "import time\nimport numpy as np\ndef task_func(samples=10, delay=0.1):\n    delay_times = []\n    for _ in range(samples):\n        start_time = time.time()\n        time.sleep(delay)\n        end_time = time.time()\n        delay_times.append(end_time - start_time)\n    mean_delay = np.mean(delay_times)\n    std_delay = np.std(delay_times)\n    return (mean_delay, std_delay)"}
{"task_id": "BigCodeBench/824", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    # Count words\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    \n    # Count punctuation marks\n    punctuation_count = sum(1 for char in text if char in PUNCTUATION)\n    \n    return (word_count, punctuation_count)"}
{"task_id": "BigCodeBench/825", "solution": "import numpy as np\nfrom itertools import product\nimport string\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    \"\"\"\n    Generate a list of 10 randomly picked strings from all possible strings of a given length from the provided series of characters, using a specific seed for reproducibility.\n\n    Parameters:\n    - length (int): The length of the strings to generate.\n    - seed (int, optional): The seed for the random number generator to ensure reproducibility. Defaults to None.\n    - alphabets (list of str, optional): The list of characters to use for generating the strings. Defaults to all lowercase letters.\n\n    Returns:\n    - list: A list of 10 generated strings.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate all possible combinations of the given length from the provided alphabets\n    all_combinations = [''.join(p) for p in product(alphabets, repeat=length)]\n    \n    # Randomly select 10 unique strings from the list of all combinations\n    selected_strings = np.random.choice(all_combinations, size=10, replace=False)\n    \n    return selected_strings.tolist()"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    moved_files_count = 0\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), os.path.join(target_dir, filename))\n            moved_files_count += 1\n    return moved_files_count"}
{"task_id": "BigCodeBench/827", "solution": "import math\nfrom sympy import isprime\ndef task_func(input_list):\n    # Filter the prime numbers from the list\n    primes = [num for num in input_list if isprime(num)]\n    \n    # Sort the prime numbers based on their radian value converted to degrees\n    sorted_primes = sorted(primes, key=lambda x: math.degrees(x))\n    \n    return sorted_primes"}
{"task_id": "BigCodeBench/828", "solution": "import os\nimport errno\nimport shutil\ndef task_func(filename, dest_dir):\n    # Check if the destination directory exists, if not, create it\n    if not os.path.exists(dest_dir):\n        try:\n            os.makedirs(dest_dir)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n\n    # Construct the destination file path\n    dest_file = os.path.join(dest_dir, os.path.basename(filename))\n\n    # Copy the file to the destination directory\n    try:\n        shutil.copy(filename, dest_file)\n    except OSError as e:\n        raise\n\n    # Clear the contents of the original file\n    with open(filename, 'w') as f:\n        f.truncate()\n\n    # Return the absolute path to the copied file within the destination directory\n    return os.path.abspath(dest_file)"}
{"task_id": "BigCodeBench/829", "solution": "import pandas as pd\nfrom statistics import mean\ndef task_func(df: pd.DataFrame) -> dict:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Name' and 'Score' columns.\")\n    \n    name_scores = df.groupby('Name')['Score'].apply(list)\n    gen_dict = {}\n    for name, scores in name_scores.items():\n        gen_dict[name] = (name, mean(scores))\n    \n    return gen_dict\ndf_sample = pd.DataFrame({\n    'Name': ['Micky', 'Donald', 'Girl'],\n    'Score': [25.2, 9, -1]\n})"}
{"task_id": "BigCodeBench/830", "solution": "import json\nimport os\ndef task_func(filename, data):\n    try:\n        with open(filename, 'w') as f:\n            json.dump(data, f)\n        with open(filename, 'r') as f:\n            written_data = json.load(f)\n        if os.path.exists(filename):\n            return (True, written_data)\n        else:\n            return (False, written_data)\n    except Exception as e:\n        return (False, str(e))"}
{"task_id": "BigCodeBench/831", "solution": "import random\nimport math\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    for _ in range(pairs_count):\n        num1 = random.randint(range_start, range_end)\n        num2 = random.randint(range_start, range_end)\n        diff = abs(num1 - num2)\n        sqrt_diff = math.sqrt(diff)\n        yield (num1, num2, sqrt_diff)"}
{"task_id": "BigCodeBench/832", "solution": "import pickle\nimport os\ndef task_func(filename, data):\n    try:\n        # Check if the directory exists, create it if necessary\n        directory = os.path.dirname(filename)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        \n        # Serialize the data using pickle\n        with open(filename, 'wb') as file:\n            pickle.dump(data, file)\n        \n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False"}
{"task_id": "BigCodeBench/833", "solution": "import random\nfrom collections import Counter\nfrom statistics import mode\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    frequency = Counter(random_list)\n    mode_val = mode(random_list)\n    return mode_val, ((num, freq) for num, freq in frequency.items())"}
{"task_id": "BigCodeBench/834", "solution": "import binascii\nimport io\nimport gzip\ndef task_func(compressed_hex):\n    try:\n        # Convert the hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n        \n        # Decompress the bytes using gzip\n        decompressed_bytes = gzip.decompress(compressed_bytes)\n        \n        # Decode the bytes to UTF-8 string\n        decoded_string = decompressed_bytes.decode('utf-8')\n        \n        return decoded_string\n    except Exception as e:\n        return str(e)"}
{"task_id": "BigCodeBench/835", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    data = np.random.randint(0, 100, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    df = df.drop(columns=remove_cols)\n    return df"}
{"task_id": "BigCodeBench/836", "solution": "import os\nimport shutil\nimport csv\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    result = {}\n    for filename in os.listdir(csv_dir):\n        if filename.endswith('.csv'):\n            filepath = os.path.join(csv_dir, filename)\n            with open(filepath, 'r') as file:\n                reader = csv.reader(file)\n                for index, row in enumerate(reader):\n                    if row and row[0] == target_value:\n                        result[filename] = index\n                        if not simulate:\n                            if not os.path.exists(processed_dir):\n                                os.makedirs(processed_dir)\n                            shutil.move(filepath, os.path.join(processed_dir, filename))\n                        break\n    return result"}
{"task_id": "BigCodeBench/837", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    data = np.random.rand(n_rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    if scale_cols:\n        scaler = StandardScaler()\n        df[scale_cols] = scaler.fit_transform(df[scale_cols])\n    return df"}
{"task_id": "BigCodeBench/838", "solution": "import re\nfrom nltk.stem import PorterStemmer\nimport nltk\ndef task_func(text_series):\n    ps = PorterStemmer()\n    def process_text(text):\n        # Lowercase the text\n        text = text.lower()\n        # Remove non-alphanumeric characters except spaces\n        text = re.sub(r'[^a-z0-9 ]', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Tokenize the text\n        words = nltk.word_tokenize(text)\n        # Stem each word\n        stemmed_words = [ps.stem(word) for word in words]\n        # Join the stemmed words back into a string\n        return ' '.join(stemmed_words)\n    # Apply the processing function to each element in the Series\n    return text_series.apply(process_text)"}
{"task_id": "BigCodeBench/839", "solution": "import csv\nimport random\nimport string\ndef task_func(file_path,\n              num_rows,\n              gender=['Male', 'Female', 'Non-Binary'],\n              countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        headers = ['Name', 'Age', 'Gender', 'Country']\n        writer.writerow(headers)\n        \n        if num_rows <= 0:\n            return file_path\n        \n        for _ in range(num_rows):\n            name = ''.join(random.choices(string.ascii_uppercase, k=5))\n            age = random.randint(20, 60)\n            gen = random.choice(gender)\n            country = random.choice(countries)\n            writer.writerow([name, age, gen, country])\n    \n    return file_path"}
{"task_id": "BigCodeBench/840", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    \"\"\"\n    Creates a CSV file on a given file path with random numeric data.\n\n    Parameters:\n    - file_path (str): The path where the CSV file will be saved.\n    - num_rows (int): The number of rows in the CSV file.\n    - data_dimensions (int, optional): The number of columns (features) in the CSV file. Defaults to 5.\n    - random_seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n    - str: The file path of the generated CSV file.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate random data\n    data = np.random.rand(num_rows, data_dimensions)\n\n    # Create DataFrame\n    columns = [f'Feature_{i+1}' for i in range(data_dimensions)]\n    df = pd.DataFrame(data, columns=columns)\n\n    # Save to CSV\n    df.to_csv(file_path, index=False)\n\n    return file_path"}
{"task_id": "BigCodeBench/841", "solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\ndef task_func(json_string):\n    try:\n        data = json.loads(json_string)\n    except json.JSONDecodeError:\n        return {}\n    \n    text = data.get(\"text\")\n    if text is None:\n        return {}\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation and non-alphanumeric characters except spaces\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n    \n    # Split text into words\n    words = text.split()\n    \n    # Count word frequency\n    word_count = defaultdict(int)\n    for word in words:\n        word_count[word] += 1\n    \n    return dict(word_count)"}
{"task_id": "BigCodeBench/842", "solution": "import sqlite3\nimport random\ndef task_func(db_path, num_entries, users=['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'], countries=['USA', 'UK', 'Canada', 'Australia', 'India'], random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    \n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n    \n    c.execute('''CREATE TABLE IF NOT EXISTS users\n                 (id INTEGER PRIMARY KEY,\n                  name TEXT,\n                  age INTEGER,\n                  country TEXT)''')\n    \n    for i in range(num_entries):\n        user = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute(\"INSERT INTO users (name, age, country) VALUES (?, ?, ?)\", (user, age, country))\n    \n    conn.commit()\n    conn.close()\n    \n    return db_path"}
{"task_id": "BigCodeBench/843", "solution": "import random\nimport re\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\ndef task_func(n_sentences):\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.sample(WORD_LIST, random.randint(1, len(WORD_LIST))))\n        sentences.append(sentence)\n    result = ' '.join(sentences)\n    result = re.sub(r'[^a-zA-Z0-9\\s.]', '', result)\n    result = result.lower()\n    return result"}
{"task_id": "BigCodeBench/844", "solution": "import csv\nimport random\nfrom faker import Faker\ndef task_func(file_path, num_rows, random_seed=None):\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be an integer >= 0\")\n    \n    fake = Faker()\n    if random_seed is not None:\n        fake.seed(random_seed)\n    \n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Address', 'Email'])\n        for _ in range(num_rows):\n            name = fake.name()\n            age = random.randint(20, 60)\n            address = fake.address().replace('\\n', ', ')\n            email = fake.email()\n            writer.writerow([name, age, address, email])\n    \n    return file_path"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean the texts\n    text1_clean = ALPHANUMERIC.sub(' ', text1).lower()\n    text2_clean = ALPHANUMERIC.sub(' ', text2).lower()\n    \n    # Tokenize the texts\n    words1 = text1_clean.split()\n    words2 = text2_clean.split()\n    \n    # Compute term frequencies\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n    \n    # Get the unique terms\n    unique_terms = set(tf1.keys()) | set(tf2.keys())\n    \n    # Create vectors for each text\n    vector1 = [tf1.get(term, 0) for term in unique_terms]\n    vector2 = [tf2.get(term, 0) for term in unique_terms]\n    \n    # Compute cosine similarity\n    dot_product = sum(a*b for a, b in zip(vector1, vector2))\n    magnitude1 = np.sqrt(sum(a*a for a in vector1))\n    magnitude2 = np.sqrt(sum(b*b for b in vector2))\n    if magnitude1 == 0 or magnitude2 == 0:\n        cosine_similarity = 0\n    else:\n        cosine_similarity = dot_product / (magnitude1 * magnitude2)\n    \n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(text1_clean, text2_clean)\n    \n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/846", "solution": "import collections\nimport pandas as pd\ndef task_func(obj_list, attr):\n    if not obj_list:\n        return pd.DataFrame(columns=['attribute', 'count'])\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    counter = collections.Counter(attr_values)\n    df = pd.DataFrame(list(counter.items()), columns=['attribute', 'count'])\n    return df"}
{"task_id": "BigCodeBench/847", "solution": "import re\nimport os\nimport string\nimport random\ndef task_func(input_string, directory='./text_files'):\n    # Create directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    \n    # Split the input string into lines\n    lines = input_string.split('\\n')\n    \n    # Initialize a list to hold file paths\n    file_paths = []\n    \n    # Iterate over each line\n    for index, line in enumerate(lines):\n        # Remove special characters using regex\n        cleaned_line = re.sub(r'[^a-zA-Z0-9\\s]', '', line)\n        \n        # Generate a unique filename\n        filename = f'file_{random.randint(1, 10000)}.txt'\n        filepath = os.path.join(directory, filename)\n        \n        # Write the cleaned line to the file\n        with open(filepath, 'w') as file:\n            file.write(cleaned_line)\n        \n        # Append the file path to the list\n        file_paths.append(filepath)\n    \n    return file_paths"}
{"task_id": "BigCodeBench/848", "solution": "import heapq\nimport random\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    # Extract the values of the specified attribute from the list of objects\n    values = [getattr(obj, attr) for obj in obj_list]\n    \n    # Find the top N values using a heap\n    top_values = heapq.nlargest(top_n, values)\n    \n    # Randomly sample a value from all attributes\n    if values:\n        random_value = random.choice(values)\n    else:\n        random_value = None\n    \n    return top_values, random_value\nclass Object:\n    def __init__(self, value):\n        self.test = value\nobj_list = [Object(random.randint(1, 12)) for _ in range(13)]"}
{"task_id": "BigCodeBench/849", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(input_string):\n    # Remove punctuation and convert to lowercase\n    input_string = re.sub(r'[^\\w\\s]', '', input_string).lower()\n    # Split the string into words\n    words = input_string.split()\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n    # Count the frequency of each word\n    word_counts = Counter(words)\n    return dict(word_counts)"}
{"task_id": "BigCodeBench/850", "solution": "import pandas as pd\nimport statistics\nimport random\ndef task_func(students, subjects, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    data = []\n    for student in students:\n        grades = [random.randint(0, 100) for _ in range(len(subjects))]\n        average_grade = statistics.mean(grades)\n        data.append([student] + grades + [average_grade])\n    columns = ['Student'] + subjects + ['Average Grade']\n    df = pd.DataFrame(data, columns=columns)\n    return df"}
{"task_id": "BigCodeBench/851", "solution": "import textwrap\nimport re\ndef task_func(input_string, width):\n    # Split the input string into separate strings based on newline characters\n    lines = input_string.split('\\n')\n    \n    # Initialize an empty list to store the wrapped lines\n    wrapped_lines = []\n    \n    # Iterate over each line in the input string\n    for line in lines:\n        # Use textwrap.fill to wrap the line to the specified width\n        wrapped_line = textwrap.fill(line, width=width)\n        # Append the wrapped line to the list\n        wrapped_lines.append(wrapped_line)\n    \n    # Join the wrapped lines with newline characters to form the final wrapped string\n    wrapped_string = '\\n'.join(wrapped_lines)\n    \n    # Return the wrapped string\n    return wrapped_string"}
{"task_id": "BigCodeBench/852", "solution": "import random\nimport string\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be at least 1\")\n    random.seed(seed)\n    result = []\n    for _ in range(n_samples):\n        length = random.randint(1, max_length)\n        random_string = ''.join(random.choices(string.ascii_lowercase, k=length))\n        result.append(random_string)\n    return result"}
{"task_id": "BigCodeBench/853", "solution": "import os\nimport shutil\nimport string\nINVALID_CHARACTERS = string.punctuation + string.whitespace\ndef task_func(directory_path):\n    summary = {}\n    invalid_dir = os.path.join(directory_path, \"Invalid\")\n    if not os.path.exists(invalid_dir):\n        os.makedirs(invalid_dir)\n    for filename in os.listdir(directory_path):\n        if filename.endswith(INVALID_CHARACTERS):\n            shutil.move(os.path.join(directory_path, filename), invalid_dir)\n            if \"Invalid\" in summary:\n                summary[\"Invalid\"] += 1\n            else:\n                summary[\"Invalid\"] = 1\n    return summary"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of integers.\")\n    if not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"All elements in the list must be integers.\")\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"All numbers must be non-negative.\")\n    \n    # Calculate factorial of each number\n    factorials = [math.factorial(num) for num in numbers]\n    \n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n    \n    # Calculate the sum of factorials for each permutation\n    sum_fac_perms = [sum(factorials) for _ in perms]\n    \n    return sum_fac_perms, perms"}
{"task_id": "BigCodeBench/855", "solution": "import random\nimport string\nimport collections\nVALID_CHARACTERS = string.ascii_letters + string.digits\ndef task_func(n_strings, string_length):\n    # Generate n random strings of a specified length\n    random_strings = [''.join(random.choice(VALID_CHARACTERS) for _ in range(string_length)) for _ in range(n_strings)]\n    \n    # Count the frequency of each character across all strings\n    all_chars = ''.join(random_strings)\n    char_count = collections.Counter(all_chars)\n    \n    # Return the result as a dictionary\n    return dict(char_count)"}
{"task_id": "BigCodeBench/856", "solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"high must be greater than low\")\n    np.random.seed(seed)\n    matrix = np.random.randint(low, high, size=shape)\n    pairs = list(combinations(matrix.flatten(), 2))\n    sum_of_products = reduce(lambda x, y: x + y, [a*b for a, b in pairs])\n    return sum_of_products, matrix"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n    for extension in EXTENSIONS:\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*{extension}'))\n        for file in files:\n            try:\n                shutil.copy2(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/858", "solution": "import string\nimport random\nfrom collections import Counter\ndef task_func(n, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    letters = [random.choice(string.ascii_lowercase) for _ in range(n)]\n    return Counter(letters)"}
{"task_id": "BigCodeBench/859", "solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\ndef task_func():\n    # Load the iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n\n    # Create an SVM classifier\n    clf = svm.SVC(kernel='linear')\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = clf.predict(X_test)\n\n    # Calculate accuracy\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    # Check if accuracy is less than 0.9\n    if accuracy < 0.9:\n        warning_msg = \"Accuracy is below 0.9\"\n        warnings.warn(warning_msg, stacklevel=2)\n    else:\n        warning_msg = None\n\n    # Return the results\n    return (accuracy, warning_msg)"}
{"task_id": "BigCodeBench/860", "solution": "import re\nimport random\nimport string\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    random_string = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(n))\n    matches = re.findall(pattern, random_string)\n    return matches"}
{"task_id": "BigCodeBench/861", "solution": "from collections import Counter\nfrom random import choice, seed\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\ndef task_func(list_of_lists):\n    baskets = []\n    for lst in list_of_lists:\n        cart = Counter()\n        for _ in range(len(lst)):\n            item = choice(POSSIBLE_ITEMS)\n            cart[item] += 1\n        baskets.append(cart)\n    return baskets"}
{"task_id": "BigCodeBench/862", "solution": "import random\nimport string\nfrom collections import defaultdict\ndef task_func(n, seed=None):\n    random.seed(seed)\n    alphabet = string.ascii_lowercase\n    result = defaultdict(list)\n    for _ in range(n):\n        letter = random.choice(alphabet)\n        result[letter].append(letter)\n    return dict(result)"}
{"task_id": "BigCodeBench/863", "solution": "import numpy as np\nimport math\nPOSSIBLE_NUMBERS = np.arange(1, 11)\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        # Determine the number of elements to consider from POSSIBLE_NUMBERS\n        num_elements = len(lst)\n        # Select the first 'num_elements' from POSSIBLE_NUMBERS\n        selected_numbers = POSSIBLE_NUMBERS[:num_elements]\n        # Calculate the sum of squares of these numbers\n        sum_squares = np.sum(selected_numbers ** 2)\n        sums.append(sum_squares)\n    return sums"}
{"task_id": "BigCodeBench/864", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(fruit_data):\n    if not fruit_data:\n        return pd.DataFrame()\n    \n    # Create a dictionary to hold the total and average counts for each fruit\n    fruit_counts = {}\n    for fruit, count in fruit_data:\n        if fruit in fruit_counts:\n            fruit_counts[fruit]['total'] += count\n            fruit_counts[fruit]['count'] += 1\n        else:\n            fruit_counts[fruit] = {'total': count, 'count': 1}\n    \n    # Create a list of dictionaries for DataFrame creation\n    data = []\n    for fruit, counts in fruit_counts.items():\n        total = counts['total']\n        count = counts['count']\n        average = total / count if count > 0 else np.nan\n        data.append({'Fruit': fruit, 'Total Count': total, 'Average Count': average})\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    df.set_index('Fruit', inplace=True)\n    \n    return df"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item for item, _, _ in data]\n    counts = [count for _, count, _ in data]\n    weights = [weight for _, _, weight in data]\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(np.array(weights).reshape(-1,1)).flatten()\n    \n    # Create a pandas DataFrame with the results\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/866", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extract coordinates from the data\n    coordinates = [point[1:] for point in data]\n    # Convert to numpy array\n    coordinates = np.array(coordinates)\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    labels = kmeans.fit_predict(coordinates)\n    return labels\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]"}
{"task_id": "BigCodeBench/867", "solution": "import re\nimport string\ndef task_func(text1, text2):\n    # Define the pattern to match ASCII punctuation\n    pattern = f'[{re.escape(string.punctuation)}]'\n    \n    # Use re.sub to remove the matched punctuation from both texts\n    cleaned_text1 = re.sub(pattern, '', text1)\n    cleaned_text2 = re.sub(pattern, '', text2)\n    \n    # Return the cleaned texts as a tuple\n    return (cleaned_text1, cleaned_text2)"}
{"task_id": "BigCodeBench/868", "solution": "from itertools import cycle\nfrom random import choice, seed\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if n_colors <= 0:\n        return []\n    \n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    color_cycle = cycle(colors)\n    random_colors = []\n    \n    for _ in range(n_colors):\n        color = next(color_cycle)\n        random_color = choice(colors)\n        random_colors.append(color)\n        random_colors.append(random_color)\n    \n    return random_colors"}
{"task_id": "BigCodeBench/869", "solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"The list of students is empty.\")\n    \n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    grade_assigner = cycle(grade_range)\n    grade_report = []\n    \n    for _ in range(n_grades):\n        student = students.pop(0)\n        grade = next(grade_assigner)\n        grade_report.append({'Student': student, 'Grade': grade})\n    \n    return pd.DataFrame(grade_report)"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize a list to hold the means for each position\n    means = []\n    \n    # Determine the number of positions based on the first tuple\n    num_positions = len(data_list[0]) if data_list else 0\n    \n    # Iterate over each position\n    for i in range(num_positions):\n        # Extract the values at the current position from all tuples\n        values = [item[i] for item in data_list if len(item) > i]\n        \n        # Filter out non-numeric values\n        numeric_values = [x for x in values if isinstance(x, (int, float))]\n        \n        # Calculate the mean of numeric values\n        if numeric_values:\n            mean_value = np.mean(numeric_values)\n        else:\n            mean_value = np.nan\n        \n        # Append the mean to the list\n        means.append(mean_value)\n    \n    # Create a DataFrame with the means\n    df = pd.DataFrame({'Mean Value': means}, index=[f'Position {i}' for i in range(num_positions)])\n    \n    return df\ndata = [('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]"}
{"task_id": "BigCodeBench/871", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list, file_name):\n    if not data_list:\n        with open(file_name, 'w') as f:\n            pass\n        return []\n    \n    # Extract numerical values from each tuple, excluding the first string\n    numerical_data = []\n    for tup in data_list:\n        if len(tup) < 2:\n            continue  # Skip tuples that don't have at least two elements\n        try:\n            # Convert all elements except the first to floats\n            numerical_values = [float(item) for item in tup[1:]]\n            numerical_data.append(numerical_values)\n        except ValueError:\n            # If conversion fails, fill with np.nan\n            numerical_data.append([np.nan] * (len(tup) - 1))\n    \n    # Calculate mean for each position\n    if not numerical_data:\n        with open(file_name, 'w') as f:\n            pass\n        return []\n    \n    # Use itertools to handle variable lengths\n    transposed = itertools.zip_longest(*numerical_data, fillvalue=np.nan)\n    means = []\n    for idx, values in enumerate(transposed, start=1):\n        mean = np.nanmean(values)\n        means.append(mean)\n        with open(file_name, 'a') as f:\n            f.write(f'Position {idx}: {mean}\\n')\n    \n    return means"}
{"task_id": "BigCodeBench/872", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        return []\n    \n    # Unzip the list of tuples\n    unzipped = list(itertools.zip_longest(*data_list, fillvalue=0))\n    \n    # Initialize a list to hold the means\n    means = []\n    \n    for position in unzipped:\n        # Filter out non-numeric values\n        numeric_values = [x for x in position if isinstance(x, (int, float))]\n        \n        if numeric_values:\n            # Calculate the mean of numeric values\n            mean_value = np.mean(numeric_values)\n            means.append(mean_value)\n        else:\n            # If no numeric values, append np.nan\n            means.append(np.nan)\n    \n    return means"}
{"task_id": "BigCodeBench/873", "solution": "import csv\nimport os\ndef task_func(data, file_path, headers):\n    if file_path is None:\n        raise ValueError(\"file_path cannot be None\")\n    \n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            writer.writerow(row)\n    \n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/874", "solution": "from itertools import zip_longest\nfrom scipy.spatial import distance\ndef task_func(points):\n    if len(points) < 2:\n        return []\n    distances = []\n    for point1, point2 in zip_longest(points, points[1:], fillvalue=()):\n        if len(point1) == 1:\n            point1 = (point1[0], point1[0])\n        if len(point2) == 1:\n            point2 = (point2[0], point2[0])\n        if len(point1) == 2 and len(point2) == 2:\n            dist = distance.euclidean(point1, point2)\n            distances.append(dist)\n    return distances"}
{"task_id": "BigCodeBench/875", "solution": "import pandas as pd\nimport random\ndef task_func(data, columns=['Name', 'Age', 'Occupation'], fill_missing=False, num_range=(0, 100), seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    # Create a list of dictionaries from the list of tuples\n    data_dicts = []\n    for row in data:\n        row_dict = {}\n        for i, value in enumerate(row):\n            row_dict[columns[i]] = value\n        data_dicts.append(row_dict)\n    \n    # Create DataFrame from the list of dictionaries\n    df = pd.DataFrame(data_dicts)\n    \n    # Fill missing values if required\n    if fill_missing:\n        for col in df.columns:\n            if df[col].dtype == 'float64' or df[col].dtype == 'int64':\n                df[col].fillna(random.uniform(num_range[0], num_range[1]), inplace=True)\n            else:\n                df[col].fillna('Missing', inplace=True)\n    \n    return df\ndata = [('Mango', 20), ('Apple', ), ('Banana', )]"}
{"task_id": "BigCodeBench/876", "solution": "import collections\nimport operator\nimport os\nimport shutil\ndef task_func(data_dict, source_directory, backup_directory):\n    # Task 1: Update the dictionary by adding a key 'a' with the value 1\n    data_dict['a'] = 1\n\n    # Task 2: Sort the dictionary by the frequency of its values in descending order\n    # Count the frequency of each value\n    value_counts = collections.Counter(data_dict.values())\n    # Sort the dictionary by the frequency of its values\n    sorted_items = sorted(data_dict.items(), key=lambda item: value_counts[item[1]], reverse=True)\n    # Convert sorted items to a list of tuples\n    value_frequencies = [(item[0], value_counts[item[1]]) for item in sorted_items]\n\n    # Task 3: Back up all files from the specified source directory to a backup directory\n    # Create the backup directory if it doesn't exist\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n    # List all files in the source directory\n    files = os.listdir(source_directory)\n    # Copy each file to the backup directory\n    backup_status = True\n    for file in files:\n        source_file = os.path.join(source_directory, file)\n        backup_file = os.path.join(backup_directory, file)\n        try:\n            shutil.copy2(source_file, backup_file)\n        except Exception as e:\n            print(f\"Error backing up {file}: {e}\")\n            backup_status = False\n\n    return data_dict, value_frequencies, backup_status\ndata_dict = {'avc': '1', 'hello': 'world', 'test': 'world', 'cat': 'meow'}"}
{"task_id": "BigCodeBench/877", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    if not data.select_dtypes(include=[np.number]).count().all():\n        raise ValueError(\"DataFrame must contain only numeric data.\")\n    if n_components > data.shape[1]:\n        raise ValueError(\"n_components cannot be greater than the number of columns in the data.\")\n    if data.empty:\n        raise ValueError(\"Input data cannot be empty.\")\n    \n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n    pca = PCA(n_components=n_components)\n    pca.fit(scaled_data)\n    transformed_data = pca.transform(scaled_data)\n    \n    return pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\ndata = pd.DataFrame({\n    'A': [-43, 212, 1, -12, 5],\n    'B': [-1, 0, 0, 9.76, 12.34],\n    'C': [1, 42, -13.2, 31, 1.23]\n})"}
{"task_id": "BigCodeBench/878", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\ndef task_func(data, target, test_size=0.2, random_state=None):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    \n    # Check if the target column exists in the DataFrame\n    if target not in df.columns:\n        raise ValueError(f\"The target column '{target}' is not in the DataFrame.\")\n    \n    # Split the data into train and test sets\n    X = df.drop(columns=[target])\n    y = df[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Train a RandomForestRegressor model\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate the mean squared error\n    mse = mean_squared_error(y_test, y_pred)\n    \n    return mse, model, df"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Column(s) {col1} and/or {col2} not found in the DataFrame.\")\n    \n    # Check if col1 and col2 are categorical\n    if not data[col1].dtype.name.startswith('category') or not data[col2].dtype.name.startswith('category'):\n        raise TypeError(\"One or both of the columns contain non-categorical data.\")\n    \n    # Check if both columns have multiple categories\n    if len(data[col1].cat.categories) < 2 or len(data[col2].cat.categories) < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Check if any category has less than 5 observations\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    if (contingency_table < 5).sum().sum() > 0:\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Perform chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p"}
{"task_id": "BigCodeBench/880", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=3, seed=None):\n    \"\"\"\n    Perform K-Means clustering on the given DataFrame using the sklearn KMeans algorithm.\n    \n    Parameters:\n    - data: pandas DataFrame with numerical values.\n    - n_clusters: int, number of clusters (default is 3).\n    - seed: int, random seed for reproducibility (default is None).\n    \n    Returns:\n    - labels: numpy.ndarray, cluster labels for each data point.\n    - model: sklearn.cluster.KMeans, the fitted KMeans model.\n    \n    Raises:\n    - ValueError: If the DataFrame contains non-numeric entries.\n    \"\"\"\n    # Check if all columns in the DataFrame are numeric\n    if not all(data.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise ValueError(\"DataFrame contains non-numeric entries.\")\n    \n    # Initialize the KMeans model\n    model = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    \n    # Fit the model and predict the labels\n    labels = model.fit_predict(data)\n    \n    return labels, model"}
{"task_id": "BigCodeBench/881", "solution": "import pandas as pd\nimport random\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_file)\n    \n    # Find rows where the specified column matches the regex pattern\n    matches = df[df[column_name].str.contains(pattern)]\n    \n    # If sample_size is specified, randomly sample the matches\n    if sample_size is not None:\n        # Set the random seed for reproducibility\n        random.seed(seed)\n        # Generate a list of random indices\n        sample_indices = random.sample(range(len(matches)), k=sample_size)\n        # Select the sampled rows\n        matches = matches.iloc[sample_indices]\n    \n    return matches"}
{"task_id": "BigCodeBench/882", "solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    if not os.path.exists(db_file):\n        raise ValueError(\"The database file does not exist.\")\n    \n    conn = sqlite3.connect(db_file)\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    conn.close()\n    \n    matches = []\n    for index, row in df.iterrows():\n        match = re.search(pattern, row[column_name])\n        if match:\n            matches.append(row)\n    \n    return pd.DataFrame(matches)"}
{"task_id": "BigCodeBench/883", "solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    # Filter rows where column_b > 50 and column_c == 900\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n    \n    # Check if column_a is empty after filtering\n    if filtered_df[column_a].empty:\n        return True\n    \n    # Check if all values in column_a are the same\n    if filtered_df[column_a].nunique() == 1:\n        return True\n    \n    # Perform Augmented Dickey-Fuller test\n    result = adfuller(filtered_df[column_a])\n    p_value = result[1]\n    \n    # Check if p_value is smaller than 0.05\n    if p_value < 0.05:\n        return True\n    else:\n        return False"}
{"task_id": "BigCodeBench/884", "solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    if len(columns) != 3:\n        raise ValueError(\"The number of specified columns is not 3.\")\n    \n    if not all(col in df.columns for col in columns):\n        raise ValueError(\"The specified columns are not contained in df.\")\n    \n    # Filter rows based on the criteria\n    filtered_df = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n    \n    if filtered_df.empty:\n        raise ValueError(\"Insufficient data for the test (no rows meeting the criteria).\")\n    \n    # Create a contingency table of the first two columns\n    contingency_table = pd.crosstab(filtered_df[columns[0]], filtered_df[columns[1]])\n    \n    # Perform chi-square independence test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p"}
{"task_id": "BigCodeBench/885", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.exceptions import NotFittedError\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if df is None or not isinstance(df, pd.DataFrame):\n        return None, None\n    if not all(col in df.columns for col in [col_a, col_b, col_c]):\n        return None, None\n    if not df[col_a].dtype in [int, float] or not df[col_b].dtype in [int, float] or not df[col_c].dtype in [int, float]:\n        return None, None\n    if df.empty:\n        return None, None\n    filtered_df = df[(df[col_b] > 50) & (df[col_c] == 900)]\n    if filtered_df.empty:\n        return None, None\n    X = filtered_df[col_a].values.reshape(-1, 1)\n    y = filtered_df[col_b].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    try:\n        predictions = model.predict(X_test)\n    except NotFittedError:\n        return None, None\n    return predictions, model"}
{"task_id": "BigCodeBench/886", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(data):\n    # Check if the required keys are present in the dictionary\n    required_keys = ['Name', 'Age', 'Score']\n    if not all(key in data for key in required_keys):\n        raise ValueError(\"The dictionary does not have the required keys.\")\n    \n    # Extract data\n    names = data['Name']\n    ages = data['Age']\n    scores = data['Score']\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Name': names,\n        'Age': ages,\n        'Score': scores\n    })\n    \n    # Sort the DataFrame by 'Name' and 'Age' in ascending order\n    df_sorted = df.sort_values(by=['Name', 'Age'])\n    \n    # Calculate the average score per student\n    avg_scores = df.groupby('Name')['Score'].mean()\n    \n    # Find the most common age\n    if len(ages) > 0:\n        most_common_age = Counter(ages).most_common(1)[0][0]\n    else:\n        most_common_age = None\n    \n    return df_sorted, avg_scores, most_common_age"}
{"task_id": "BigCodeBench/887", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(T1, row_num=50, seed=None):\n    # Convert elements in 'T1' to integers\n    T1 = tuple(map(int, itertools.chain.from_iterable(T1)))\n    \n    # Calculate the number of columns\n    num_cols = sum(T1)\n    \n    # Set the seed for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate random numbers for the DataFrame\n    data = np.random.randint(0, 100, size=(row_num, num_cols))\n    \n    # Create DataFrame with column names\n    columns = [f'Col_{i+1}' for i in range(num_cols)]\n    df = pd.DataFrame(data, columns=columns)\n    \n    return df"}
{"task_id": "BigCodeBench/888", "solution": "import pandas as pd\nimport os\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    if not csv_files:\n        return pd.DataFrame()\n    \n    df_list = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        if os.path.exists(file_path):\n            df = pd.read_csv(file_path)\n            df_list.append(df)\n    \n    if df_list:\n        return pd.concat(df_list, ignore_index=True)\n    else:\n        return pd.DataFrame()"}
{"task_id": "BigCodeBench/889", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    \"\"\"\n    Load a CSV file into a pandas DataFrame and replace the NaN values in numeric columns with the mean of the corresponding column.\n    The resulting DataFrame is returned. If an empty csv is passed, an empty DataFrame is returned.\n    The function should raise the exception for: FileNotFoundError: If csv_file does not exist.\n    \"\"\"\n    # Construct the full path to the CSV file\n    file_path = os.path.join(data_dir, csv_file)\n    \n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}\")\n    \n    # Load the CSV file into a pandas DataFrame\n    df = pd.read_csv(file_path)\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        return pd.DataFrame()\n    \n    # Replace NaN values in numeric columns with the mean of the column\n    for col in df.columns:\n        if df[col].dtype in [np.int64, np.float64]:\n            df[col].fillna(df[col].mean(), inplace=True)\n    \n    return df"}
{"task_id": "BigCodeBench/890", "solution": "import os\nimport random\nimport pandas as pd\ndef task_func(data_dir,\n              csv_files=['file1.csv', 'file2.csv', 'file3.csv'],\n              seed=None):\n    # Set the seed for reproducibility\n    if seed is not None:\n        random.seed(seed)\n    \n    # List all files in the data directory\n    files = os.listdir(data_dir)\n    \n    # Filter the list to only include csv files\n    csv_files_in_dir = [f for f in files if f.endswith('.csv')]\n    \n    # Check if there are any csv files in the directory\n    if not csv_files_in_dir:\n        return None, pd.DataFrame()\n    \n    # Select a random csv file from the list\n    selected_file = random.choice(csv_files_in_dir)\n    \n    # Construct the full path to the selected file\n    file_path = os.path.join(data_dir, selected_file)\n    \n    # Read the csv file into a DataFrame\n    df = pd.read_csv(file_path)\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        return selected_file, pd.DataFrame()\n    \n    # Select a certain number of records from the file at random\n    # For this example, let's select 5 records\n    num_records = 5\n    if num_records > len(df):\n        num_records = len(df)\n    selected_df = df.sample(n=num_records, random_state=seed)\n    \n    return selected_file, selected_df"}
{"task_id": "BigCodeBench/891", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_file_path)\n    \n    # Separate the data into features (X) and the target attribute (y)\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n    \n    # Return the trained model and the predictions\n    return model, predictions"}
{"task_id": "BigCodeBench/892", "solution": "import random\nfrom collections import Counter\ndef task_func(strings: list) -> dict:\n    # Initialize a dictionary to store the counts of each pattern\n    pattern_counts = {}\n    \n    # Define the pattern to look for\n    pattern = \"example\"  # Replace with the desired pattern\n    \n    # Iterate through each string in the list\n    for string in strings:\n        # Count the occurrences of the pattern in the string\n        count = string.count(pattern)\n        # Store the count in the dictionary\n        pattern_counts[string] = count\n    \n    return pattern_counts"}
{"task_id": "BigCodeBench/893", "solution": "import re\nfrom datetime import datetime\ndef task_func(logs: list):\n    error_times = []\n    for log in logs:\n        match = re.search(r'\\[(.*?)\\]', log)\n        if match:\n            time_str = match.group(1)\n            time_obj = datetime.strptime(time_str, '%H:%M:%S').time()\n            error_times.append(time_obj)\n    avg_time = sum(error_times, datetime.min.time()) / len(error_times)\n    return {\n        'list': error_times,\n        'time': avg_time\n    }"}
{"task_id": "BigCodeBench/894", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Generate random integers\n    arr = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(arr, bins=100, density=True, alpha=0.6, color='g')\n    \n    # Plot mean as a red dashed line\n    ax.axvline(mean, color='r', linestyle='dashed', linewidth=2)\n    \n    # Plot standard deviation as purple dashed lines\n    ax.axvline(mean + std_dev, color='purple', linestyle='dashed', linewidth=2)\n    ax.axvline(mean - std_dev, color='purple', linestyle='dashed', linewidth=2)\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Random Integers')\n    \n    # Show plot\n    plt.show()\n    \n    # Return the array, mean, standard deviation, and the histogram plot\n    return (arr, mean, std_dev, ax)"}
{"task_id": "BigCodeBench/895", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Generate random integers between 1 and 100\n    data = np.random.randint(1, 101, size=ARRAY_SIZE)\n    \n    # Calculate mean and standard deviation\n    mean = np.mean(data)\n    std_dev = np.std(data)\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(data, bins=50, color='skyblue', edgecolor='black')\n    \n    # Plot mean as a red dashed line\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=2)\n    \n    # Plot standard deviation as purple dashed lines\n    ax.axvline(mean + std_dev, color='purple', linestyle='dashed', linewidth=2)\n    ax.axvline(mean - std_dev, color='purple', linestyle='dashed', linewidth=2)\n    \n    # Set labels and title\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    \n    # Return the array, mean, standard deviation, and the histogram plot\n    return (data, mean, std_dev, ax)"}
{"task_id": "BigCodeBench/896", "solution": "from collections import Counter\nimport random\nimport itertools\ndef task_func(length, count, seed=0):\n    random.seed(seed)\n    letters = 'abcde'\n    strings = [''.join(random.choice(letters) for _ in range(length)) for _ in range(count)]\n    all_letters = itertools.chain.from_iterable(strings)\n    counter = Counter(all_letters)\n    return counter"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    results = [random.randint(1, 6) for _ in range(rolls)]\n    frequency = np.bincount(results, minlength=7)[1:]\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=6, range=(0.5, 6.5), align='mid', rwidth=0.8)\n    ax.set_title('Histogram of Dice Rolls')\n    ax.set_xlabel('Dice Value')\n    ax.set_ylabel('Frequency')\n    return frequency, ax"}
{"task_id": "BigCodeBench/898", "solution": "from collections import Counter\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = []\n    for _ in range(count):\n        pair = random.choices(LETTERS, k=2)\n        pairs.append(''.join(pair))\n    counter = Counter(pairs)\n    return counter"}
{"task_id": "BigCodeBench/899", "solution": "import numpy as np\nimport random\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random.seed(seed)\n    positions = [0]\n    for _ in range(length):\n        step = random.choice([-1, 1])\n        positions.append(positions[-1] + step)\n    return np.array(positions)"}
{"task_id": "BigCodeBench/900", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(d):\n    if not isinstance(d, list) or not all(isinstance(item, dict) for item in d):\n        raise ValueError(\"Input must be a list of dictionaries.\")\n    \n    x_values = [item.get('x') for item in d]\n    y_values = [item.get('y') for item in d]\n    z_values = [item.get('z') for item in d]\n    \n    stats = {}\n    for key, values in zip(['x', 'y', 'z'], [x_values, y_values, z_values]):\n        if all(isinstance(val, (int, float)) for val in values):\n            stats[key] = {\n                'mean': np.mean(values),\n                'sum': np.sum(values),\n                'max': np.max(values),\n                'min': np.min(values),\n                'std': np.std(values)\n            }\n        else:\n            stats[key] = {\n                'mean': None,\n                'sum': None,\n                'max': None,\n                'min': None,\n                'std': None\n            }\n    \n    return stats"}
{"task_id": "BigCodeBench/901", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n    \n    # Extract the values for scaling\n    x_values = [item['x'] for item in d]\n    y_values = [item['y'] for item in d]\n    z_values = [item['z'] for item in d]\n    \n    # Create a DataFrame from the extracted values\n    df = pd.DataFrame({'x': x_values, 'y': y_values, 'z': z_values})\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the data\n    scaled_data = scaler.fit_transform(df)\n    \n    # Create a new DataFrame with scaled values\n    scaled_df = pd.DataFrame(scaled_data, columns=['x', 'y', 'z'])\n    \n    return scaled_df"}
{"task_id": "BigCodeBench/902", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(d):\n    result = {}\n    for key in ['x', 'y', 'z']:\n        values = [item[key] for item in d if key in item]\n        result[key] = Counter(values)\n    return result"}
{"task_id": "BigCodeBench/903", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df[['x']]\n    y = df[target]\n    model = LinearRegression().fit(X, y)\n    return model"}
{"task_id": "BigCodeBench/904", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, keys=['x', 'y']):\n    df = pd.DataFrame(data)\n    ax = plt.axes()\n    df.plot(x=keys[0], y=keys[1], ax=ax)\n    return ax"}
{"task_id": "BigCodeBench/905", "solution": "import os\nimport glob\nimport csv\ndef task_func(directory_path, file_extension='.csv'):\n    data = {}\n    # Get all files with the specified extension in the directory\n    files = glob.glob(os.path.join(directory_path, '*' + file_extension))\n    for file in files:\n        # Open the file and read its rows\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            rows = [row for row in reader]\n        # Use the filename without extension as a key in the output dictionary\n        filename = os.path.splitext(os.path.basename(file))[0]\n        data[filename] = rows\n    return data"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Create a zip file in the target directory\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the source directory\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                # Check if the file has '_processed' in its name\n                if '_processed' in file:\n                    # Construct the full path of the file\n                    file_path = os.path.join(root, file)\n                    # Add the file to the zip archive\n                    zipf.write(file_path, os.path.relpath(file_path, source_dir))\n    \n    return archive_path"}
{"task_id": "BigCodeBench/907", "solution": "import os\nimport re\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    try:\n        # List all files in the directory\n        files = os.listdir(directory)\n        for file in files:\n            # Check if the file matches the pattern\n            if re.match(pattern, file):\n                # Construct the new name\n                new_name = re.sub(pattern, replacement, file)\n                # Rename the file\n                os.rename(os.path.join(directory, file), os.path.join(directory, new_name))\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False"}
{"task_id": "BigCodeBench/908", "solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\ndef task_func(directory: str, pattern: str) -> list:\n    # List to hold the axes objects\n    axes_list = []\n    \n    # Compile the regular expression pattern\n    regex = re.compile(pattern)\n    \n    # List all files in the directory\n    files = os.listdir(directory)\n    \n    # Iterate over each file\n    for file in files:\n        # Check if the file matches the pattern\n        if regex.match(file):\n            # Construct the full file path\n            file_path = os.path.join(directory, file)\n            \n            # Read the CSV file into a pandas DataFrame\n            df = pd.read_csv(file_path)\n            \n            # Create a plot\n            ax = plt.axes()\n            ax.plot(df['Month'], df['Sales'])\n            ax.set_xlabel('Month')\n            ax.set_ylabel('Sales')\n            ax.set_title(f'Sales Data from {file}')\n            \n            # Append the axes object to the list\n            axes_list.append(ax)\n    \n    return axes_list"}
{"task_id": "BigCodeBench/909", "solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    # Create a list of tuples, each containing a letter and a category\n    data = list(itertools.product(letters, categories))\n    \n    # Shuffle the list of tuples\n    shuffle(data)\n    \n    # Create a DataFrame from the shuffled list of tuples\n    df = pd.DataFrame(data, columns=['Letter', 'Category'])\n    \n    return df"}
{"task_id": "BigCodeBench/910", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(letters, repetitions, colors):\n    if len(letters) != len(repetitions) or len(letters) != len(colors) or len(letters) == 0:\n        raise ValueError(\"The lengths of the input lists do not match or if any list is empty.\")\n    \n    # Flatten the list by repeating each letter according to its repetition count\n    flattened = [letter for letter, repeat in zip(letters, repetitions) for _ in range(repeat)]\n    \n    # Count the frequency of each letter\n    frequency = {letter: flattened.count(letter) for letter in set(flattened)}\n    \n    # Create a bar chart\n    letters = list(frequency.keys())\n    frequencies = list(frequency.values())\n    \n    # Create a color map for each letter\n    color_map = {letter: color for letter, color in zip(letters, colors)}\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar(letters, frequencies, color=[color_map[letter] for letter in letters])\n    \n    # Add some text for labels, title, and custom x-axis tick labels, etc.\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n    ax.set_xticks(letters)\n    ax.set_xticklabels(letters)\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/911", "solution": "from functools import reduce\nimport operator\nimport string\ndef task_func(letters):\n    # Create a dictionary mapping each uppercase letter to its corresponding number\n    letter_to_num = {letter: index + 1 for index, letter in enumerate(string.ascii_uppercase)}\n    \n    # Convert the list of letters to their corresponding numbers\n    numbers = [letter_to_num[letter] for letter in letters]\n    \n    # Calculate the product of the numbers\n    product = reduce(operator.mul, numbers, 1)\n    \n    return product"}
{"task_id": "BigCodeBench/912", "solution": "from collections import Counter\nimport itertools\ndef task_func(letters: list, repetitions: int) -> dict:\n    # Repeat the list of letters the specified number of times\n    repeated_letters = list(itertools.chain.from_iterable(itertools.repeat(letters, repetitions)))\n    \n    # Count the frequency of each letter\n    frequency = Counter(repeated_letters)\n    \n    return dict(frequency)"}
{"task_id": "BigCodeBench/913", "solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\ndef task_func(data: List[Union[int, str]], repetitions: int = 1) -> dict:\n    if not data or repetitions <= 0:\n        return {'mode': np.array([]), 'count': np.array([])}\n    \n    # Repeat the data\n    repeated_data = data * repetitions\n    \n    # Calculate the mode(s)\n    unique, counts = np.unique(repeated_data, return_counts=True)\n    max_count = np.max(counts)\n    modes = unique[counts == max_count]\n    \n    # Sort the modes in ascending order\n    sorted_modes = np.sort(modes)\n    \n    # Calculate the count(s) of the mode(s)\n    sorted_counts = counts[np.isin(unique, sorted_modes)]\n    \n    # Calculate the Fast Fourier Transform\n    fft_result = scipy.fft.fft(repeated_data)\n    \n    return {\n        'mode': sorted_modes,\n        'count': sorted_counts,\n        'fft': fft_result\n    }"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Sort the dataframe by date\n    df = df.sort_values(by='Date')\n    \n    # Create a new column 'Day' which is the day number since the first date\n    df['Day'] = (df['Date'] - df['Date'].min()).dt.days\n    \n    # Prepare the data for the linear regression model\n    X = df[['Day']]\n    y = df['Close']\n    \n    # Fit the linear regression model\n    model = LinearRegression().fit(X, y)\n    \n    # Predict the next 7 days\n    last_day = df['Day'].max()\n    future_days = np.array(range(last_day + 1, last_day + 8)).reshape(-1, 1)\n    future_prices = model.predict(future_days)\n    \n    # Plot the data\n    plt.figure(figsize=(10,5))\n    plt.plot(df['Date'], df['Close'], label='Historical Prices')\n    plt.plot(df['Date'].max() + pd.to_timedelta(np.arange(1,8), unit='d'), future_prices, label='Predicted Prices', color='red')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.legend()\n    plt.title('Stock Prices')\n    plt.show()\n    \n    # Return the predicted prices and the plot\n    return (future_prices.tolist(), plt.gca())"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    # Calculate Z-Scores\n    z_scores = np.abs(zscore(df['closing_price']))\n    \n    # Identify outliers\n    outliers = df[z_scores > z_threshold]\n    \n    # Plot outliers\n    fig, ax = plt.subplots()\n    ax.scatter(outliers.index, outliers['closing_price'], color='red')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \n    Parameters:\n    - df: pandas DataFrame containing the stock data, with a column named 'Close' for closing prices.\n    \n    Returns:\n    - A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot and the second for the histogram.\n    \"\"\"\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Box plot\n    sns.boxplot(x=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    \n    # Histogram\n    sns.histplot(df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return axes[0], axes[1]"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Assuming the DataFrame has a column named 'Close' which contains the closing prices\n    # Convert the DataFrame to a Series for easier manipulation\n    series = df['Close']\n    \n    # Fit an ARIMA model to the data\n    model = ARIMA(series, order=(5,1,0))  # You can adjust the order parameters as needed\n    model_fit = model.fit()\n    \n    # Make predictions for the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    \n    # Plot the forecast\n    plt.figure(figsize=(10,5))\n    plt.plot(series, label='Historical Prices')\n    plt.plot(forecast, label='Forecasted Prices', color='red')\n    plt.legend()\n    plt.title('Forecasted Share Closing Prices')\n    plt.xlabel('Days')\n    plt.ylabel('Price')\n    axes = plt.gca()\n    \n    return forecast.tolist(), axes"}
{"task_id": "BigCodeBench/918", "solution": "import pandas as pd\nimport re\ndef task_func(data, mapping):\n    # Create a regex pattern for all acronyms in the mapping\n    acronym_pattern = re.compile(r'\\b(' + '|'.join(re.escape(acronym) for acronym in mapping.keys()) + r')\\b')\n    \n    # Function to replace matched acronyms with their full words\n    def replace_acronym(match):\n        return mapping[match.group(0)]\n    \n    # Apply the replacement to all string cells in the DataFrame\n    for col in data.columns:\n        if data[col].dtype == 'object':\n            data[col] = data[col].astype(str).apply(lambda x: acronym_pattern.sub(replace_acronym, x))\n    \n    return data"}
{"task_id": "BigCodeBench/919", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data)\n    \n    # Define the categories\n    CATEGORIES = ['A', 'B', 'C', 'D', 'E']\n    \n    # Get the counts for each category\n    counts = df[column].value_counts(dropna=False)\n    \n    # Create a dictionary to hold the counts for all categories\n    category_counts = {cat: counts.get(cat, 0) for cat in CATEGORIES}\n    \n    # Create a DataFrame from the category counts\n    category_df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n    \n    # Create the bar chart\n    ax = category_df.plot(kind='bar', x='Category', y='Count', legend=False)\n    \n    # Set the labels and title\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/920", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draw and return a correlation matrix heatmap for a DataFrame containing numerical columns.\n    The title of the heatmap is set to 'Correlation Matrix'.\n\n    Parameters:\n    - data: pandas DataFrame containing numerical columns.\n\n    Returns:\n    - matplotlib.axes._axes.Axes: The matplotlib Axes object representing the heatmap.\n    \"\"\"\n    # Calculate the correlation matrix\n    corr = data.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n    # Set the title\n    plt.title('Correlation Matrix')\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/921", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data, columns):\n    \"\"\"\n    Normalizes specified columns of a DataFrame using min-max scaling.\n\n    Parameters:\n    - data (pandas.DataFrame): The input DataFrame to be normalized.\n    - columns (list of str): The list of column names to be normalized.\n\n    Returns:\n    - pandas.DataFrame: A new DataFrame with the specified columns normalized between 0 and 1.\n    \"\"\"\n    # Create a copy of the original DataFrame to avoid modifying it directly\n    normalized_data = data.copy()\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Normalize the specified columns\n    for col in columns:\n        if col in normalized_data.columns:\n            # Fit and transform the column\n            normalized_data[col] = scaler.fit_transform(normalized_data[[col]])\n        else:\n            print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping normalization.\")\n\n    return normalized_data"}
{"task_id": "BigCodeBench/922", "solution": "import pandas as pd\nimport re\nSTOPWORDS = set([\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n    \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n    \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n    \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n    \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n    \"don\", \"should\", \"now\"\n])\ndef task_func(data, column):\n    # Remove punctuation and convert to lowercase\n    data[column] = data[column].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()\n    # Split the text into words\n    data[column] = data[column].str.split()\n    # Remove stopwords\n    data[column] = data[column].apply(lambda x: [word for word in x if word not in STOPWORDS])\n    # Join the words back into a string\n    data[column] = data[column].apply(lambda x: ' '.join(x))\n    return data"}
{"task_id": "BigCodeBench/923", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(person_names, email_domains, num_records=5):\n    if len(person_names) < num_records:\n        raise ValueError(\"The number of names provided is less than the number of records requested.\")\n    if not email_domains:\n        raise ValueError(\"No email domains are provided.\")\n    \n    # Create a list to hold the data\n    data = []\n    \n    # Randomly select names and email domains\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        \n        # Create an email by combining the name and domain\n        email = f\"{name.lower().replace(' ', '')}@{domain}\"\n        \n        # Clean the email by replacing \"@\" with \"[at]\"\n        cleaned_email = re.sub(r'@', '[at]', email)\n        \n        # Append the name and cleaned email to the data list\n        data.append({'Name': name, 'Email': cleaned_email})\n    \n    # Create a DataFrame from the data list\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/924", "solution": "import pandas as pd\nimport os\nimport sys\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n    \n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    \n    # Return the cleaned DataFrame\n    return df"}
{"task_id": "BigCodeBench/925", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_size=1000, column_names=['A', 'B', 'C', 'D', 'E'], seed=0):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate a DataFrame with random numeric values between 1 and 100\n    data = np.random.randint(1, 101, size=(data_size, len(column_names)))\n    df = pd.DataFrame(data, columns=column_names)\n    \n    # Replace values less than 10 with -1\n    df[df < 10] = -1\n    \n    return df"}
{"task_id": "BigCodeBench/926", "solution": "import pandas as pd\nimport sqlite3\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n    \n    # Load data from the specified table into a DataFrame\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    \n    # Close the database connection\n    conn.close()\n    \n    # Return the modified DataFrame\n    return df"}
{"task_id": "BigCodeBench/927", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load the CSV file into a Pandas DataFrame\n    df = pd.read_csv(file_path)\n    \n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    \n    # Encode the specified column as a categorical variable using LabelEncoder\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    \n    return df"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of the lowercase English alphabets\n    two_letter_combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Count the frequency of each two-letter combination in the given word\n    word_combinations = [word[i:i+2] for i in range(len(word)-1)]\n    combination_counts = Counter(word_combinations)\n    \n    # Create a dictionary with all possible two-letter combinations and their counts in the word\n    result = {combo: combination_counts.get(combo, 0) for combo in two_letter_combinations}\n    \n    return result"}
{"task_id": "BigCodeBench/929", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(word: str) -> np.ndarray:\n    # Calculate the difference between the ASCII values of each pair of adjacent letters in the input word\n    differences = np.array([ord(word[i+1]) - ord(word[i]) for i in range(len(word)-1)])\n    \n    # Calculate the entropy of the differences\n    entropy = stats.entropy(np.bincount(differences), base=2)\n    \n    return differences, entropy"}
{"task_id": "BigCodeBench/930", "solution": "import random\nimport string\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\ndef task_func(word):\n    if not word.isalpha():\n        raise ValueError(\"Input contains non-letter characters.\")\n    if len(word) < 2:\n        return [''] * len(POSSIBLE_LETTERS)\n    pairs = [word[i:i+2] for i in range(len(word)-1)]\n    random.shuffle(pairs)\n    return pairs[:len(POSSIBLE_LETTERS)]"}
{"task_id": "BigCodeBench/931", "solution": "from collections import defaultdict\nimport re\ndef task_func(word: str) -> dict:\n    # Sanitize the word: remove non-alphabetic characters and convert to lowercase\n    sanitized_word = re.sub(r'[^a-zA-Z]', '', word).lower()\n    \n    # Initialize a defaultdict to store the counts of each two-letter combination\n    two_letter_combinations = defaultdict(int)\n    \n    # Iterate over the sanitized word to find all two-letter combinations\n    for i in range(len(sanitized_word) - 1):\n        # Extract the two-letter combination\n        combination = sanitized_word[i:i+2]\n        # Increment the count for this combination\n        two_letter_combinations[combination] += 1\n    \n    return dict(two_letter_combinations)"}
{"task_id": "BigCodeBench/932", "solution": "from collections import Counter\nimport re\ndef task_func(word: str) -> list:\n    # Clean the word: lowercase and remove non-alphabetic characters\n    cleaned_word = re.sub(r'[^a-zA-Z]', '', word).lower()\n    \n    # Check if the cleaned word has fewer than 2 alphabetic characters\n    if len(cleaned_word) < 2:\n        return []\n    \n    # Generate all possible two-letter combinations\n    two_letter_combinations = [cleaned_word[i:i+2] for i in range(len(cleaned_word)-1)]\n    \n    # Count the frequency of each two-letter combination\n    combination_counts = Counter(two_letter_combinations)\n    \n    # Find the most common two-letter combination\n    most_common = combination_counts.most_common(1)\n    \n    # If there are no combinations, return an empty list\n    if not most_common:\n        return []\n    \n    # Return the result as a list containing a single tuple: (combination, frequency)\n    return [(most_common[0][0], most_common[0][1])]"}
{"task_id": "BigCodeBench/933", "solution": "import string\nimport wordninja\ndef task_func(word):\n    alphabet = string.ascii_lowercase\n    word_list = []\n    for letter in word:\n        if letter in alphabet:\n            position = alphabet.index(letter) + 1\n            word_list.append((letter, position))\n    return word_list"}
{"task_id": "BigCodeBench/934", "solution": "from collections import Counter\nimport hashlib\ndef task_func(word: str) -> dict:\n    # Count the occurrence of each adjacent pair of letters\n    pairs = [word[i:i+2] for i in range(len(word)-1)]\n    pair_counts = Counter(pairs)\n    \n    # Encode the result as an MD5 hash\n    hash_object = hashlib.md5(str(pair_counts).encode())\n    hash_hex = hash_object.hexdigest()\n    \n    return pair_counts, hash_hex"}
{"task_id": "BigCodeBench/935", "solution": "import pandas as pd\nimport string\ndef task_func(word):\n    if not word.islower() or not word.isalpha():\n        raise ValueError(\"Input word must be in lowercase and contain only alphabetic characters.\")\n    \n    alphabet = string.ascii_lowercase\n    positions = [alphabet.index(char) + 1 for char in word]\n    \n    data = {'Letter': list(word), 'Position': positions}\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/936", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\nALPHABET = list(string.ascii_lowercase)\ndef task_func(word):\n    # Initialize a list to hold the positions of each letter in the word\n    positions = []\n    \n    # Iterate through each character in the word\n    for char in word:\n        # Find the position of the character in the alphabet\n        position = ALPHABET.index(char) + 1  # +1 because the alphabet starts at 1\n        # Append the position to the list\n        positions.append(position)\n    \n    # Convert the list of positions to a numpy array\n    positions_array = np.array(positions)\n    \n    # Create a bar chart using matplotlib\n    fig, ax = plt.subplots()\n    ax.bar(range(len(positions)), positions_array)\n    \n    # Set the x-axis labels to the original letters\n    ax.set_xticks(range(len(word)))\n    ax.set_xticklabels(word)\n    \n    # Set the y-axis label to 'Position in Alphabet'\n    ax.set_ylabel('Position in Alphabet')\n    \n    # Set the title to 'Positions of Letters in Word'\n    ax.set_title('Positions of Letters in Word')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/937", "solution": "import re\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove all non-alphanumeric characters\n    cleaned_str = re.sub(r'[^a-zA-Z0-9]', '', input_str)\n    # Convert to lowercase\n    cleaned_str = cleaned_str.lower()\n    # Count the frequency of each character\n    frequency = Counter(cleaned_str)\n    return dict(frequency)"}
{"task_id": "BigCodeBench/938", "solution": "import re\nimport pandas as pd\ndef task_func(input_df):\n    # Define a function to clean the text\n    def clean_text(text):\n        # Remove all special characters, punctuation marks, and spaces\n        return re.sub(r'[^a-zA-Z0-9]', '', text)\n    \n    # Apply the clean_text function to the 'text' column\n    input_df['clean_text'] = input_df['text'].apply(clean_text)\n    \n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].str.len()\n    \n    return input_df"}
{"task_id": "BigCodeBench/939", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path: str) -> list:\n    # List all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n    \n    # Initialize a list to store the new names\n    new_names = []\n    \n    # Iterate over each file\n    for file in files:\n        # Extract the base name and the extension\n        base_name, extension = os.path.splitext(file)\n        \n        # Remove all special characters, punctuation marks, and spaces\n        # Keep only alphanumeric characters\n        new_base_name = re.sub(r'[^a-zA-Z0-9]', '', base_name)\n        \n        # Construct the new file name\n        new_file_name = new_base_name + extension\n        \n        # Rename the file\n        os.rename(file, os.path.join(dir_path, new_file_name))\n        \n        # Append the new name to the list\n        new_names.append(new_file_name)\n    \n    return new_names"}
{"task_id": "BigCodeBench/940", "solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks and spaces from the string\n    cleaned_str = re.sub(r'[^a-zA-Z0-9]', '', input_str)\n    # Tokenize the cleaned string into words\n    words = word_tokenize(cleaned_str)\n    # Count the frequency of each word\n    word_counts = Counter(words)\n    # Return the dictionary with word frequencies\n    return dict(word_counts)"}
{"task_id": "BigCodeBench/941", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> (pd.DataFrame, plt.Axes):\n    \"\"\"\n    Generates and plots a sales forecast starting from a given date, for a specified number of periods and frequency.\n\n    Parameters:\n    - start_date (str): The starting date for the forecast in 'YYYY-MM-DD' format.\n    - periods (int): The number of periods to forecast.\n    - freq (str): The frequency of the forecast, e.g., 'D' for daily, 'M' for monthly.\n    - random_seed (int, optional): The random seed for reproducibility. Defaults to 0.\n\n    Returns:\n    - A tuple containing:\n        1. A DataFrame with columns ['Date', 'Sales'], where 'Date' is the forecast date and 'Sales' are the forecasted sales.\n        2. A matplotlib Axes object for the sales forecast plot.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Create a date range starting from start_date for periods with the given frequency\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random sales data\n    sales = np.random.rand(periods) * 100  # Assuming sales are between 0 and 100\n    \n    # Create a DataFrame\n    forecast_df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    \n    # Plot the sales forecast\n    fig, ax = plt.subplots()\n    ax.plot(forecast_df['Date'], forecast_df['Sales'], marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Forecast')\n    \n    return forecast_df, ax"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with 'Date' and 'Category' columns\n    df = pd.DataFrame({'Date': date_range, 'Category': np.random.choice(categories, periods)})\n    \n    # Generate random sales data\n    df['Sales'] = np.random.randint(100, 1000, periods)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/943", "solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    # Generate a sales time-series\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales = np.random.rand(periods) * 100  # Random sales data\n    sales_series = pd.Series(sales, index=dates)\n    \n    # Decompose the time-series\n    decomposition = seasonal_decompose(sales_series, model=model)\n    \n    # Extract trend, seasonal, and residual components\n    trend = decomposition.trend\n    seasonal = decomposition.seasonal\n    residual = decomposition.resid\n    \n    # Return the components as a dictionary\n    return {'trend': trend, 'seasonal': seasonal, 'residual': residual}"}
{"task_id": "BigCodeBench/944", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate random share prices between 100 and 500\n    prices = np.random.uniform(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Price': prices})\n    \n    # Plot the share prices\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Price'])\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.set_title('Share Prices Over Time')\n    \n    # Return the DataFrame and the plot\n    return df, ax"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # If sales_data is not provided, generate a random sales data\n    if sales_data is None:\n        sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'date': date_range, 'sales': sales_data})\n    \n    # Convert date to numerical format for modeling\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n    \n    # Fit a linear regression model\n    X = df[['date']]\n    y = df['sales']\n    model = LinearRegression().fit(X, y)\n    \n    # Predict future sales\n    future_dates = np.array(range(df['date'].max() + 1, df['date'].max() + periods + 1)).reshape(-1, 1)\n    future_sales = model.predict(future_dates)\n    \n    return future_sales"}
{"task_id": "BigCodeBench/946", "solution": "import numpy as np\nimport pandas as pd\nimport random\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    if seed != 0:\n        random.seed(seed)\n    matrix = np.random.randint(min_val, max_val, size=(rows, cols))\n    df = pd.DataFrame(matrix)\n    return df"}
{"task_id": "BigCodeBench/947", "solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    all_dates = []\n    current_date = start_date\n    while current_date <= end_date:\n        all_dates.append(current_date)\n        current_date += timedelta(days=1)\n    unique_dates = random.sample(all_dates, rows * columns)\n    matrix = np.array(unique_dates).reshape(rows, columns)\n    return matrix"}
{"task_id": "BigCodeBench/948", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    random_matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(random_matrix)\n    return scaled_matrix"}
{"task_id": "BigCodeBench/949", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    random_matrix = np.random.rand(rows, columns)\n    df = pd.DataFrame(random_matrix)\n    return df"}
{"task_id": "BigCodeBench/950", "solution": "import numpy as np\nfrom scipy.linalg import svd\ndef task_func(rows=3, columns=2, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(matrix)\n    return U, s, Vh"}
{"task_id": "BigCodeBench/951", "solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\ndef task_func(mystrings, n_products, seed=0):\n    # Set the seed for reproducibility\n    random.seed(seed)\n    # Initialize an empty list to store product dictionaries\n    products = []\n    # Generate product data\n    for _ in range(n_products):\n        # Generate a random category\n        category = random.choice(CATEGORIES)\n        # Generate a random price based on a normal distribution\n        price = round(np.random.normal(50, 10), 2)\n        # Generate a product name with spaces replaced by underscores\n        product_name = random.choice(mystrings).replace(\" \", \"_\")\n        # Append the product dictionary to the list\n        products.append({'Product Name': product_name, 'Category': category, 'Price': price})\n    # Create a DataFrame from the list of product dictionaries\n    df = pd.DataFrame(products)\n    return df"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(task_list, n_tasks, employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"], seed=None):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    if seed is not None:\n        random.seed(seed)\n    current_date = datetime.today().date()\n    assigned_tasks = []\n    for _ in range(n_tasks):\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        assigned_tasks.append({\n            'Task Name': task.replace(\" \", \"_\"),\n            'Assigned To': employee,\n            'Due Date': current_date\n        })\n    df = pd.DataFrame(assigned_tasks)\n    return df"}
{"task_id": "BigCodeBench/953", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(mystrings, folder_path, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n    file_names = []\n    for name in mystrings:\n        data = np.random.rand(10)\n        plt.bar(range(10), data)\n        plt.title(name)\n        file_name = name.replace(' ', '_') + '.png'\n        file_path = os.path.join(folder_path, file_name)\n        plt.savefig(file_path)\n        file_names.append(file_name)\n        plt.close()\n    return file_names"}
{"task_id": "BigCodeBench/954", "solution": "import random\nimport re\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0 or not vocabulary:\n        raise ValueError(\"n_sentences cannot be negative and vocabulary cannot be empty.\")\n    \n    sentences = []\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choice(vocabulary) for _ in range(10))\n        for word in target_words:\n            pattern = re.compile(re.escape(word), re.IGNORECASE)\n            sentence = pattern.sub(lambda match: match.group(0).replace(' ', '_'), sentence)\n        sentences.append(sentence.lower())\n    \n    return sentences"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Replace spaces with underscores and convert to lowercase\n    modified_text = re.sub(r'\\s+', '_', text.lower())\n    \n    # Split the text into words\n    words = modified_text.split('_')\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n    \n    # Extract words and their frequencies\n    words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency')\n    \n    return ax"}
{"task_id": "BigCodeBench/956", "solution": "import re\nimport string\nimport random\ndef task_func(text: str, seed=None) -> str:\n    if seed is not None:\n        random.seed(seed)\n    \n    # Remove special characters\n    text = re.sub(r'[' + re.escape(string.punctuation) + ']', '', text)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Randomize character casing\n    text = ''.join(random.choice([char.upper(), char.lower()]) for char in text)\n    \n    # Replace spaces, tabs, and newlines with '_', '__', and '___' respectively\n    text = text.replace(' ', '_').replace('\\t', '__').replace('\\n', '___')\n    \n    return text"}
{"task_id": "BigCodeBench/957", "solution": "import string\nimport re\ndef task_func(text: str) -> tuple:\n    # Count the number of words\n    words = re.findall(r'\\b\\w+\\b', text)\n    num_words = len(words)\n    \n    # Count the number of characters excluding whitespace and special characters\n    text = re.sub(r'[' + string.whitespace + string.punctuation + ']', '', text)\n    num_chars = len(text)\n    \n    # Count the number of unique characters\n    unique_chars = set(text)\n    num_unique_chars = len(unique_chars)\n    \n    return (num_words, num_chars, num_unique_chars)"}
{"task_id": "BigCodeBench/958", "solution": "import random\nimport re\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    words = re.findall(r'\\b\\w+\\b', text)\n    scrambled_words = []\n    for word in words:\n        if len(word) > 3:\n            middle = list(word[1:-1])\n            random.shuffle(middle)\n            scrambled_word = word[0] + ''.join(middle) + word[-1]\n        else:\n            scrambled_word = word\n        scrambled_words.append(scrambled_word)\n    scrambled_text = ' '.join(scrambled_words)\n    return scrambled_text"}
{"task_id": "BigCodeBench/959", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    alphabet_lower = string.ascii_lowercase\n    alphabet_upper = string.ascii_uppercase\n    result = []\n    for char in text:\n        if char.islower():\n            new_char = random.choice(alphabet_lower)\n            result.append(new_char)\n        elif char.isupper():\n            new_char = random.choice(alphabet_upper)\n            result.append(new_char)\n        else:\n            result.append(char)\n    return ''.join(result)"}
{"task_id": "BigCodeBench/960", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n    \n    if seed is not None:\n        random.seed(seed)\n    \n    result = []\n    for char in text:\n        if char.isalpha():\n            result.append(random.choice(string.ascii_lowercase))\n        elif char.isdigit():\n            result.append(random.choice(string.digits))\n        elif char.isspace():\n            choice = random.choice([string.digits, string.ascii_lowercase])\n            result.append(random.choice(choice))\n        else:\n            result.append(char)\n    \n    return ''.join(result)"}
{"task_id": "BigCodeBench/961", "solution": "import os\nimport glob\nfrom collections import Counter\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    if not os.path.exists(directory):\n        raise OSError(\"The specified directory does not exist.\")\n    \n    file_counts = Counter()\n    for ext in extensions:\n        pattern = os.path.join(directory, f\"*{ext}\")\n        files = glob.glob(pattern)\n        file_counts[ext] = len(files)\n    \n    if not keep_zero:\n        file_counts = {ext: count for ext, count in file_counts.items() if count > 0}\n    \n    return file_counts"}
{"task_id": "BigCodeBench/962", "solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\ndef task_func(source_directory: str, target_directory: str):\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n    \n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n    \n    if not target_path.exists():\n        target_path.mkdir(parents=True)\n    \n    extensions = [\".txt\", \".docx\", \".xlsx\", \".csv\"]\n    files_to_move = []\n    \n    for ext in extensions:\n        files = source_path.glob(f\"**/*{ext}\")\n        for file in files:\n            files_to_move.append(file)\n    \n    moved_count = 0\n    for file in files_to_move:\n        target_file = target_path / file.name\n        if target_file.exists():\n            base, ext = os.path.splitext(file.name)\n            counter = 1\n            while True:\n                new_name = f\"{base}-{counter}{ext}\"\n                new_target = target_path / new_name\n                if not new_target.exists():\n                    target_file = new_target\n                    break\n                counter += 1\n        shutil.move(file, target_file)\n        moved_count += 1\n    \n    return moved_count"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if source_directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(\"Source directory does not exist\")\n    \n    # Create target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Find files with valid extensions in source_directory\n    files = glob.glob(os.path.join(source_directory, '*'))\n    files = [f for f in files if os.path.splitext(f)[1] in valid_extensions]\n    \n    # Create zip file\n    zip_path = os.path.join(target_directory, zip_name + '.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if source directory exists\n    source_path = Path(source_directory)\n    if not source_path.exists():\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n    \n    # Create target directory if it doesn't exist\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # List of supported file extensions\n    supported_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Counter for successful conversions\n    success_count = 0\n    \n    # Iterate over all files in the source directory\n    for file in source_path.iterdir():\n        if file.is_file():\n            # Check if the file has a supported extension\n            if file.suffix.lower() in supported_extensions:\n                # Determine the target file path\n                target_file = target_path / file.name.replace(file.suffix, '.csv')\n                \n                # Read the file based on its extension\n                if file.suffix.lower() == '.txt':\n                    with open(file, 'r', encoding='utf-8') as f:\n                        text = f.read()\n                elif file.suffix.lower() == '.docx':\n                    doc = docx.Document(file)\n                    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n                elif file.suffix.lower() == '.xlsx':\n                    df = pd.read_excel(file)\n                    text = df.to_string(index=False)\n                elif file.suffix.lower() == '.csv':\n                    with open(file, 'r', encoding='utf-8') as f:\n                        reader = csv.reader(f)\n                        text = '\\n'.join([','.join(row) for row in reader])\n                \n                # Write the text to a CSV file\n                with open(target_file, 'w', newline='', encoding='utf-8') as f:\n                    writer = csv.writer(f)\n                    writer.writerow(['Text'])\n                    writer.writerow([text])\n                \n                # Increment the success counter\n                success_count += 1\n    \n    return success_count"}
{"task_id": "BigCodeBench/965", "solution": "import os\nimport re\nimport shutil\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    if not os.path.exists(source_directory) or not os.path.isdir(source_directory):\n        return 0\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    count = 0\n    for filename in os.listdir(source_directory):\n        if re.match(pattern, filename):\n            shutil.move(os.path.join(source_directory, filename), os.path.join(target_directory, filename))\n            count += 1\n    return count"}
{"task_id": "BigCodeBench/966", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(df: pd.DataFrame) -> tuple:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise ValueError(\"DataFrame contains non-numeric data\")\n    \n    # Calculate cumulative sum for each column, ignoring NaNs\n    cum_sum_df = df.cumsum(axis=0, skipna=True)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    cum_sum_df.plot(kind='bar', ax=ax)\n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    ax.legend(title='Columns')\n    \n    return cum_sum_df, fig"}
{"task_id": "BigCodeBench/967", "solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    \"\"\"\n    Calculates and plots both a given function and its cumulative integral over a specified range, \n    using a linearly spaced range of x-values. The plot includes a legend and labels for the x and y axes \n    that include the function's name.\n\n    Parameters:\n    - func: The function to be plotted and integrated.\n    - x_range: A tuple (start, end) defining the range of x-values.\n    - num_points: The number of points to use for the x-values.\n\n    Returns:\n    - matplotlib.axes.Axes: The Axes object containing the plots of the function and its integral.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(x_range[0], x_range[1], num_points)\n    \n    # Calculate y values for the function\n    y = func(x)\n    \n    # Calculate the cumulative integral\n    integral, _ = integrate.cumulative_trapezoid(y, x, initial=0)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot the function\n    ax.plot(x, y, label=f'f(x) = {func.__name__}(x)')\n    \n    # Plot the integral\n    ax.plot(x, integral, label='\u222bf(x) dx')\n    \n    # Add legend\n    ax.legend()\n    \n    # Label the axes\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/968", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check if there are any numeric columns\n    numeric_cols = data.select_dtypes(include=[np.number]).columns\n    if not numeric_cols:\n        raise ValueError(\"No numeric columns present\")\n    \n    # Calculate cumulative sum for each numeric column\n    cumulative_sums = data[numeric_cols].cumsum()\n    \n    # Create a heatmap using seaborn\n    heatmap = sns.heatmap(cumulative_sums, annot=True, fmt=\".0f\")\n    \n    # Return the Axes object of the heatmap\n    return heatmap"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if df.isnull().any().any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    scaler = MinMaxScaler()\n    for col in df.columns:\n        df[col] = scaler.fit_transform(df[[col]]).flatten()\n    return df"}
{"task_id": "BigCodeBench/970", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data: np.ndarray) -> plt.Axes:\n    if not np.issubdtype(data.dtype, np.number):\n        raise TypeError(\"Input array must contain only numeric values.\")\n    if np.any(data < 0) or np.any(np.isnan(data)):\n        raise ValueError(\"Input array must not contain negative numbers or NaNs.\")\n    \n    sorted_data = np.sort(data)\n    cumulative_prob = np.cumsum(sorted_data) / np.sum(sorted_data)\n    \n    ax = plt.axes()\n    ax.plot(cumulative_prob, marker='o', linestyle='-')\n    ax.set_title(\"Cumulative Probability Plot\")\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Cumulative Probability\")\n    \n    return ax"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # List all entries in the directory\n    entries = os.listdir(directory_path)\n    \n    # Initialize an empty list to store the results\n    result = []\n    \n    # Iterate over each entry\n    for entry in entries:\n        # Construct the full path of the entry\n        full_path = os.path.join(directory_path, entry)\n        \n        # Check if the entry is a file\n        if os.path.isfile(full_path):\n            # Get file size in bytes\n            file_size = os.path.getsize(full_path)\n            \n            # Get creation time\n            # Note: os.path.getctime is used for creation time, but it's not universally available\n            # For more consistent behavior, we can use os.stat to get the creation time\n            try:\n                creation_time = os.path.getctime(full_path)\n            except AttributeError:\n                # If getctime is not available, use modification time as a fallback\n                creation_time = os.path.getmtime(full_path)\n            \n            # Get modification time\n            modification_time = os.path.getmtime(full_path)\n            \n            # Convert times to datetime objects and then to ISO format\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            \n            # Append the tuple to the result list\n            result.append((entry, file_size, creation_time_iso, modification_time_iso))\n    \n    return result"}
{"task_id": "BigCodeBench/972", "solution": "import pathlib\nimport os\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    invalid_chars = '<>:\"|?*'\n    if any(char in path for char in invalid_chars):\n        raise ValueError(\"Invalid characters in path\")\n    path = path.replace('\\\\', '/')\n    return path.split(delimiter)"}
{"task_id": "BigCodeBench/973", "solution": "import os\nimport shutil\ndef task_func(path, delimiter=\"/\"):\n    if not isinstance(path, str):\n        raise ValueError(\"The path must be a string.\")\n    if not path:\n        raise ValueError(\"The path cannot be empty.\")\n    if delimiter not in path:\n        raise ValueError(\"The delimiter is not present in the path.\")\n    \n    path_components = path.split(delimiter)\n    if not all(os.path.exists(component) for component in path_components):\n        raise FileNotFoundError(\"One or more path components do not exist.\")\n    \n    result = []\n    for component in path_components:\n        total, used, free = shutil.disk_usage(component)\n        result.append((component, {'total': total, 'used': used, 'free': free}))\n    \n    return result"}
{"task_id": "BigCodeBench/974", "solution": "import shutil\nimport pathlib\nfrom typing import List, Tuple\ndef task_func(source_path: str, destination_path: str) -> Tuple[str, List[str]]:\n    source = pathlib.Path(source_path)\n    destination = pathlib.Path(destination_path)\n    \n    if not source.is_dir():\n        raise ValueError(\"Source path does not exist or is not a directory.\")\n    \n    files = [f.name for f in source.iterdir() if f.is_file()]\n    \n    for file in files:\n        shutil.copy(source / file, destination / file)\n    \n    return source.name, files"}
{"task_id": "BigCodeBench/975", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    df = df[columns]\n    return df"}
{"task_id": "BigCodeBench/976", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    if len(records.shape) != 2:\n        raise ValueError(\"Input records must be 2D array.\")\n    \n    np.random.seed(random_seed)\n    feature_names = [f\"f{i+1}\" for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n    \n    scaler = StandardScaler()\n    scaled_records = scaler.fit_transform(records)\n    \n    df = pd.DataFrame(scaled_records, columns=feature_names)\n    return df"}
{"task_id": "BigCodeBench/977", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(array, features=None, seed=None):\n    if not isinstance(array, np.ndarray) or len(array.shape) != 2:\n        raise ValueError(\"Input 'array' must be a 2-dimensional numpy array.\")\n    if array.size == 0:\n        raise ValueError(\"Input 'array' cannot be empty.\")\n    if features is not None:\n        if not isinstance(features, list) or len(features) != array.shape[1]:\n            raise ValueError(\"If 'features' is provided, it must be a list of strings with the same length as the number of columns in 'array'.\")\n    else:\n        features = [str(i) for i in range(1, array.shape[1] + 1)]\n    \n    if seed is not None:\n        np.random.seed(seed)\n    shuffled_indices = np.random.permutation(array.shape[1])\n    shuffled_array = array[:, shuffled_indices]\n    \n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(shuffled_array, xticklabels=features, yticklabels=False, cmap='viridis')\n    plt.show()\n    \n    return heatmap.axes"}
{"task_id": "BigCodeBench/978", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(array, seed=None):\n    if not isinstance(array, np.ndarray) or array.ndim != 2:\n        raise ValueError(\"Input array must be a 2D numpy array.\")\n    \n    if array.size == 0:\n        return pd.DataFrame()\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Shuffle columns randomly\n    shuffled_array = array[:, np.random.permutation(array.shape[1])]\n    \n    # Perform PCA to reduce dimensionality to 2 principal components\n    pca = PCA(n_components=2)\n    pca.fit(shuffled_array)\n    principal_components = pca.transform(shuffled_array)\n    \n    # Create a DataFrame with columns 'PC1' and 'PC2'\n    df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n    \n    return df"}
{"task_id": "BigCodeBench/979", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    # Shuffle the columns of the feature array\n    np.random.seed(seed)\n    shuffled_indices = np.random.permutation(feature_array.shape[1])\n    shuffled_feature_array = feature_array[:, shuffled_indices]\n    \n    # Convert to DataFrame for better handling\n    df = pd.DataFrame(shuffled_feature_array, columns=feature_names)\n    df[target_name] = target_array\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df[target_name], test_size=0.2, random_state=seed)\n    \n    # Initialize and train the Random Forest Classifier\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy: {accuracy}\")\n    \n    return clf"}
{"task_id": "BigCodeBench/980", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"No numeric columns present in the DataFrame\")\n    \n    scaler = StandardScaler()\n    df_standardized = df.copy()\n    df_standardized[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    \n    corr_matrix = df_standardized[numeric_cols].corr()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n    \n    return df_standardized, plt.gcf()"}
{"task_id": "BigCodeBench/981", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(start_date, end_date, num_series, seed=None):\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date\")\n    if num_series < 1:\n        raise ValueError(\"num_series must be at least 1\")\n    \n    if seed is not None:\n        random.seed(seed)\n    \n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    data = {f'series_{i}': [random.randint(0, 100) for _ in date_range] for i in range(1, num_series + 1)}\n    df = pd.DataFrame(data, index=date_range)\n    \n    plt.figure(figsize=(10, 6))\n    for i in range(1, num_series + 1):\n        plt.plot(df.index, df[f'series_{i}'], label=f'series_{i}')\n    plt.title('Random Time Series')\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.show()\n    \n    return df, plt.gca()"}
{"task_id": "BigCodeBench/982", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    \"\"\"\n    Plots a histogram for a specified column of a pandas DataFrame and overlays it with a fitted normal distribution curve.\n\n    Parameters:\n    - df: pandas DataFrame containing the data.\n    - column: string, the name of the column to plot.\n    - bins: int, the number of bins for the histogram.\n    - density: bool, if True, the histogram height shows a probability density.\n    - alpha: float, the opacity of the histogram.\n    - color: string, the color of the histogram bars.\n    - seed: int, seed for random number generation.\n\n    Returns:\n    - matplotlib.axes._axes.Axes: The matplotlib Axes object with the plot.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Extract the data from the DataFrame\n    data = df[column]\n    \n    # Plot the histogram\n    ax = data.hist(bins=bins, density=density, alpha=alpha, color=color)\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n    \n    # Generate x values for the normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    \n    # Plot the normal distribution curve\n    plt.plot(x, p, 'k', linewidth=2)\n    \n    return ax"}
{"task_id": "BigCodeBench/983", "solution": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    \n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n    \n    return (covariance_df, pair_plot)"}
{"task_id": "BigCodeBench/984", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, x_column, y_column):\n    \"\"\"\n    Draws a scatter plot for the specified columns from a pandas DataFrame and fits a linear regression model to the data.\n    After plotting the scatterplot, this function overlays the predicted regression line on top in red on the same Axes.\n    The function should output with:\n        matplotlib.axes._axes.Axes: The Axes object containing the scatter plot and the linear regression line.\n    \"\"\"\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n\n    # Plot the scatter plot\n    ax.scatter(df[x_column], df[y_column], color='blue', label='Data points')\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(df[[x_column]], df[y_column])\n\n    # Generate x values for the regression line\n    x_values = df[x_column].values\n    x_values = x_values.reshape(-1, 1)\n\n    # Predict y values using the model\n    y_pred = model.predict(x_values)\n\n    # Plot the regression line\n    ax.plot(df[x_column], y_pred, color='red', label='Regression line')\n\n    # Add labels and legend\n    ax.set_xlabel(x_column)\n    ax.set_ylabel(y_column)\n    ax.legend()\n\n    # Show the plot\n    plt.show()\n\n    return ax"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Malformed JSON data\")\n    \n    if not data:\n        raise ValueError(\"Empty JSON data\")\n    \n    country_populations = []\n    for item in data:\n        if not isinstance(item, dict) or len(item) != 2:\n            raise ValueError(\"Invalid JSON format\")\n        country, population = item[\"country\"], item[\"population\"]\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name\")\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Invalid population value\")\n        country_populations.append({\"Country\": country, \"Population\": math.floor(population)})\n    \n    df = pd.DataFrame(country_populations)\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    output_path = os.path.join(output_dir, file_name)\n    df.to_csv(output_path, index=False)\n    \n    return output_path, df"}
{"task_id": "BigCodeBench/986", "solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef task_func(json_data: str, key_path: list):\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data\")\n    \n    current_data = data\n    for key in key_path:\n        if isinstance(current_data, dict) and key in current_data:\n            current_data = current_data[key]\n        else:\n            raise KeyError(f\"Key not found: {key}\")\n    \n    if not isinstance(current_data, list):\n        raise ValueError(\"The data at the specified path is not a list\")\n    \n    numeric_data = []\n    for item in current_data:\n        if isinstance(item, (int, float)):\n            numeric_data.append(item)\n        elif isinstance(item, str):\n            try:\n                numeric_data.append(float(item))\n            except ValueError:\n                pass\n        else:\n            pass  # Ignore non-numeric data types\n    \n    if not numeric_data:\n        raise ValueError(\"No numeric data found\")\n    \n    df = pd.DataFrame(numeric_data, columns=['Values'])\n    plt.figure(figsize=(10,6))\n    sns.boxplot(x=df['Values'])\n    plt.title('Boxplot of Numeric Data')\n    plt.show()"}
{"task_id": "BigCodeBench/987", "solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(json_data: str, data_key: str):\n    try:\n        data = json.loads(json_data)\n        if data_key not in data:\n            raise KeyError(f\"Key '{data_key}' not found in the data.\")\n        original_data = pd.Series(data[data_key], dtype='float64')\n        if original_data.empty:\n            return original_data, None, None\n        scaler = MinMaxScaler()\n        normalized_data = scaler.fit_transform(original_data.values.reshape(-1,1))\n        normalized_data = pd.Series(normalized_data.flatten(), dtype='float64')\n        fig, ax = plt.subplots()\n        ax.plot(original_data, label='Original Data')\n        ax.plot(normalized_data, label='Normalized Data')\n        ax.set_title('Comparison of Original and Normalized Data')\n        ax.set_xlabel('Index')\n        ax.set_ylabel('Value')\n        ax.legend()\n        plt.show()\n        return original_data, normalized_data, ax\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data.\")\n    except KeyError as e:\n        raise e\n    except Exception as e:\n        raise e"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if directory exists and is a directory\n    dir_path = Path(dir_path)\n    if not dir_path.is_dir():\n        raise FileNotFoundError(f\"The specified directory does not exist or is not a directory: {dir_path}\")\n\n    # Deduplicate predicates\n    unique_predicates = list(set(predicates))\n\n    # Check if any valid predicates are provided\n    if not unique_predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n\n    # Evaluate each item in the directory against the predicates\n    results = {}\n    for item in dir_path.iterdir():\n        item_name = item.name\n        for predicate in unique_predicates:\n            if re.match(predicate, item_name):\n                results[item_name] = True\n                break\n        else:\n            results[item_name] = False\n\n    return results"}
{"task_id": "BigCodeBench/989", "solution": "import random\nimport string\ndef task_func(length: int, predicates: list, seed: int = None):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    if seed is not None:\n        random.seed(seed)\n    characters = string.ascii_letters + string.digits + string.punctuation\n    text = ''.join(random.choice(characters) for _ in range(length))\n    characteristics = {}\n    for predicate in predicates:\n        if predicate == 'isalnum':\n            characteristics['isalnum'] = text.isalnum()\n        elif predicate == 'isalpha':\n            characteristics['isalpha'] = text.isalpha()\n        elif predicate == 'isdigit':\n            characteristics['isdigit'] = text.isdigit()\n        elif predicate == 'islower':\n            characteristics['islower'] = text.islower()\n        elif predicate == 'isupper':\n            characteristics['isupper'] = text.isupper()\n        elif predicate == 'istitle':\n            characteristics['istitle'] = text.istitle()\n        elif predicate == 'isspace':\n            characteristics['isspace'] = text.isspace()\n        else:\n            raise KeyError(f\"Unknown predicate: {predicate}\")\n    return (text, characteristics)"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hex to bytes\n    try:\n        bytes_data = binascii.unhexlify(hex_string)\n    except binascii.Error:\n        return {'hex': hex_string, 'base64': '', 'utf-8': '', 'utf-16': '', 'utf-32': '', 'ASCII': '', 'URL': '', 'ROT13': ''}\n\n    # Convert bytes to UTF-8 string\n    try:\n        utf8_string = bytes_data.decode('utf-8')\n    except UnicodeDecodeError:\n        return {'hex': hex_string, 'base64': '', 'utf-8': '', 'utf-16': '', 'utf-32': '', 'ASCII': '', 'URL': '', 'ROT13': ''}\n\n    # Encode UTF-8 string to various formats\n    hex_encoded = hex_string\n    base64_encoded = base64.b64encode(bytes_data).decode('utf-8')\n    utf16_encoded = utf8_string.encode('utf-16').hex()\n    utf32_encoded = utf8_string.encode('utf-32').hex()\n    try:\n        ascii_encoded = utf8_string.encode('ascii').decode('ascii')\n    except UnicodeEncodeError:\n        ascii_encoded = 'Not representable in ASCII'\n    url_encoded = urllib.parse.quote(utf8_string)\n    rot13_encoded = codecs.encode(utf8_string, 'rot_13')\n\n    # Return the results in a dictionary\n    return {\n        'hex': hex_encoded,\n        'base64': base64_encoded,\n        'utf-8': utf8_string,\n        'utf-16': utf16_encoded,\n        'utf-32': utf32_encoded,\n        'ASCII': ascii_encoded,\n        'URL': url_encoded,\n        'ROT13': rot13_encoded\n    }"}
{"task_id": "BigCodeBench/991", "solution": "import binascii\nimport string\nimport random\ndef task_func(length):\n    # Generate a random hexadecimal string of the given length\n    hex_string = ''.join(random.choice(string.hexdigits) for _ in range(length))\n    # Attempt to decode the hexadecimal string in ASCII\n    try:\n        ascii_string = binascii.unhexlify(hex_string).decode('ascii')\n    except binascii.Error:\n        # If decoding fails, return an empty string\n        ascii_string = ''\n    # Return the decoded ASCII string\n    return ascii_string"}
{"task_id": "BigCodeBench/992", "solution": "import sys\nimport sqlite3\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(database)\n    c = conn.cursor()\n    \n    # Create the table if it doesn't exist\n    c.execute('''CREATE TABLE IF NOT EXISTS paths (path TEXT PRIMARY KEY)''')\n    \n    # Check if the path already exists in the database\n    c.execute(\"SELECT * FROM paths WHERE path = ?\", (path_to_append,))\n    if c.fetchone() is None:\n        # Insert the path into the database\n        c.execute(\"INSERT INTO paths (path) VALUES (?)\", (path_to_append,))\n        conn.commit()\n    \n    # Close the connection\n    conn.close()\n    \n    # Return the path that was appended and inserted\n    return path_to_append"}
{"task_id": "BigCodeBench/993", "solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate the lengths of the words\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    plt.figure(figsize=(10, 6))\n    plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), align='left', rwidth=0.8, color='skyblue', edgecolor='black')\n    plt.title('Distribution of Word Lengths')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    \n    # Calculate the KDE of word lengths\n    kde = gaussian_kde(word_lengths)\n    x = np.linspace(min(word_lengths), max(word_lengths), 100)\n    plt.plot(x, kde(x), color='red', label='KDE')\n    plt.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return plt.gca()"}
{"task_id": "BigCodeBench/994", "solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, csv_file_path: str) -> list:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Error fetching URL: {e}\")\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    containers = soup.find_all('div', class_='container')\n    data = []\n\n    for container in containers:\n        title = container.find('h1').get_text(strip=True) if container.find('h1') else 'No Title'\n        date = container.find('span', class_='date').get_text(strip=True) if container.find('span', class_='date') else 'No Date'\n        author = container.find('span', class_='author').get_text(strip=True) if container.find('span', class_='author') else 'No Author'\n        data.append((title, date, author))\n\n    df = pd.DataFrame(data, columns=['Title', 'Date', 'Author'])\n    df.to_csv(csv_file_path, index=False)\n\n    return data"}
{"task_id": "BigCodeBench/995", "solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n    \n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise ValueError(f\"Error reading the CSV file: {e}\")\n    \n    data = df.iloc[:, 0].dropna().astype(float)\n    \n    if data.empty:\n        mean = np.nan\n        median = np.nan\n    else:\n        mean = data.mean()\n        median = data.median()\n    \n    plt.figure()\n    plt.plot(data, label='Data')\n    plt.title('Data Visualization')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.savefig(plot_path)\n    plt.close()\n    \n    return mean, median, plot_path"}
{"task_id": "BigCodeBench/996", "solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return None\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.title.string if soup.title else None\n\n    data = {\"title\": title}\n    json_data = json.dumps(data) + \"\\n\"\n\n    try:\n        with open(file_name, 'a') as file:\n            file.write(json_data)\n    except IOError as e:\n        print(f\"Error writing to file: {e}\")\n        return None\n\n    return file_name"}
{"task_id": "BigCodeBench/997", "solution": "import urllib.request\nimport os\nimport zipfile\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\ndef task_func(url):\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n    \n    # Download the zip file from the specified URL\n    urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n    \n    # Extract the zip file to the target directory\n    with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(TARGET_DIR)\n    \n    # Delete the local zip file after extraction\n    os.remove(TARGET_ZIP_FILE)\n    \n    # Return the path of the extracted directory\n    return os.path.abspath(TARGET_DIR)"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate MD5 checksum\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        md5 = hashlib.md5(f.read()).hexdigest()\n\n    # Validate MD5 checksum\n    if md5 != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match the expected value.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Extraction failed: {e}\")\n        return False\n\n    return True"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Open the CSV file and read its contents\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            \n            # Check if the specified column_name exists in the CSV file\n            if column_name not in reader.fieldnames:\n                # Delete the downloaded file\n                os.remove(csv_file_path)\n                # Raise a ValueError with the appropriate message\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count the occurrences of each value in the specified column\n            value_counts = collections.Counter()\n            for row in reader:\n                value = row.get(column_name)\n                if value is not None:\n                    value_counts[value] += 1\n        \n        # Delete the downloaded file\n        os.remove(csv_file_path)\n        \n        # Return the dictionary of value counts\n        return dict(value_counts)\n    \n    except Exception as e:\n        # Delete the downloaded file if it exists\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n        # Re-raise the exception\n        raise e"}
{"task_id": "BigCodeBench/1000", "solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\nTARGET_JSON_FILE = \"downloaded_file.json\"\ndef task_func(url):\n    # Download the JSON file\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n    \n    # Read the JSON file\n    with open(TARGET_JSON_FILE, 'r') as file:\n        data = json.load(file)\n    \n    # Convert JSON to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Delete the temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n    \n    return df"}
{"task_id": "BigCodeBench/1001", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path: str):\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_file_path)\n    \n    # Normalize the 'column1' column\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n    \n    # Create the plot\n    plt.plot(df.index, df['column1'])\n    \n    # Set the title with formatted strings\n    plt.title(f\"{'Plot Title':<20} : {'Normalized Column 1':<20}\")\n    \n    # Set the x-label with formatted strings\n    plt.xlabel(f\"{'Index':<20} : {'Normalized Value':<20}\")\n    \n    # Set the y-label with formatted strings\n    plt.ylabel(f\"{'Frequency':<20} : {'Normalized Value':<20}\")\n    \n    # Return the matplotlib.axes.Axes object\n    return plt.gca()"}
{"task_id": "BigCodeBench/1002", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column_name=\"target_column\"):\n    # Convert JSON data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n    \n    # Convert non-numeric columns to categorical type and then to numeric codes\n    if not pd.api.types.is_numeric_dtype(df[column_name]):\n        df[column_name] = df[column_name].astype('category').cat.codes\n    \n    # Plot histogram\n    ax = df[column_name].plot.hist(bins=10, title=f'Histogram of {column_name}', xlabel=column_name)\n    \n    return df, ax"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch the XML file from {url}: {e.reason}\")\n\n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the root element is 'root' and contains 'item' elements\n    if root.tag != 'root':\n        raise ValueError(\"XML structure does not match expected format. Root element should be 'root'.\")\n\n    items = root.findall('item')\n    if not items:\n        raise ValueError(\"No 'item' elements found in the XML.\")\n\n    # Extract data from 'item' elements\n    data = []\n    for item in items:\n        row = {}\n        for child in item:\n            row[child.tag] = child.text\n        data.append(row)\n\n    # Create DataFrame from the extracted data\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the specified URL\n    with urllib.request.urlopen(url) as response:\n        text = response.read().decode('utf-8')\n    \n    # Use a basic regular expression to find all words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    most_common = word_counts.most_common(10)\n    \n    # Extract words and their counts for plotting\n    words, counts = zip(*most_common)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Words')\n    \n    # Return the word counts and the axes object\n    return word_counts, ax\nurl = 'https://raw.githubusercontent.com/yourusername/yourrepo/main/yourfile.txt'"}
{"task_id": "BigCodeBench/1005", "solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\ndef task_func(url: str, save_path: str = \"downloaded_file.zip\", extract_path: str = \"extracted_files\") -> str:\n    try:\n        # Download the ZIP file\n        urllib.request.urlretrieve(url, save_path)\n    except urllib.error.URLError as e:\n        return f\"URL Error: {e.reason}\"\n    except Exception as e:\n        return f\"An error occurred while downloading the file: {e}\"\n\n    try:\n        # Extract the ZIP file\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n    except zipfile.BadZipFile:\n        os.remove(save_path)\n        return \"Error: The downloaded file is not a valid ZIP file.\"\n    except Exception as e:\n        os.remove(save_path)\n        return f\"An error occurred while extracting the ZIP file: {e}\"\n\n    # Delete the ZIP file\n    os.remove(save_path)\n\n    return extract_path"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Check if the download_path exists, if not, create it\n        if not os.path.exists(download_path):\n            os.makedirs(download_path)\n        \n        # Send a GET request to the URL\n        response = requests.get(url)\n        \n        # Check if the request was successful\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Check if the content type is 'application/zip'\n        if 'application/zip' not in response.headers.get('Content-Type', ''):\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Define the path to save the downloaded ZIP file\n        zip_path = os.path.join(download_path, 'downloaded.zip')\n        \n        # Save the content to a file\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n        \n        # Try to open the ZIP file\n        try:\n            with ZipFile(zip_path, 'r') as zip_ref:\n                # Extract all contents to the download_path\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            os.remove(zip_path)  # Remove the corrupted ZIP file\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        # Remove the ZIP file after extraction\n        os.remove(zip_path)\n        \n        # Return the path to the directory containing the extracted contents\n        return download_path\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1007", "solution": "import requests\nimport pandas as pd\ndef task_func(url: str) -> pd.DataFrame:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n        data = response.json()\n        df = pd.DataFrame(data)\n        return df\n    except requests.RequestException as e:\n        raise SystemError(f\"Network-related issue: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Invalid JSON format: {e}\")"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.exceptions.HTTPError as e:\n        raise ValueError(f\"HTTP request failed: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', id=table_id)\n\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()\n\n    headers = []\n    for th in rows[0].find_all('th'):\n        headers.append(th.text.strip())\n\n    data = []\n    for row in rows[1:]:\n        cells = row.find_all(['th', 'td'])\n        row_data = []\n        for cell in cells:\n            row_data.append(cell.text.strip())\n        data.append(row_data)\n\n    df = pd.DataFrame(data, columns=headers)\n    return df"}
{"task_id": "BigCodeBench/1009", "solution": "import xml.etree.ElementTree as ET\nimport csv\nimport os\ndef task_func(xml_content, output_csv_path):\n    try:\n        # Parse the XML content\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        # Raise an exception if the XML content is malformed\n        raise Exception(f\"Error parsing XML: {e}\")\n\n    # Extract data from XML\n    data = []\n    for child in root:\n        row = {}\n        for subchild in child:\n            row[subchild.tag] = subchild.text\n        data.append(row)\n\n    # Write data to CSV\n    try:\n        with open(output_csv_path, mode='w', newline='') as file:\n            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n            writer.writeheader()\n            for row in data:\n                writer.writerow(row)\n    except IOError as e:\n        # Raise an exception if there is an issue with writing to the CSV file\n        raise Exception(f\"Error writing to CSV: {e}\")"}
{"task_id": "BigCodeBench/1010", "solution": "import requests\nfrom PIL import Image\nimport io\ndef task_func(url):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.exceptions.MissingSchema:\n        raise ValueError(\"Invalid URL\")\n    except requests.exceptions.Timeout:\n        raise ValueError(\"URL cannot be reached within 5 seconds\")\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n    if 'image' not in response.headers.get('Content-Type', ''):\n        raise ValueError(\"The content fetched from the URL is not a valid image format\")\n\n    image = Image.open(io.BytesIO(response.content))\n    return image"}
{"task_id": "BigCodeBench/1011", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(csv_file_path)\n    \n    # Group the DataFrame by col1_name and calculate the mean of col2_name for each group\n    grouped = df.groupby(col1_name)[col2_name].mean().reset_index()\n    \n    # Create a bar plot\n    ax = plt.bar(grouped[col1_name], grouped[col2_name])\n    \n    # Set the title of the plot\n    plt.title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    \n    # Set the labels for the x and y axes\n    plt.xlabel(col1_name)\n    plt.ylabel(f\"Mean of {col2_name}\")\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    # Create download directory if it doesn't exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    # Create unzip directory if it doesn't exist\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    try:\n        # Download the zip file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception occurred - {}\".format(e), [])\n    \n    try:\n        # Save the zip file\n        zip_path = DOWNLOAD_DIR / filename\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n    except IOError as e:\n        return (\"Error: File-related exception occurred - {}\".format(e), [])\n    \n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        # List files in the unzipped directory\n        files = [f.name for f in zip_ref.infolist()]\n        return (\"Success\", files)\n    except zipfile.BadZipFile as e:\n        return (\"Error: Invalid zip file - {}\".format(e), [])\n    except Exception as e:\n        return (\"Error: An unexpected error occurred - {}\".format(e), [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n        return 0\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find all <a> tags\n    links = soup.find_all('a')\n    \n    # Set to store unique absolute URLs\n    unique_links = set()\n    \n    # Iterate through each <a> tag\n    for link in links:\n        # Extract the href attribute\n        href = link.get('href')\n        \n        # If href is not None, join it with the base URL to get the absolute URL\n        if href is not None:\n            absolute_url = urljoin(base_url, href)\n            unique_links.add(absolute_url)\n    \n    # Write the unique absolute URLs to a CSV file\n    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        for url in unique_links:\n            writer.writerow([url])\n    \n    # Return the number of unique absolute links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1014", "solution": "import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"The provided API URL must be a string.\")\n    \n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n        data = response.json()\n    except requests.exceptions.HTTPError as e:\n        raise e\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"An error occurred while fetching data from the API: {e}\")\n    \n    if not data:\n        return pd.DataFrame(), None\n    \n    df = pd.DataFrame(data)\n    if not df.empty:\n        axes = df.plot()\n        return df, axes\n    else:\n        return df, None"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content from the URL\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    try:\n        # Parse the HTML content\n        tree = html.fromstring(response.content)\n        # Find the table in the HTML\n        table = tree.xpath('//table')[0]\n    except Exception as e:\n        print(f\"Error parsing the HTML: {e}\")\n        return 0\n\n    try:\n        # Convert the table to a pandas DataFrame\n        df = pd.read_html(html.tostring(table), header=0)[0]\n    except Exception as e:\n        print(f\"Error converting table to DataFrame: {e}\")\n        return 0\n\n    if df.empty:\n        print(\"The table is empty.\")\n        return 0\n\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        # Create a cursor object\n        cursor = conn.cursor()\n        # Drop the existing table if it exists\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n        # Create a new table with the same columns as the DataFrame\n        df.to_sql(\"my_table\", conn, if_exists=\"append\", index=False)\n        # Commit the transaction\n        conn.commit()\n        # Close the connection\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        print(f\"Error connecting to or writing to the database: {e}\")\n        return 0\n\n    return len(df.index)"}
{"task_id": "BigCodeBench/1016", "solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading the image: {e}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n        image = image.convert('L')\n    except IOError as e:\n        raise IOError(f\"Error opening or processing the image: {e}\")\n\n    grayscale_array = np.array(image)\n    plt.figure()\n    plt.hist(grayscale_array.ravel(), bins=256, color='gray')\n    plt.title('Grayscale Histogram')\n    plt.xlabel('Grayscale Value')\n    plt.ylabel('Frequency')\n    return plt.gca()"}
{"task_id": "BigCodeBench/1017", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    try:\n        # Read the CSV file into a pandas DataFrame\n        df = pd.read_csv(csv_file_path)\n        \n        # Check if the target_column exists in the DataFrame\n        if target_column not in df.columns:\n            raise ValueError(f\"The specified target_column '{target_column}' is not found in the CSV file.\")\n        \n        # Separate features and target variable\n        X = df.drop(columns=[target_column])\n        y = df[target_column]\n        \n        # Split the data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n        \n        # Initialize and train the Random Forest classifier\n        clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n        clf.fit(X_train, y_train)\n        \n        # Make predictions on the test set\n        y_pred = clf.predict(X_test)\n        \n        # Generate the classification report\n        report = classification_report(y_test, y_pred)\n        \n        return report\n    except Exception as e:\n        return str(e)"}
{"task_id": "BigCodeBench/1018", "solution": "from bs4 import BeautifulSoup\nimport requests\nURL = \"http://example.com\"\ndef task_func(url=URL, from_encoding=\"cp1251\", use_lxml=False):\n    if not url:\n        return None\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        print(f\"Network error: {e}\")\n        return None\n    try:\n        content = response.content.decode(from_encoding)\n    except UnicodeDecodeError as e:\n        print(f\"Decoding error: {e}\")\n        return None\n    try:\n        if use_lxml:\n            soup = BeautifulSoup(content, 'lxml')\n        else:\n            soup = BeautifulSoup(content, 'html.parser')\n        return soup\n    except Exception as e:\n        print(f\"BeautifulSoup parsing error: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nimport io\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n        # Extract text using OCR\n        text = pytesseract.image_to_string(image)\n        # Convert the text encoding\n        try:\n            text = text.encode(from_encoding).decode(to_encoding)\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\") from e\n        return text\n    except:\n        # If OCR extraction fails, try to get the image comment\n        try:\n            with open(filename, 'rb') as f:\n                image_data = f.read()\n            # Convert the image comment to the target encoding\n            try:\n                comment = image_data.decode(from_encoding)\n                comment = comment.encode(from_encoding).decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\") from e\n            return comment\n        except:\n            # If both OCR extraction and comment processing fail, return an empty string\n            return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Failed to retrieve data from {url}: {e}\")\n\n    content = response.content\n    if not content:\n        return {}\n\n    if from_encoding is None:\n        try:\n            encoding = chardet.detect(content)['encoding']\n            if encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n        except Exception as e:\n            raise Exception(f\"Error detecting encoding: {e}\")\n    else:\n        encoding = from_encoding\n\n    try:\n        decoded_content = content.decode(encoding)\n    except UnicodeDecodeError as e:\n        raise Exception(f\"Decoding error: {e}\")\n\n    try:\n        reencoded_content = decoded_content.encode(to_encoding)\n    except UnicodeEncodeError as e:\n        raise Exception(f\"Re-encoding error: {e}\")\n\n    try:\n        data = json.loads(reencoded_content.decode(to_encoding))\n    except json.JSONDecodeError as e:\n        raise Exception(f\"JSON parsing error: {e}\")\n\n    return data"}
{"task_id": "BigCodeBench/1021", "solution": "import binascii\nimport hashlib\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"Input must be a string\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None\")\n    \n    # Compute SHA256 hash\n    sha256_hash = hashlib.sha256(input_string.encode()).hexdigest()\n    \n    # Verify hash if provided\n    if verify_hash is not None:\n        if sha256_hash == verify_hash:\n            return sha256_hash, True\n        else:\n            return sha256_hash, False\n    else:\n        return sha256_hash, None"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n    \n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    \n    current_date = datetime.now().date()\n    filtered_df = df[df[column_name].dt.date == current_date]\n    \n    sorted_df = filtered_df.sort_values(by=column_name)\n    \n    return sorted_df"}
{"task_id": "BigCodeBench/1023", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(dataframe):\n    if dataframe.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n    \n    if not all(dataframe.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"All columns in the DataFrame must be numeric.\")\n    \n    if dataframe.shape[1] < 2:\n        raise ValueError(\"DataFrame must contain at least two columns.\")\n    \n    corr_matrix = dataframe.corr()\n    corr_values = corr_matrix.unstack()\n    corr_values = corr_values.sort_values(ascending=False)\n    corr_values = corr_values[corr_values != 1]\n    \n    if corr_values.empty:\n        raise ValueError(\"No pairs of columns with correlation other than 1.\")\n    \n    col1, col2 = corr_values.index[0]\n    plt.scatter(dataframe[col1], dataframe[col2])\n    plt.xlabel(col1)\n    plt.ylabel(col2)\n    plt.title(f\"Scatter plot of {col1} and {col2}\")\n    return plt.gca()"}
{"task_id": "BigCodeBench/1024", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nPLOT_TITLE = \"Value Distribution\"\ndef task_func(data_dict):\n    # Create a DataFrame from the input dictionary\n    df = pd.DataFrame(data_dict)\n    \n    # Remove None values\n    df = df.replace([None], np.nan).dropna()\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        return df, None\n    \n    # Check if all values are the same\n    if df.nunique().sum() == 1:\n        return df, None\n    \n    # Calculate the number of bins\n    n = len(df)\n    bins = min(11, max(2, int(n / 2)))\n    \n    # Generate histogram\n    ax = sns.histplot(df, bins=bins, kde=False)\n    ax.set_title(PLOT_TITLE)\n    \n    return df, ax"}
{"task_id": "BigCodeBench/1025", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nPLOT_TITLE = \"Scaled Values\"\ndef task_func(data_dict):\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data_dict)\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the data\n    scaled_data = scaler.fit_transform(df)\n    \n    # Convert the scaled data back to a DataFrame\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n    \n    # Plot the scaled data\n    plt.figure()\n    scaled_df.plot()\n    plt.title(PLOT_TITLE)\n    plt.show()\n    \n    # Return the scaled DataFrame and the plot\n    return scaled_df, plt.gca()\ndata_dict = {'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]}"}
{"task_id": "BigCodeBench/1026", "solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\ndef task_func(kwargs):\n    group1 = kwargs.get('group1', [])\n    group2 = kwargs.get('group2', [])\n    \n    # Check if either group is empty or contains only NaNs\n    if not group1 or all(np.isnan(group1)):\n        raise ValueError(\"Group1 is empty or contains only NaNs.\")\n    if not group2 or all(np.isnan(group2)):\n        raise ValueError(\"Group2 is empty or contains only NaNs.\")\n    \n    # Remove NaNs from both groups\n    group1 = np.array(group1)\n    group2 = np.array(group2)\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n    \n    # Check if there are at least two non-NaN values in each group\n    if len(group1) < 2:\n        raise ValueError(\"Group1 has less than two non-NaN values.\")\n    if len(group2) < 2:\n        raise ValueError(\"Group2 has less than two non-NaN values.\")\n    \n    # Check if variance in each group is above a threshold\n    var1 = np.var(group1)\n    var2 = np.var(group2)\n    if var1 < 1e-8 or var2 < 1e-8:\n        raise ValueError(\"Variance in one or both groups is below the threshold (1e-8).\")\n    \n    # Perform two-sample t-test\n    t_stat, p_value = ttest_ind(group1, group2, nan_policy='omit')\n    \n    # Determine if means are significantly different\n    significant = p_value < 0.05\n    \n    # Compute descriptive statistics\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n    \n    # Create boxplot and histograms\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Boxplot\n    axes[0].boxplot([group1, group2], labels=['Group1', 'Group2'])\n    axes[0].set_title('Boxplot of Group1 and Group2')\n    \n    # Histograms\n    axes[1].hist([group1, group2], bins=10, label=['Group1', 'Group2'], alpha=0.7)\n    axes[1].legend()\n    axes[1].set_title('Histograms of Group1 and Group2')\n    \n    # Return results\n    return {\n        'significant': significant,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': axes[0],\n        'ax_histogram': axes[1]\n    }"}
{"task_id": "BigCodeBench/1027", "solution": "import binascii\nimport urllib.parse\ndef task_func(url):\n    try:\n        # Parse the URL to extract the query parameters\n        parsed_url = urllib.parse.urlparse(url)\n        query_params = urllib.parse.parse_qs(parsed_url.query)\n        \n        # Check if 'q' parameter exists\n        if 'q' in query_params:\n            # Extract the first value of 'q' parameter\n            hex_string = query_params['q'][0]\n            \n            # Decode the hexadecimal string to UTF-8\n            try:\n                decoded_string = binascii.unhexlify(hex_string).decode('utf-8')\n                return decoded_string\n            except (binascii.Error, UnicodeDecodeError):\n                # If decoding fails, return None\n                return None\n        else:\n            # If 'q' parameter does not exist, return None\n            return None\n    except Exception as e:\n        # If any other exception occurs, return None\n        return None"}
{"task_id": "BigCodeBench/1028", "solution": "import subprocess\nimport time\nimport json\nimport platform\nLOGFILE_PATH = \"logfile.log\"\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Interval and duration must be greater than zero.\")\n    \n    try:\n        with open(LOGFILE_PATH, 'w') as log_file:\n            start_time = time.time()\n            while time.time() - start_time < duration:\n                if platform.system() == 'Windows':\n                    result = subprocess.run(['wmic', 'cpu', 'get', 'LoadPercentage'], capture_output=True, text=True)\n                    cpu_usage = result.stdout.strip().split('\\n')[-1]\n                else:\n                    result = subprocess.run(['top', '-bn1', '-p', '1'], capture_output=True, text=True)\n                    cpu_usage = result.stdout.strip().split('\\n')[-1].split()[-2]\n                \n                timestamp = time.time()\n                log_entry = {\"timestamp\": timestamp, \"cpu_usage\": cpu_usage}\n                json.dump(log_entry, log_file)\n                log_file.write('\\n')\n                time.sleep(interval)\n    except IOError as e:\n        print(f\"An error occurred: {e}\")\n        return None\n    return LOGFILE_PATH"}
{"task_id": "BigCodeBench/1029", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(rows=100, columns=3):\n    # Create a list of column names\n    columns_list = [chr(ord('a') + i) for i in range(columns)]\n    \n    # Create a DataFrame with random alphabets\n    data = np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=(rows, columns))\n    df = pd.DataFrame(data, columns=columns_list)\n    \n    return df"}
{"task_id": "BigCodeBench/1030", "solution": "import itertools\nimport string\nimport pandas as pd\ndef task_func():\n    # Define the alphabet\n    alphabet = string.ascii_lowercase\n\n    # Generate all possible combinations with replacement of three letters\n    combinations = list(itertools.combinations_with_replacement(alphabet, 3))\n\n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Letter1', 'Letter2', 'Letter3'])\n\n    return df"}
{"task_id": "BigCodeBench/1031", "solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows must be greater than 0\")\n    \n    # Generate random 3-letter strings\n    random_strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n    \n    # Count the frequency of each string\n    frequency = pd.Series(random_strings).value_counts()\n    \n    # Get the top 30 most common strings\n    top_30 = frequency.head(30)\n    \n    # Plot a histogram\n    ax = top_30.plot(kind='bar')\n    \n    return ax"}
{"task_id": "BigCodeBench/1032", "solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\nLETTERS = list(string.ascii_lowercase)\ndef task_func(rows=1000, string_length=3):\n    if rows == 0:\n        print(\"No data to generate heatmap.\")\n        return None\n    else:\n        # Generate random strings\n        data = [''.join(random.choices(LETTERS, k=string_length)) for _ in range(rows)]\n        df = pd.DataFrame(data, columns=['strings'])\n        \n        # Convert strings to one-hot encoded format\n        one_hot = pd.get_dummies(df['strings'].str.split('', expand=True).stack()).sum(level=0)\n        \n        # Calculate frequency of each letter\n        frequency = one_hot.sum()\n        \n        # Create a correlation matrix\n        correlation_matrix = frequency.corr()\n        \n        # Plot heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n        plt.title('Correlation Heatmap of Letter Frequencies')\n        plt.show()\n        \n        return plt.gca()"}
{"task_id": "BigCodeBench/1033", "solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate all possible 3-letter combinations\n    letters = string.ascii_lowercase\n    combinations = list(itertools.product(letters, repeat=3))\n    combinations = [''.join(combination) for combination in combinations]\n    \n    # Create a DataFrame from these combinations\n    df = pd.DataFrame(combinations, columns=['3-letter combinations'])\n    \n    # Extract the first letters\n    first_letters = [combination[0] for combination in combinations]\n    \n    # Count the frequency of each first letter\n    frequency = pd.Series(first_letters).value_counts().sort_index()\n    \n    # Plot a histogram of the frequency\n    fig, ax = plt.subplots()\n    frequency.plot(kind='bar', ax=ax)\n    ax.set_title('Frequency of first letters in 3-letter combinations')\n    ax.set_xlabel('First letter')\n    ax.set_ylabel('Frequency')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/1034", "solution": "import pandas as pd\nimport numpy as scipy\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\ndef task_func(s1, s2):\n    # Create a DataFrame with the sales data\n    df = pd.DataFrame({'Store1': s1, 'Store2': s2}, index=CATEGORIES)\n    \n    # Filter categories where both stores have sales exceeding the threshold of 200\n    filtered_df = df[(df['Store1'] > 200) & (df['Store2'] > 200)]\n    \n    # If no categories meet the threshold, return None for the plot and 0.0 for the distance\n    if filtered_df.empty:\n        return None, 0.0\n    \n    # Generate a bar plot for the filtered categories\n    ax = filtered_df.plot(kind='bar')\n    \n    # Compute the Euclidean distance between the two series\n    distance = scipy.spatial.distance.euclidean(filtered_df['Store1'], filtered_df['Store2'])\n    \n    return ax, distance"}
{"task_id": "BigCodeBench/1035", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Reshape the feature to be 2D\n    feature = feature.values.reshape(-1, 1)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size=0.2, random_state=42)\n    \n    # Initialize and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Predict the target for the test set\n    y_pred = model.predict(X_test)\n    \n    # Compute the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # Plot the confusion matrix\n    plt.figure()\n    sns.heatmap(cm, annot=True, fmt='d')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    # Return the confusion matrix and the Axes object\n    return cm, plt.gca()"}
{"task_id": "BigCodeBench/1036", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2):\n    # Create a DataFrame from the two Series\n    df = pd.DataFrame({'s1': s1, 's2': s2})\n    \n    # Create a swarm plot\n    ax = sns.swarmplot(data=df)\n    \n    # Find intersecting data points\n    intersection = s1[s1.isin(s2)]\n    intersection_count = len(intersection)\n    \n    # Highlight intersecting points\n    for x, y in zip(s1.index, intersection):\n        ax.plot([x, x], [s1[x], y], 'r--')\n    \n    return ax, intersection_count"}
{"task_id": "BigCodeBench/1037", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2, n_clusters=3):\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 must be pandas Series\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must have the same length\")\n    \n    data = pd.concat([s1, s2], axis=1)\n    data.columns = ['s1', 's2']\n    \n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n    \n    plt.scatter(data['s1'], data['s2'], c=labels, cmap='viridis')\n    plt.title('K-Means Clustering')\n    plt.legend()\n    plt.show()\n    \n    return labels, plt.gca()"}
{"task_id": "BigCodeBench/1038", "solution": "from datetime import datetime\nimport json\ndef task_func(client_socket):\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    greeting = \"Hello, client!\"\n    response = {\"time\": current_time, \"greeting\": greeting}\n    client_socket.sendall(json.dumps(response).encode())"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL/TLS\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode()\n        \n        # Check if the file exists\n        if not os.path.exists(file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n        \n        # Open the file in binary mode\n        with open(file_path, 'rb') as file:\n            # Read the file in chunks and compute SHA256 hash\n            sha256_hash = hashlib.sha256()\n            while chunk := file.read(buffer_size):\n                sha256_hash.update(chunk)\n        \n        # Send the SHA256 hash back to the client\n        hash_hex = sha256_hash.hexdigest().encode()\n        secure_socket.sendall(hash_hex)\n        \n        return hash_hex.decode()\n    \n    except Exception as e:\n        # Handle any exceptions that occur during processing\n        error_message = f'An error occurred: {str(e)}'\n        secure_socket.sendall(error_message.encode())\n        return error_message"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5):\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server.bind((server_address, server_port))\n    server.listen(5)\n    server.setblocking(0)\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n    while datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 1)\n        for s in readable:\n            if s is server:\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    message_queues[s].put(data)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                s.send(next_msg + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").encode())\n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n    server.close()\n    return f\"Server ran for {run_duration} seconds.\""}
{"task_id": "BigCodeBench/1041", "solution": "import re\nimport os\ndef task_func(request):\n    # Define the base directory where static files are stored\n    base_dir = 'static_files'\n    \n    # Extract the filename from the request\n    filename = re.search(r'GET /(.+) HTTP/1.1', request).group(1)\n    \n    # Construct the full path to the file\n    file_path = os.path.join(base_dir, filename)\n    \n    # Check if the file exists\n    if os.path.exists(file_path):\n        # Read the file content\n        with open(file_path, 'r') as file:\n            content = file.read()\n        \n        # Create the HTTP response\n        response = f\"HTTP/1.1 200 OK\\r\\nContent-Length: {len(content)}\\r\\n\\r\\n{content}\"\n    else:\n        # File not found\n        response = \"HTTP/1.1 404 Not Found\\r\\n\\r\\nFile not found\"\n    \n    return response"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Receive the message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode()\n    # Ask for sender's email\n    sender_email = input(\"Enter your email: \")\n    # Ask for recipient's email\n    recipient_email = input(\"Enter recipient's email: \")\n    # Ask for sender's email password\n    sender_password = getpass.getpass(\"Enter your email password: \")\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(message)\n    msg['Subject'] = 'Test Email'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    # Connect to the SMTP server\n    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n    server.starttls()\n    # Login to the SMTP server\n    server.login(sender_email, sender_password)\n    # Send the email\n    server.send_message(msg)\n    # Close the connection\n    server.quit()\n    # Output None\n    return None"}
{"task_id": "BigCodeBench/1043", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data list is empty.\")\n    \n    # Count the occurrences of each category in the data_list\n    category_counts = pd.Series(data_list).value_counts()\n    \n    # Find all unique categories in the data_list\n    all_categories = set(data_list)\n    \n    # Check if all predefined categories are present\n    missing_categories = set(CATEGORIES) - all_categories\n    if missing_categories:\n        print(f\"The following predefined categories are missing: {missing_categories}\")\n    \n    # Check for uniformity in the distribution of predefined categories\n    predefined_counts = category_counts.get(CATEGORIES, 0)\n    if not predefined_counts.equals(pd.Series(predefined_counts).value_counts()):\n        print(\"The distribution of predefined categories is not uniform.\")\n    \n    # Include all predefined categories in the histogram, with count 0 for missing ones\n    all_categories = set(CATEGORIES) | all_categories\n    all_categories = sorted(all_categories)\n    \n    # Create a bar plot\n    ax = plt.axes()\n    ax.bar(all_categories, category_counts.reindex(all_categories, fill_value=0), width=0.8, align=\"center\")\n    \n    # Set the title and labels\n    ax.set_title('Category Distribution')\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Counts')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/1044", "solution": "import pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\ndef task_func(date_str, booking_data):\n    # Validate the date string\n    try:\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"Date must be in 'yyyy-mm-dd' format and a valid date.\")\n    \n    # Check if the date is in the past\n    if date < datetime.today():\n        raise ValueError(\"Date cannot be in the past.\")\n    \n    # Filter booking data for the specified date\n    filtered_data = booking_data[booking_data['date'] == date_str]\n    \n    # If no bookings for the date, return empty DataFrame and plot\n    if filtered_data.empty:\n        df = pd.DataFrame(columns=['room', 'status'])\n        ax = df.plot(kind='bar', x='room', y='status')\n        return df, ax\n    \n    # Compile booking status report\n    status_report = {}\n    for room in ROOMS:\n        room_data = filtered_data[filtered_data['room'] == room]\n        if not room_data.empty:\n            status_report[room] = room_data['status'].iloc[0]\n        else:\n            status_report[room] = 'Unbooked'\n    \n    # Create DataFrame from the status report\n    df = pd.DataFrame(list(status_report.items()), columns=['room', 'status'])\n    \n    # Create bar plot\n    ax = df.plot(kind='bar', x='room', y='status')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/1045", "solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\ndef task_func(date_str):\n    given_date = parse(date_str)\n    current_date = datetime.now()\n    delta = current_date - given_date\n    total_seconds = delta.total_seconds()\n    leap_seconds = 0\n    for year in LEAP_SECONDS:\n        if given_date.year <= year < current_date.year:\n            leap_seconds += 1\n    return int(total_seconds + leap_seconds)"}
{"task_id": "BigCodeBench/1046", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\ndef task_func(date_str):\n    # Convert the input date string to a datetime object\n    start_date = datetime.strptime(date_str, '%Y-%m-%d')\n    \n    # Create a list to hold the data for the DataFrame\n    data = []\n    \n    # Generate the next 10 days from the start date\n    for employee in EMPLOYEES:\n        for day in range(10):\n            date = start_date + timedelta(days=day)\n            data.append({'Employee': employee, 'Date': date.strftime('%Y-%m-%d')})\n    \n    # Create the DataFrame\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/1047", "solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n    \n    # Extract the day of the month\n    day = date.day\n    \n    # Generate a list of random integers with the count equal to the day of the month\n    random_integers = [random.randint(1, 100) for _ in range(day)]\n    \n    # Create a line plot of the random integers\n    fig, ax = plt.subplots()\n    ax.plot(random_integers)\n    \n    # Return the Axes object of the plot\n    return ax"}
{"task_id": "BigCodeBench/1048", "solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    # Convert the date string to a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n    \n    # Extract the day of the month\n    day = date.day\n    \n    # Generate x values (time)\n    x = np.linspace(0, 2 * np.pi, 100)\n    \n    # Calculate the frequency based on the day of the month\n    frequency = day / 30  # Assuming a month has 30 days for simplicity\n    \n    # Calculate the y values (sine wave)\n    y = np.sin(frequency * x)\n    \n    # Create a figure and an axes object\n    fig, ax = plt.subplots()\n    \n    # Plot the sine wave\n    ax.plot(x, y)\n    \n    # Set the title and labels\n    ax.set_title(f'Sine Wave with Frequency {frequency}')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/1049", "solution": "import re\nimport pandas as pd\ndef task_func(input_string: str) -> pd.DataFrame:\n    # Split the input string into lines\n    lines = input_string.splitlines()\n    \n    # Filter out empty or whitespace-only lines and replace tabs with spaces\n    filtered_lines = [re.sub(r'\\t', ' ', line) for line in lines if line.strip()]\n    \n    # Create a DataFrame with a single column 'Text' containing the filtered and processed lines\n    df = pd.DataFrame(filtered_lines, columns=['Text'])\n    \n    return df"}
{"task_id": "BigCodeBench/1050", "solution": "import os\nimport hashlib\nDIRECTORY = \"./hashed_files\"\ndef task_func(input_string):\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n    file_paths = []\n    lines = input_string.splitlines()\n    for line in lines:\n        if line.strip():\n            hash_object = hashlib.sha256(line.encode())\n            hash_hex = hash_object.hexdigest()\n            filename = hash_hex[:10] + \".txt\"\n            file_path = os.path.join(DIRECTORY, filename)\n            with open(file_path, \"w\") as file:\n                file.write(hash_hex)\n            file_paths.append(file_path)\n    return file_paths"}
{"task_id": "BigCodeBench/1051", "solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"The distribution is uniform.\"\n    \n    counts = list(data_dict.values())\n    avg_count = np.mean(counts)\n    \n    for count in counts:\n        if abs(count - avg_count) > 1e-5:\n            return None, \"The distribution is not uniform.\"\n    \n    categories = list(data_dict.keys())\n    counts = list(data_dict.values())\n    \n    unique_counts = set(counts)\n    num_bins = min(10, len(unique_counts))\n    \n    plt.hist(counts, bins=num_bins, edgecolor='black')\n    plt.xticks(ticks=range(len(categories)), labels=categories, rotation=45)\n    plt.xlabel('Categories')\n    plt.ylabel('Counts')\n    plt.title('Distribution of Categories')\n    \n    return plt.gca(), \"The distribution is uniform.\""}
{"task_id": "BigCodeBench/1052", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    # Read the CSV file\n    try:\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        print(\"The file was not found.\")\n        return None\n    except pd.errors.EmptyDataError:\n        print(\"The file is empty.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n    # Check if the 'text' column exists\n    if 'text' not in df.columns:\n        print(\"The 'text' column is missing in the CSV file.\")\n        return None\n    # Vectorize the text data\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['text'])\n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n    # Sum the counts across all documents\n    word_counts = X.sum(axis=0)\n    # Create a DataFrame with words and their counts\n    word_df = pd.DataFrame({'word': feature_names, 'count': word_counts.A1})\n    # Sort the DataFrame by count in descending order\n    word_df = word_df.sort_values(by='count', ascending=False)\n    # Get the top 10 words\n    top_words = word_df.head(10)\n    # Check if there are any valid words\n    if top_words.empty:\n        print(\"No valid words found in the input data.\")\n        return None\n    # Plot the histogram\n    plt.figure(figsize=(10, 6))\n    plt.barh(top_words['word'], top_words['count'], color='skyblue')\n    plt.xlabel('Frequency')\n    plt.title('Top 10 Most Common Words')\n    # If save_path is provided, save the plot to the specified location\n    if save_path:\n        plt.savefig(save_path)\n        print(f\"Plot saved to {save_path}\")\n        return None\n    # Otherwise, display the plot and return the Axes object\n    plt.show()\n    return plt.gca()"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if the CSV has a header\n        if 'Text' in df.columns:\n            text_data = df['Text']\n        else:\n            text_data = df.iloc[:, 0]\n        \n        # Initialize the CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the text data\n        X = vectorizer.fit_transform(text_data)\n        \n        # Get the feature names (words)\n        feature_names = vectorizer.get_feature_names_out()\n        \n        # Sum the counts across all documents\n        counts = X.sum(axis=0)\n        \n        # Get the indices of the top 10 words\n        top_indices = counts.argsort()[:-11:-1]\n        \n        # Get the top 10 words and their counts\n        top_words = feature_names[top_indices]\n        top_counts = counts[0, top_indices]\n        \n        # Create a DataFrame for the top words and their counts\n        top_words_df = pd.DataFrame({'Word': top_words, 'Count': top_counts})\n        \n        # Plot the histogram\n        plt.figure(figsize=(10, 6))\n        plt.barh(top_words, top_counts, color='skyblue')\n        plt.xlabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        \n        if save_path:\n            plt.savefig(save_path)\n            print(f\"Plot saved to {save_path}\")\n            return None\n        else:\n            return plt.gca()\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1054", "solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            reader = csv.reader(file)\n            data = [float(row[0]) for row in reader]\n    except FileNotFoundError:\n        print(\"The file was not found.\")\n        return None, None, None\n    except IOError:\n        print(\"An error occurred while reading the file.\")\n        return None, None, None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return None, None, None\n\n    sample = np.random.choice(data, size=30, replace=False)\n    sample_mean = np.mean(sample)\n    sample_std = np.std(sample, ddof=1)\n\n    plt.figure()\n    n, bins, patches = plt.hist(sample, bins='auto', density=True, color='lightblue', edgecolor='black')\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, sample_mean, sample_std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title('Histogram of Sample with Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return sample_mean, sample_std, plt.gca()"}
{"task_id": "BigCodeBench/1055", "solution": "import pandas as pd\nimport itertools\nimport random\ndef task_func(colors, states):\n    # Generate the Cartesian product of colors and states\n    combinations = list(itertools.product(colors, states))\n    \n    # Shuffle the combinations\n    random.shuffle(combinations)\n    \n    # Determine the number of columns\n    num_columns = min(len(colors), len(states))\n    \n    # Create a list to hold the data for each column\n    column_data = []\n    \n    # Distribute the combinations across columns\n    for i in range(num_columns):\n        column = []\n        for j in range(len(combinations)):\n            if j % num_columns == i:\n                color, state = combinations[j]\n                column.append(f\"{color}:{state}\")\n        column_data.append(column)\n    \n    # Create the DataFrame\n    df = pd.DataFrame(column_data).T\n    df.columns = [f\"Column {i+1}\" for i in range(num_columns)]\n    \n    return df"}
{"task_id": "BigCodeBench/1056", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\ndef task_func(n_pairs=26):\n    if not 1 <= n_pairs <= 26:\n        raise ValueError(\"n_pairs must be between 1 and 26, inclusive.\")\n    \n    # Create pairs\n    pairs = [f\"{letter}:{number}\" for letter, number in zip(LETTERS, NUMBERS)]\n    \n    # Shuffle pairs\n    random.shuffle(pairs)\n    \n    # Select the first n_pairs\n    selected_pairs = pairs[:n_pairs]\n    \n    # Assign random counts\n    counts = [random.randint(1, 9) for _ in range(n_pairs)]\n    \n    # Create bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar(range(n_pairs), counts, tick_label=selected_pairs)\n    \n    # Set title and labels\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n    \n    # Show the plot\n    plt.show()\n    \n    return bars"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['dog', 'cat', 'bird']\n    if foods is None:\n        foods = ['meat', 'fish', 'vegetables']\n    \n    if not animals or not foods:\n        return pd.DataFrame()\n    \n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n    \n    df = pd.DataFrame(combinations, columns=['animal', 'food'])\n    df['combined'] = df['animal'] + ':' + df['food']\n    \n    return df[['combined']]"}
{"task_id": "BigCodeBench/1058", "solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\ndef task_func(num_pairs=10):\n    # Generate all possible combinations of shapes and colors\n    combinations = list(itertools.product(SHAPES, COLORS))\n    \n    # Select the specified number of unique shape-color pairs\n    selected_pairs = combinations[:num_pairs]\n    \n    # Create a DataFrame from the selected pairs\n    data = {\n        \"Shape\": [pair[0] for pair in selected_pairs],\n        \"Color\": [pair[1] for pair in selected_pairs]\n    }\n    df = pd.DataFrame(data)\n    \n    # Create a countplot using seaborn\n    ax = sns.countplot(x=\"Shape\", hue=\"Color\", data=df)\n    \n    # Display the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/1059", "solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\ndef task_func():\n    # Create a list of all possible planet-element pairs\n    pairs = [f\"{planet}:{element}\" for planet in PLANETS for element in ELEMENTS]\n    \n    # Shuffle the list of pairs\n    random.shuffle(pairs)\n    \n    # Create a DataFrame with the shuffled pairs\n    df = pd.DataFrame(pairs, columns=['Planet:Element'])\n    \n    # Reshape the DataFrame into a grid with number of rows equal to the number of planets\n    # and number of columns equal to the number of elements\n    df = df.unstack().reset_index(drop=True).stack().reset_index(drop=True)\n    df = df.unstack(level=0)\n    \n    return df"}
{"task_id": "BigCodeBench/1060", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        plt.figure()\n        plt.title(f\"Distribution of values in {column_name} (No Data)\")\n        return \"The DataFrame is empty or the specified column has no data.\", plt.gca()\n    \n    unique_values = df[column_name].unique()\n    if len(unique_values) == 0:\n        plt.figure()\n        plt.title(f\"Distribution of values in {column_name} (No Data)\")\n        return \"The DataFrame is empty or the specified column has no data.\", plt.gca()\n    \n    value_counts = df[column_name].value_counts()\n    is_uniform = value_counts.nunique() == 1\n    \n    plt.figure()\n    plt.hist(df[column_name], bins=len(unique_values), edgecolor='black', alpha=0.7)\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    plt.title(f\"Distribution of values in {column_name}\")\n    \n    if is_uniform:\n        return \"The distribution of values is uniform.\", plt.gca()\n    else:\n        return \"The distribution of values is not uniform.\", plt.gca()"}
{"task_id": "BigCodeBench/1061", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculate the sum of elements in each row\n    row_sums = np.sum(arr, axis=1)\n    \n    # Calculate mean and standard deviation of the row sums\n    mean = np.mean(row_sums)\n    std = np.std(row_sums)\n    \n    # Normalize the row sums\n    if std == 0:\n        normalized_data = np.zeros_like(row_sums)\n    else:\n        normalized_data = (row_sums - mean) / std\n    \n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of normalized data\n    ax.hist(normalized_data, bins=30, density=True, color='g', alpha=0.6, label='Normalized Data')\n    \n    # Calculate the range for the standard normal PDF\n    x_min = norm.ppf(0.005)\n    x_max = norm.ppf(0.995)\n    x = np.linspace(x_min, x_max, 100)\n    \n    # Plot the standard normal PDF\n    pdf = norm.pdf(x)\n    ax.plot(x, pdf, color='r', linewidth=2, label='Standard Normal PDF')\n    \n    # Set the title and labels\n    ax.set_title('Histogram of Normalized Data with Standard Normal PDF')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Probability Density')\n    \n    # Add legend\n    ax.legend()\n    \n    # Return the axes object and the normalized data\n    return ax, normalized_data"}
{"task_id": "BigCodeBench/1062", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\ndef task_func(arr):\n    if arr.size == 0:\n        fig, ax = plt.subplots()\n        ax.set_title('Time Series of Row Sums')\n        return ax\n    else:\n        dates = pd.date_range(start='1/1/2020', periods=arr.shape[0])\n        df = pd.DataFrame(arr, index=dates)\n        df.plot()\n        plt.title('Time Series of Row Sums')\n        return plt.gca()"}
{"task_id": "BigCodeBench/1063", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(arr):\n    # Perform PCA on the sum of rows of the 2D numpy array\n    pca = PCA()\n    pca.fit(arr.sum(axis=1).reshape(-1,1))\n    \n    # Plot the explained variance ratio\n    plt.title(\"Explained Variance Ratio of Principal Components\")\n    plt.plot(pca.explained_variance_ratio_)\n    plt.xlabel('Principal Component')\n    plt.ylabel('Explained Variance Ratio')\n    plt.show()\n    \n    return plt.gca()\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": "BigCodeBench/1064", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n    print(\"Row sums:\", row_sums)\n    \n    # Determine the color range based on the minimum and maximum values in the array\n    min_val = np.min(arr)\n    max_val = np.max(arr)\n    \n    # Plot the heatmap\n    ax = sns.heatmap(arr, cmap='viridis', center=0, vmin=min_val, vmax=max_val)\n    \n    return ax\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": "BigCodeBench/1065", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import fftpack\ndef task_func(arr):\n    \"\"\"\n    Performs a Fast Fourier Transform (FFT) on the sum of each row in a 2D array and plots the absolute values of the FFT coefficients.\n\n    Parameters:\n    arr (np.ndarray): A 2D numpy array.\n\n    Returns:\n    matplotlib.axes.Axes: An Axes object displaying the plot of the absolute values of the FFT coefficients.\n    \"\"\"\n    # Sum of each row\n    row_sums = np.sum(arr, axis=1)\n    \n    # Perform FFT on the sum of each row\n    fft_result = fftpack.fft(row_sums)\n    \n    # Compute the absolute values of the FFT coefficients\n    abs_fft = np.abs(fft_result)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot the absolute values of the FFT coefficients\n    ax.plot(abs_fft)\n    \n    # Set the title and labels\n    ax.set_title('Absolute Values of FFT Coefficients')\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('Magnitude')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/1066", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normally distributed data\n    normal_data = np.random.normal(loc=0, scale=1, size=num_samples)\n    \n    # Generate artificially introduced outliers\n    outliers = np.random.uniform(low=-10, high=10, size=num_outliers)\n    \n    # Combine the data\n    data = np.concatenate((normal_data, outliers))\n    \n    # Detect outliers using IQR method on the normally distributed data\n    Q1 = np.percentile(normal_data, 25)\n    Q3 = np.percentile(normal_data, 75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers_detected = data[(data < lower_bound) | (data > upper_bound)]\n    \n    # Plot histogram of the combined data\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=30, color='skyblue', edgecolor='black')\n    ax.set_title('Histogram of Combined Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return data, outliers_detected, ax"}
{"task_id": "BigCodeBench/1067", "solution": "import requests\nimport logging\ndef task_func(repo_url: str) -> dict:\n    \"\"\"\n    Fetches and returns information about a GitHub repository using its API URL.\n    It makes an HTTP GET request to the provided repository URL.\n    It incorporates error handling for various scenarios including API rate limits,\n    other HTTP errors, and general request issues. The function also checks for a large\n    number of open issues in the repository and prints a warning if they exceed a\n    certain threshold.\n\n    Parameters:\n    - repo_url (str): The URL of the GitHub repository.\n\n    Returns:\n    - dict: A dictionary containing information about the GitHub repository.\n\n    Raises:\n    - requests.exceptions.HTTPError: If an HTTP error occurs, particularly when the\n      GitHub API rate limit is exceeded.\n    - requests.exceptions.RequestException: For other general issues encountered\n      during the API request, such as network problems, invalid responses, or timeouts.\n    \"\"\"\n    # Set up logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    # Define the threshold for open issues\n    issue_threshold = 1000  # Example threshold, can be adjusted\n\n    try:\n        # Make the GET request to the GitHub API\n        response = requests.get(repo_url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n\n        # Parse the JSON response\n        repo_info = response.json()\n\n        # Check for open issues\n        open_issues_url = repo_info['issues_url'].replace('{/number}', '')\n        issues_response = requests.get(open_issues_url)\n        issues_response.raise_for_status()\n        issues = issues_response.json()\n\n        # Count the number of open issues\n        open_issues_count = len(issues)\n\n        # Check if the number of open issues exceeds the threshold\n        if open_issues_count > issue_threshold:\n            logger.warning(f\"Warning: The repository has {open_issues_count} open issues, which exceeds the threshold of {issue_threshold}.\")\n\n        return repo_info\n\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors, including rate limits\n        if response.status_code == 403 and 'rate limit' in str(http_err):\n            logger.error(\"Error: API rate limit exceeded. Please try again later.\")\n        else:\n            logger.error(f\"HTTP error occurred: {http_err}\")\n        raise\n\n    except requests.exceptions.RequestException as err:\n        # Handle other request exceptions\n        logger.error(f\"Request exception occurred: {err}\")\n        raise\n\n    except Exception as e:\n        # Handle any other exceptions\n        logger.error(f\"An error occurred: {e}\")\n        raise"}
{"task_id": "BigCodeBench/1068", "solution": "import warnings\nimport sqlite3\nimport pandas as pd\ndef task_func(db_path, query, warn_large_dataset=True):\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n        \n        # Execute the SQL query\n        cursor.execute(query)\n        \n        # Fetch all rows from the query result\n        rows = cursor.fetchall()\n        \n        # Get the column names from the cursor description\n        colnames = [desc[0] for desc in cursor.description]\n        \n        # Create a pandas DataFrame from the fetched data\n        df = pd.DataFrame(rows, columns=colnames)\n        \n        # Check if the dataset is large\n        if warn_large_dataset and len(df) > 10000:\n            warnings.warn(\"The data contains more than 10000 rows.\")\n        \n        return df\n    \n    except Exception as e:\n        # Raise an exception with a detailed error message\n        raise Exception(f\"Error fetching data from the database: {str(e)}\")"}
{"task_id": "BigCodeBench/1069", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data_dict):\n    df = pd.DataFrame(data_dict)\n    axes = []\n    for col in df.columns:\n        plt.figure()\n        ax = plt.hist(df[col], bins=30)\n        axes.append(ax)\n        if not np.all(np.diff(np.histogram(df[col], bins=30)[0]) == 0):\n            print(f\"The distribution of {col} is non-uniform.\")\n    return axes"}
{"task_id": "BigCodeBench/1070", "solution": "import pandas as pd\nfrom random import shuffle\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\ndef task_func(list_of_lists):\n    result = []\n    for sublist in list_of_lists:\n        # Create a copy of POSSIBLE_VALUES to shuffle\n        shuffled_values = POSSIBLE_VALUES.copy()\n        shuffle(shuffled_values)\n        # Create a DataFrame with columns as per sublist\n        df = pd.DataFrame({col: shuffled_values for col in sublist})\n        result.append(df)\n    return result"}
{"task_id": "BigCodeBench/1071", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\ndef task_func(list_of_lists):\n    fig, ax = plt.subplots()\n    color_cycle = cycle(COLORS)\n    for lst in list_of_lists:\n        if not lst:\n            continue\n        y = lst.copy()\n        shuffle(y)\n        x = np.arange(1, len(y)+1)\n        color = next(color_cycle)\n        ax.plot(x, y, color=color)\n    return fig, ax"}
{"task_id": "BigCodeBench/1072", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(list_of_lists):\n    series_list = []\n    for sub_list in list_of_lists:\n        # Create a Series with unique integers from 1 to the length of the sub-list\n        series = pd.Series(range(1, len(sub_list) + 1), index=sub_list)\n        # Shuffle the values in the Series\n        series = series.sample(frac=1, random_state=42)\n        series_list.append(series)\n    return series_list\nlist_of_lists = [\n    ['a', 'b', 'c'],\n    ['x', 'y', 'z'],\n    [1, 2, 3, 4]\n]"}
{"task_id": "BigCodeBench/1073", "solution": "import time\nimport matplotlib.pyplot as plt\nimport re\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    seconds_list = []\n    for time_str in time_strings:\n        try:\n            parsed_time = time.strptime(time_str, time_format)\n            seconds = parsed_time.tm_sec\n            seconds_list.append(seconds)\n        except ValueError as e:\n            raise ValueError(f\"Error parsing time string '{time_str}': {e}\")\n    \n    if seconds_list:\n        fig, ax = plt.subplots()\n        ax.hist(seconds_list, bins=range(0, 61, 5), align='left', rwidth=0.8)\n        ax.set_xlabel('Seconds')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Histogram of Seconds Component')\n        return ax\n    else:\n        return None"}
{"task_id": "BigCodeBench/1074", "solution": "import pytz\nfrom dateutil.parser import parse\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_string, from_tz, to_tz):\n    # Parse the input time string\n    time = parse(time_string)\n    # Localize the time to the source timezone\n    time = pytz.timezone(from_tz).localize(time)\n    # Convert the time to the target timezone\n    time = time.astimezone(pytz.timezone(to_tz))\n    # Format the converted time as a string\n    return time.strftime(TIME_FORMAT)"}
{"task_id": "BigCodeBench/1075", "solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings):\n    # Convert datetime strings to datetime objects\n    datetime_objects = [datetime.datetime.strptime(ts, TIME_FORMAT) for ts in time_strings]\n    \n    # Calculate differences in seconds\n    differences = []\n    for i in range(1, len(datetime_objects)):\n        diff = datetime_objects[i] - datetime_objects[i-1]\n        differences.append(diff.total_seconds())\n    \n    # Create bar chart\n    x = np.arange(len(differences))\n    plt.bar(x, differences)\n    plt.xlabel('Consecutive Pairs')\n    plt.ylabel('Time Difference (seconds)')\n    plt.title('Time Differences Between Consecutive Datetime Strings')\n    plt.show()\n    \n    return plt.gca()\ntime_strings = [\n    \"01/01/21 12:00:00.000000\",\n    \"01/01/21 12:00:05.000000\",\n    \"01/01/21 12:00:10.000000\",\n    \"01/01/21 12:00:15.000000\"\n]"}
{"task_id": "BigCodeBench/1076", "solution": "from datetime import datetime\nimport pandas as pd\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings, target_tz):\n    # Create an empty list to store the results\n    results = []\n    # Iterate over each time string in the input list\n    for time_str in time_strings:\n        # Convert the UTC time string to a datetime object\n        utc_time = datetime.strptime(time_str, TIME_FORMAT)\n        # Convert the UTC datetime object to the target timezone\n        target_time = utc_time.astimezone(ZoneInfo(target_tz))\n        # Append the original and converted times to the results list\n        results.append({'Original Time': time_str, 'Converted Time': target_time.strftime(TIME_FORMAT)})\n    # Create a DataFrame from the results list\n    df = pd.DataFrame(results)\n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert each timestamp to the specified timezone\n    timezones = pytz.all_timezones\n    if timezone not in timezones:\n        raise ValueError(\"Invalid timezone\")\n\n    time_list = []\n    for time_string in time_strings:\n        # Convert string to datetime object\n        time_obj = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        # Convert to specified timezone\n        tz = pytz.timezone(timezone)\n        time_obj = tz.localize(time_obj)\n        time_list.append(time_obj)\n\n    # Calculate the time differences in seconds\n    time_diffs = []\n    for i in range(len(time_list) - 1):\n        diff = time_list[i+1] - time_list[i]\n        time_diffs.append(diff.total_seconds())\n\n    # Calculate the mean of the time differences\n    if len(time_diffs) == 0:\n        return 0.0\n    else:\n        return np.mean(time_diffs)"}
{"task_id": "BigCodeBench/1078", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    uniform_distribution = np.all(counts == counts[0])\n    ax = plt.hist(arr, bins=np.arange(len(unique) + 1) - 0.5, align='left')\n    return uniform_distribution, ax"}
{"task_id": "BigCodeBench/1079", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert string prices to float\n    prices = [float(price.replace(',', '')) for price in data['Price_String']]\n    \n    # Calculate mean, median, and standard deviation\n    mean = np.mean(prices)\n    median = np.median(prices)\n    std_dev = np.std(prices, ddof=1)  # Sample standard deviation\n    \n    # Create a DataFrame for the prices\n    df = pd.DataFrame(prices, columns=['Price'])\n    \n    # Generate histogram\n    ax = df['Price'].plot.hist(bins='auto', color='blue', alpha=0.7, rwidth=0.85, title='Histogram of Product Prices', xlabel='Price', ylabel='Frequency')\n    \n    # Return the results\n    return {'mean': mean, 'median': median, 'std_dev': std_dev}, ax\ndata = {\n    'Product': ['Product1', 'Product2', 'Product3', 'Product4', 'Product5'],\n    'Price_String': ['1,000', '2,000', '3,000', '4,000', '5,000']\n}"}
{"task_id": "BigCodeBench/1080", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\ndef task_func(area_string, data=DATA):\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Convert the Area_String to float\n    df['Area'] = df['Area_String'].str.replace(',', '').astype(float)\n    \n    # Drop the Area_String column\n    df.drop('Area_String', axis=1, inplace=True)\n    \n    # Initialize and train the model\n    model = LinearRegression()\n    model.fit(df[['Area']], df['Price'])\n    \n    # Predict the price for the given area\n    area = float(area_string.replace(',', ''))\n    predicted_price = model.predict([[area]])\n    \n    return float(predicted_price)"}
{"task_id": "BigCodeBench/1081", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data=None):\n    if data is None:\n        data = {\n            'Weight_String': ['50', '60', '70', '80', '90'],\n            'Height': [1.5, 1.6, 1.7, 1.8, 1.9]\n        }\n    \n    if 'Weight_String' not in data or 'Height' not in data:\n        raise ValueError(\"Input dictionary must contain 'Weight_String' and 'Height' keys.\")\n    \n    if not isinstance(data['Weight_String'], list) or not all(isinstance(w, str) for w in data['Weight_String']):\n        raise ValueError(\"The 'Weight_String' key must contain a list of strings.\")\n    \n    if not isinstance(data['Height'], list) or not all(isinstance(h, (int, float)) for h in data['Height']):\n        raise ValueError(\"The 'Height' key must contain a list of numbers.\")\n    \n    if len(data['Weight_String']) != len(data['Height']):\n        raise ValueError(\"The 'Weight_String' and 'Height' lists must be of the same length.\")\n    \n    try:\n        weights = [float(w) for w in data['Weight_String']]\n    except ValueError:\n        raise ValueError(\"All values in 'Weight_String' must be convertible to float.\")\n    \n    df = pd.DataFrame({'Weight': weights, 'Height': data['Height']})\n    \n    ax = sns.scatterplot(x='Weight', y='Height', data=df)\n    ax.set_title('Weight vs Height')\n    \n    return ax"}
{"task_id": "BigCodeBench/1082", "solution": "import pandas as pd\nfrom scipy.stats import pearsonr\nimport numpy as np\ndef task_func(data):\n    if len(data) < 2:\n        return np.nan\n    data['score'] = data['score'].astype(float)\n    grade_mapping = {'A': 5, 'B': 4, 'C': 3, 'D': 2, 'F': 1}\n    data['grade'] = data['grade'].map(grade_mapping)\n    correlation, _ = pearsonr(data['score'], data['grade'])\n    return correlation"}
{"task_id": "BigCodeBench/1083", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Input Validation\n    required_keys = ['Salary_String', 'Experience']\n    if not all(key in data for key in required_keys):\n        raise ValueError(\"Input data must contain keys: {}\".format(', '.join(required_keys)))\n    \n    # DataFrame Conversion\n    df = pd.DataFrame(data, index=[0])\n    \n    # Empty Data Handling\n    if df.empty:\n        return plt.gca()\n    \n    # Salary Conversion\n    try:\n        df['Salary'] = df['Salary_String'].str.replace(',', '').astype(float)\n    except ValueError as e:\n        raise ValueError(\"Error converting salary strings to float: {}\".format(e))\n    \n    # Salary Normalization\n    scaler = MinMaxScaler()\n    df['Normalized_Salary'] = scaler.fit_transform(df[['Salary']])\n    \n    # Data Plotting\n    plt.scatter(df['Experience'], df['Normalized_Salary'])\n    plt.xlabel('Experience')\n    plt.ylabel('Normalized Salary')\n    plt.title('Normalized Salary vs Experience')\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/1084", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import f_oneway\ndef task_func(data_file_path: str):\n    # Read the CSV file\n    df = pd.read_csv(data_file_path)\n    \n    # Convert string representations of numbers with commas into floating point numbers\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            df[col] = df[col].str.replace(',', '').astype(float)\n    \n    # Calculate the mean and standard deviation for each numerical column\n    means = df.mean()\n    std_devs = df.std()\n    \n    # Generate a histogram plot for each numerical column\n    axes = []\n    for col in df.columns:\n        if df[col].dtype in [float, int]:\n            ax = df[col].hist(bins=10)\n            axes.append(ax)\n    \n    # Perform an ANOVA test to check the statistical significance of differences between means of numerical columns\n    anova_results = []\n    if len(means) > 1:\n        for i in range(len(means)):\n            for j in range(i+1, len(means)):\n                col1 = df[df.columns[i]]\n                col2 = df[df.columns[j]]\n                f_val, p_val = f_oneway(col1, col2)\n                anova_results.append({'Column1': df.columns[i], 'Column2': df.columns[j], 'F-value': f_val, 'P-value': p_val})\n    \n    # Convert anova_results to a DataFrame\n    if anova_results:\n        anova_results = pd.DataFrame(anova_results)\n    else:\n        anova_results = pd.DataFrame()\n    \n    return means, std_devs, axes, anova_results"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation using regular expression\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_words)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return list(top_words), ax\ntext = \"This is a test text. This text is just a test.\""}
{"task_id": "BigCodeBench/1086", "solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\nNUM_SAMPLES = 1000\ndef task_func():\n    # Generate a list of random strings\n    random_strings = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)]\n    \n    # Generate a list of random floats between 0 and 10000, formatted with two decimal places and a comma as the thousands separator\n    random_floats = [f\"{np.random.uniform(0, 10000):,.2f}\" for _ in range(NUM_SAMPLES)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'String Field': random_strings,\n        'Float Field': random_floats\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/1087", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate a random sample from a normal distribution\n    sample = np.random.normal(loc=mean, scale=std_dev, size=1000)\n    \n    # Analyze skewness and kurtosis\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n    \n    # Create a histogram\n    plt.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n    plt.title('Histogram of the Sample')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    if save_plots:\n        plt.savefig('histogram.png')\n        plt.close()\n    else:\n        plt.show()\n    \n    # Create a QQ plot\n    stats.probplot(sample, dist=\"norm\", plot=plt)\n    plt.title('QQ Plot of the Sample')\n    if save_plots:\n        plt.savefig('qq_plot.png')\n        plt.close()\n    else:\n        plt.show()\n    \n    # Return skewness, kurtosis, and paths to saved plots\n    plot_paths = ['histogram.png', 'qq_plot.png'] if save_plots else []\n    return skewness, kurtosis, plot_paths"}
{"task_id": "BigCodeBench/1088", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data=None):\n    # Convert the input data to a Pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Replace values less than 0.5 with zeros\n    df[df < 0.5] = 0\n    \n    # Standardize the data using StandardScaler\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n    \n    # Convert the scaled data back to a DataFrame\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n    \n    return scaled_df"}
{"task_id": "BigCodeBench/1089", "solution": "import numpy as np\nfrom collections import Counter\ndef task_func(list_of_tuples):\n    # Extract numeric values and categories from the list of tuples\n    numbers = [t[0] for t in list_of_tuples]\n    categories = [t[1] for t in list_of_tuples]\n    \n    # Calculate the sum of numeric values\n    sum_numbers = np.sum(numbers)\n    \n    # Count the occurrences of each category\n    count_categories = dict(Counter(categories))\n    \n    # Return the sum and the dictionary of category counts\n    return (sum_numbers, count_categories)"}
{"task_id": "BigCodeBench/1090", "solution": "import ast\nimport json\nfrom collections import Counter\nimport io\nimport unittest\ndef task_func(file_pointer):\n    data = json.load(file_pointer)\n    all_keys = []\n    for item in data:\n        if isinstance(item, str):\n            item = ast.literal_eval(item)\n        if isinstance(item, dict):\n            all_keys.extend(item.keys())\n    return Counter(all_keys)"}
{"task_id": "BigCodeBench/1091", "solution": "import ast\nimport os\nimport glob\ndef task_func(directory):\n    # List to hold the dictionaries\n    dictionaries = []\n    \n    # Get all files in the directory\n    files = glob.glob(os.path.join(directory, '*'))\n    \n    for file in files:\n        # Check if the file is a text file\n        if os.path.isfile(file) and file.endswith('.txt'):\n            try:\n                # Read the file\n                with open(file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                # Use ast.literal_eval to safely evaluate the string as a Python dictionary\n                data = ast.literal_eval(content)\n                \n                # Check if the result is indeed a dictionary\n                if isinstance(data, dict):\n                    dictionaries.append(data)\n                else:\n                    raise ValueError(f\"File {file} does not contain a valid dictionary representation.\")\n            except Exception as e:\n                raise ValueError(f\"Error processing file {file}: {e}\")\n    \n    return dictionaries"}
{"task_id": "BigCodeBench/1092", "solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\ndef task_func(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return []\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    script_tags = soup.find_all('script')\n\n    result = []\n    for tag in script_tags:\n        script_content = tag.string\n        if script_content:\n            try:\n                # Attempt to evaluate the string as a Python dictionary\n                data = ast.literal_eval(script_content)\n                if isinstance(data, dict):\n                    result.append(data)\n            except (SyntaxError, ValueError):\n                # If it's not a valid dictionary, skip it\n                pass\n\n    return result"}
{"task_id": "BigCodeBench/1093", "solution": "import ast\nimport re\ndef task_func(text_file: str) -> list:\n    with open(text_file, 'r') as file:\n        content = file.read()\n    pattern = r'\\{.*?\\}'\n    matches = re.findall(pattern, content, re.DOTALL)\n    result = []\n    for match in matches:\n        try:\n            # Remove any leading/trailing whitespace and convert to a dictionary\n            dict_str = match.strip()\n            dict_obj = ast.literal_eval(dict_str)\n            if isinstance(dict_obj, dict):\n                result.append(dict_obj)\n        except (ValueError, SyntaxError):\n            # Ignore invalid dictionary strings\n            pass\n    return result"}
{"task_id": "BigCodeBench/1094", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\ndef task_func(text):\n    tokenizer = RegexpTokenizer(r'\\b\\$[a-zA-Z0-9]+\\b')\n    tokens = tokenizer.tokenize(text)\n    words = [token[1:] for token in tokens]\n    word_counts = Counter(words)\n    most_common = word_counts.most_common(5)\n    return most_common"}
{"task_id": "BigCodeBench/1095", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\ndef task_func(text, output_filename):\n    # Create a tokenizer that splits text into words based on non-word characters\n    tokenizer = RegexpTokenizer(r'\\w+')\n    # Tokenize the input text\n    words = tokenizer.tokenize(text)\n    # Filter words that start with '$' and are not solely composed of punctuation\n    filtered_words = [word for word in words if word.startswith('$') and not all(char in punctuation for char in word)]\n    # Write the filtered words to the output file\n    with open(output_filename, 'w') as file:\n        for word in filtered_words:\n            file.write(word + '\\n')\n    # Return the absolute path to the output file\n    return os.path.abspath(output_filename)"}
{"task_id": "BigCodeBench/1096", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\nPUNCTUATION = set(punctuation)\ndef task_func(text, filename):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    words = tokenizer.tokenize(text)\n    words = [word for word in words if word[0] == \"$\" and not all(char in PUNCTUATION for char in word)]\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Word\"])\n        for word in words:\n            writer.writerow([word])\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/1097", "solution": "import re\nfrom string import punctuation\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', punctuation))\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Split text into words\n    words = text.split()\n    \n    # Remove stopwords\n    words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n    \n    # Join words back into a string\n    cleaned_text = ' '.join(words)\n    \n    return cleaned_text"}
{"task_id": "BigCodeBench/1098", "solution": "import re\nfrom collections import Counter\ndef task_func(text, top_n):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|https\\S+', '', text)\n    \n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the N most common words\n    most_common = word_counts.most_common(top_n)\n    \n    return most_common"}
{"task_id": "BigCodeBench/1099", "solution": "import re\nfrom collections import Counter\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|https\\S+', '', text)\n    \n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Convert all words to lowercase\n    words = [word.lower() for word in words]\n    \n    # Remove stopwords\n    filtered_words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n    \n    # Count the frequency of each word\n    word_counts = Counter(filtered_words)\n    \n    # Convert the Counter object to a list of tuples\n    result = list(word_counts.items())\n    \n    return result"}
{"task_id": "BigCodeBench/1100", "solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef task_func(texts):\n    # Remove URLs from the text documents\n    cleaned_texts = [re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) for text in texts]\n    \n    # Initialize the TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n    \n    # Fit and transform the cleaned texts to get the TF-IDF matrix\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n    \n    # Get the feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Convert the TF-IDF matrix to a list of tuples\n    tfidf_tuples = [tuple(tfidf_matrix[i, :].A1) for i in range(tfidf_matrix.shape[0])]\n    \n    # Round the TF-IDF scores to 8 decimal places\n    tfidf_tuples = [tuple(round(score, 8) for score in doc) for doc in tfidf_tuples]\n    \n    return (tfidf_tuples, feature_names.tolist())\ntexts = [\n    \"This is the first document. It contains some words and some more words.\",\n    \"This document is the second document. It also contains words, but different ones.\",\n    \"And this is the third one. It is a short one.\"\n]"}
{"task_id": "BigCodeBench/1101", "solution": "import subprocess\nimport os\nimport glob\nimport time\ndef task_func(test_dir):\n    script_times = {}\n    for script in glob.glob(os.path.join(test_dir, '*.py')):\n        start_time = time.time()\n        subprocess.run(['python', script], check=True)\n        end_time = time.time()\n        script_times[os.path.basename(script)] = end_time - start_time\n    return script_times"}
{"task_id": "BigCodeBench/1102", "solution": "import subprocess\nimport shlex\nfrom datetime import datetime\ndef task_func(script_path: str) -> dict:\n    start_time = datetime.now()\n    try:\n        result = subprocess.run(shlex.split(script_path), capture_output=True, text=True, check=True)\n        end_time = datetime.now()\n        return {\n            'start_time': start_time,\n            'end_time': end_time,\n            'stdout': result.stdout,\n            'stderr': result.stderr\n        }\n    except subprocess.CalledProcessError as e:\n        end_time = datetime.now()\n        return {\n            'start_time': start_time,\n            'end_time': end_time,\n            'stdout': e.stdout,\n            'stderr': e.stderr\n        }"}
{"task_id": "BigCodeBench/1103", "solution": "import subprocess\nimport shutil\nimport os\ndef task_func(script_path: str, temp_dir: str) -> str:\n    try:\n        # Create a temporary directory\n        os.makedirs(temp_dir, exist_ok=True)\n        \n        # Copy the script to the temporary directory\n        shutil.copy(script_path, temp_dir)\n        \n        # Change to the temporary directory\n        os.chdir(temp_dir)\n        \n        # Execute the script\n        result = subprocess.run(['python', os.path.basename(script_path)], capture_output=True, text=True)\n        \n        # Check if the script executed successfully\n        if result.returncode == 0:\n            return \"Script executed successfully!\"\n        else:\n            return \"Script execution failed!\"\n    except Exception as e:\n        return f\"An error occurred: {e}\""}
{"task_id": "BigCodeBench/1104", "solution": "import subprocess\nimport os\nimport threading\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script file {script_path} does not exist.\")\n    \n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    def kill_process():\n        process.kill()\n    \n    timer = threading.Timer(timeout, kill_process)\n    timer.start()\n    \n    try:\n        stdout, stderr = process.communicate()\n        if process.poll() is None:\n            process.kill()\n            return \"Terminating process due to timeout.\"\n        else:\n            return \"Script executed successfully.\"\n    finally:\n        timer.cancel()"}
{"task_id": "BigCodeBench/1105", "solution": "import subprocess\nimport os\nimport time\nimport glob\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    start_time = time.time()\n    try:\n        subprocess.run(['Rscript', r_script_path], check=True)\n    except subprocess.CalledProcessError as e:\n        return False, f\"R script execution failed: {e}\"\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    if elapsed_time <= duration:\n        if os.path.exists(output_path):\n            return True, 'File generated successfully within the specified duration.'\n        else:\n            return False, 'Output file not found.'\n    else:\n        return False, 'File not generated within the specified duration.'"}
{"task_id": "BigCodeBench/1106", "solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(file_path):\n    \"\"\"\n    Determine the creation time of a file and convert it to a formatted string '%Y-%m-%d %H:%M:%S'.\n\n    Args:\n    file_path (str): The path to the file.\n\n    Returns:\n    str: The creation time of the file in the specified format.\n    \"\"\"\n    # Get the file's creation time\n    creation_time = os.path.getctime(file_path)\n    # Convert the creation time to a datetime object\n    dt_object = datetime.fromtimestamp(creation_time)\n    # Format the datetime object to the specified string format\n    formatted_time = dt_object.strftime(DATE_FORMAT)\n    return formatted_time"}
{"task_id": "BigCodeBench/1107", "solution": "from datetime import datetime\nimport pytz\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(unix_timestamp, target_timezone):\n    # Convert the Unix timestamp to a datetime object in UTC\n    utc_time = datetime.utcfromtimestamp(unix_timestamp)\n    \n    # Localize the UTC datetime to the target timezone\n    target_tz = pytz.timezone(target_timezone)\n    localized_time = utc_time.replace(tzinfo=pytz.UTC).astimezone(target_tz)\n    \n    # Format the datetime object to a string\n    formatted_time = localized_time.strftime(DATE_FORMAT)\n    \n    return formatted_time"}
{"task_id": "BigCodeBench/1108", "solution": "from collections import Counter\nimport re\ndef task_func(result):\n    # Initialize an empty list to store the values associated with the 'url' key\n    url_values = []\n    \n    # Iterate through each dictionary in the list\n    for item in result:\n        # Check if the 'url' key exists in the dictionary\n        if 'url' in item:\n            # Extract the value associated with the 'url' key\n            url_value = item['url']\n            # Append the url value to the list\n            url_values.append(url_value)\n    \n    # Use Counter to count the frequency of each url value\n    url_counts = Counter(url_values)\n    \n    # Find the most common url value(s)\n    most_common = url_counts.most_common(1)\n    \n    # If there are multiple url values with the same highest count, return all of them\n    if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n        most_common = url_counts.most_common()\n    else:\n        most_common = [most_common[0]]\n    \n    # Create a dictionary with the most common url value(s) and their counts\n    most_common_dict = {url: count for url, count in most_common}\n    \n    # Return the dictionary\n    return most_common_dict"}
{"task_id": "BigCodeBench/1109", "solution": "import os\nfrom nltk import word_tokenize\ndef task_func(file_path='File.txt'):\n    with open(file_path, 'r') as file:\n        text = file.read()\n    tokens = word_tokenize(text)\n    return tokens"}
{"task_id": "BigCodeBench/1110", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\ndef task_func(word_dict):\n    # Initialize a Counter to keep track of letter frequencies\n    letter_freq = Counter()\n    \n    # Iterate over each word in the dictionary\n    for word in word_dict.values():\n        # Count the frequency of each letter in the word\n        letter_freq.update(word)\n    \n    # Return the frequency dictionary\n    return dict(letter_freq)"}
{"task_id": "BigCodeBench/1111", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\ndef task_func(animal_dict):\n    # Filter the dictionary to only include animals from the ANIMAL constant\n    filtered_dict = {k: v for k, v in animal_dict.items() if k in ANIMAL}\n    # Flatten the list of letters into a single list\n    letters = list(itertools.chain.from_iterable(filtered_dict.values()))\n    # Count the frequency of each letter\n    frequency = Counter(letters)\n    # Sort the frequency dictionary by frequency in descending order\n    sorted_frequency = dict(sorted(frequency.items(), key=itemgetter(1), reverse=True))\n    return sorted_frequency"}
{"task_id": "BigCodeBench/1112", "solution": "import csv\nimport random\nimport os\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\ndef task_func(file_name=\"data.csv\"):\n    # Create a list to hold the data for each hour\n    hourly_data = []\n    \n    # Get the current hour\n    current_hour = datetime.now().hour\n    \n    # Generate data for each hour of the current day\n    for hour in range(24):\n        # Generate random values for Temperature, Humidity, and Pressure\n        temperature = random.uniform(*RANGE['Temperature'])\n        humidity = random.uniform(*RANGE['Humidity'])\n        pressure = random.uniform(*RANGE['Pressure'])\n        \n        # Append the data for this hour to the list\n        hourly_data.append({\n            'Hour': hour,\n            'Temperature': temperature,\n            'Humidity': humidity,\n            'Pressure': pressure\n        })\n    \n    # Write the data to a CSV file\n    with open(file_name, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Hour'] + DATA)\n        writer.writeheader()\n        for data in hourly_data:\n            writer.writerow(data)\n    \n    # Return the path to the created file\n    return os.path.abspath(file_name)"}
{"task_id": "BigCodeBench/1113", "solution": "import csv\nimport collections\ndef task_func(csv_file, emp_prefix='EMP$$'):\n    emp_dict = collections.defaultdict(int)\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n        for row in reader:\n            emp_id = row[0]\n            if emp_id.startswith(emp_prefix):\n                emp_dict[emp_id] += 1\n    return dict(emp_dict)"}
{"task_id": "BigCodeBench/1114", "solution": "from collections import defaultdict\nfrom random import randint\ndef task_func(dict1):\n    result = defaultdict(list)\n    for key, value in dict1.items():\n        if key.startswith('EMP$$'):\n            for salary in value:\n                result[key].append(salary)\n    return dict(result)\ndict1 = {\n    'EMP$$1': [1000, 2000, 3000],\n    'EMP$$2': [4000, 5000],\n    'EMP$$3': [6000, 7000, 8000, 9000],\n    'EMP$1': [100, 200],\n    'EMP$2': [300, 400],\n    'EMP$3': [500, 600, 700, 800]\n}"}
{"task_id": "BigCodeBench/1115", "solution": "import random\nfrom string import ascii_uppercase\ndef task_func(dict1):\n    # Create a dictionary to store the unique employee IDs for each department\n    department_ids = {}\n    \n    # Iterate through each department in the input dictionary\n    for department, employees in dict1.items():\n        # Initialize a list to store employee IDs for the current department\n        department_employee_ids = []\n        \n        # Iterate through each employee in the department\n        for employee in employees:\n            # Generate a random string of 5 uppercase letters\n            random_string = ''.join(random.choice(ascii_uppercase) for _ in range(5))\n            # Create the employee ID by combining the department code and the random string\n            employee_id = f\"{department}-{random_string}\"\n            # Append the employee ID to the department's list\n            department_employee_ids.append(employee_id)\n        \n        # Assign the list of employee IDs to the department in the department_ids dictionary\n        department_ids[department] = department_employee_ids\n    \n    # Return the dictionary containing unique employee IDs for all departments\n    return department_ids"}
{"task_id": "BigCodeBench/1116", "solution": "import random\nimport statistics\ndef task_func(dict1):\n    # Extract ages of employees in \"EMP$$\" department\n    ages = [employee['age'] for employee in dict1.values() if employee['department'] == \"EMP$$\"]\n    \n    # Calculate mean\n    mean_age = statistics.mean(ages)\n    \n    # Calculate median\n    median_age = statistics.median(ages)\n    \n    # Calculate mode(s)\n    mode_ages = statistics.multimode(ages)\n    \n    return (mean_age, median_age, mode_ages)\ndict1 = {\n    'emp1': {'age': 25, 'department': 'EMP$$'},\n    'emp2': {'age': 30, 'department': 'EMP$$'},\n    'emp3': {'age': 25, 'department': 'EMP$$'},\n    'emp4': {'age': 35, 'department': 'EMP$$'},\n    'emp5': {'age': 22, 'department': 'EMP$$'},\n    'emp6': {'age': 60, 'department': 'EMP$$'},\n    'emp7': {'age': 45, 'department': 'EMP$$'},\n    'emp8': {'age': 45, 'department': 'EMP$$'},\n    'emp9': {'age': 30, 'department': 'EMP$$'},\n    'emp10': {'age': 28, 'department': 'EMP$$'},\n}"}
{"task_id": "BigCodeBench/1117", "solution": "import collections\nimport random\nimport json\ndef task_func(department_data):\n    # Initialize a dictionary to hold department codes and their corresponding employee levels\n    department_levels = collections.defaultdict(list)\n    \n    # Iterate over each employee in the department_data\n    for employee in department_data:\n        # Extract the department code from the employee's ID\n        department_code = employee['id'].split('$$')[0]\n        \n        # Assign the employee's level to the corresponding department\n        department_levels[department_code].append(employee['level'])\n    \n    # Convert the dictionary to a JSON object\n    json_object = json.dumps(dict(department_levels), indent=4)\n    \n    return json_object"}
{"task_id": "BigCodeBench/1118", "solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    # Download the CSV file\n    response = requests.get(csv_url)\n    response.raise_for_status()  # Raise an error for bad status codes\n\n    # Read the CSV content\n    csv_content = response.content.decode('utf-8')\n\n    # Create a CSV reader\n    csv_reader = csv.DictReader(StringIO(csv_content))\n\n    # Convert CSV to JSON\n    json_data = [row for row in csv_reader]\n\n    # Write JSON to file\n    with open(json_file_path, 'w') as json_file:\n        json.dump(json_data, json_file, indent=4)\n\n    return json_file_path"}
{"task_id": "BigCodeBench/1119", "solution": "import codecs\nimport random\nimport string\nimport hashlib\ndef task_func(password_length=10, salt=\"salty\"):\n    # Generate a random password\n    password = ''.join(random.choice(string.ascii_letters + string.digits + string.punctuation) for _ in range(password_length))\n    \n    # Mix the password with the salt\n    mixed = password + salt\n    \n    # Hash the mixed password using SHA256\n    hashed = hashlib.sha256(mixed.encode()).hexdigest()\n    \n    return hashed"}
{"task_id": "BigCodeBench/1120", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    # Extract all URLs from the provided string\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    urls = url_pattern.findall(myString)\n    \n    # Extract domain from each URL\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n    \n    # Use IP API to get geolocation data for each domain\n    geolocation_data = {}\n    for domain in domains:\n        try:\n            response = requests.get(f'http://ip-api.com/json/{domain}?fields=status,country,region,city,isp,org,as,reverse,proxy,hosting,query,message&key={API_KEY}')\n            response.raise_for_status()\n            data = response.json()\n            geolocation_data[domain] = data\n        except requests.RequestException as e:\n            geolocation_data[domain] = None\n    \n    return geolocation_data"}
{"task_id": "BigCodeBench/1121", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    # Extract all URLs from the provided string\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    urls = url_pattern.findall(myString)\n    \n    # Extract domain from each URL\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n    \n    # Use IP API to get geolocation data for each domain\n    geolocation_data = {}\n    for domain in domains:\n        try:\n            response = requests.get(f'http://ip-api.com/json/{domain}?fields=status,country,region,city,isp,org,as,reverse,proxy,hosting,query,message&key={API_KEY}')\n            response.raise_for_status()\n            data = response.json()\n            geolocation_data[domain] = data\n        except requests.RequestException as e:\n            geolocation_data[domain] = None\n    \n    return geolocation_data"}
{"task_id": "BigCodeBench/1122", "solution": "import re\nimport socket\nimport urllib.parse\ndef task_func(myString):\n    # Regular expression to match URLs\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    # Find all URLs in the string\n    urls = url_pattern.findall(myString)\n    # Dictionary to store domain and IP address\n    domain_ip_dict = {}\n    for url in urls:\n        # Parse the URL to get the domain\n        parsed_url = urllib.parse.urlparse(url)\n        domain = parsed_url.netloc\n        try:\n            # Get the IP address of the domain\n            ip_address = socket.gethostbyname(domain)\n            domain_ip_dict[domain] = ip_address\n        except socket.gaierror:\n            # If the domain cannot be resolved, set IP address to None\n            domain_ip_dict[domain] = None\n    return domain_ip_dict"}
{"task_id": "BigCodeBench/1123", "solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\nfrom datetime import datetime\ndef task_func(myString):\n    # Regular expression to match URLs\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    urls = url_pattern.findall(myString)\n    \n    # Filter out only HTTPS URLs\n    https_urls = [url for url in urls if url.startswith('https://')]\n    \n    # Function to get domain from URL\n    def get_domain(url):\n        parsed_url = urllib.parse.urlparse(url)\n        return parsed_url.netloc\n    \n    # Function to get SSL certificate expiration date\n    def get_ssl_expiration(url):\n        try:\n            hostname = urllib.parse.urlparse(url).netloc\n            context = ssl.create_default_context()\n            with socket.create_connection((hostname, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n                    cert = ssock.getpeercert()\n                    # Convert the expiration date to UTC format\n                    expiration_date = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')\n                    return expiration_date\n        except Exception as e:\n            # Ignore any errors and return None\n            return None\n    \n    # Dictionary to store domain and SSL expiration date\n    result = {}\n    for url in https_urls:\n        domain = get_domain(url)\n        expiration_date = get_ssl_expiration(url)\n        if expiration_date is not None:\n            result[domain] = expiration_date.strftime('%Y-%m-%d %H:%M:%S')\n    \n    return result"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    urls = url_pattern.findall(myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    # Try to fetch the first URL\n    url = urls[0]\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the content with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find the title tag\n    title = soup.title\n    if title is None:\n        return \"No title tag found in the webpage.\"\n    \n    return title.string"}
{"task_id": "BigCodeBench/1125", "solution": "import re\nimport json\nimport requests\ndef task_func(myString, token):\n    # Extract URL from the string\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    urls = url_pattern.findall(myString)\n    if urls:\n        url = urls[0]\n    else:\n        return {\"error\": \"No URL found in the string\"}\n\n    # Prepare the payload\n    payload = {\"url\": url}\n    headers = {\n        \"Authorization\": f\"Bearer {token}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    # Send the POST request\n    response = requests.post(\"https://api.example.com/endpoint\", headers=headers, data=json.dumps(payload))\n\n    # Return the API response\n    return response.json()"}
{"task_id": "BigCodeBench/1126", "solution": "import re\nimport hashlib\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks, and spaces\n    cleaned_str = re.sub(r'[^a-zA-Z0-9]', '', input_str)\n    # Hash the cleaned string with SHA256\n    sha256_hash = hashlib.sha256(cleaned_str.encode()).hexdigest()\n    return sha256_hash"}
{"task_id": "BigCodeBench/1127", "solution": "import os\nimport re\nimport hashlib\ndef task_func(path, delimiter):\n    # Split the path by the delimiter\n    components = re.split(delimiter, path)\n    \n    # Initialize an empty list to store the results\n    result = []\n    \n    # Iterate over each component\n    for component in components:\n        # Check if the component is a file\n        if os.path.isfile(component):\n            # Compute the hash of the file\n            with open(component, 'rb') as f:\n                file_hash = hashlib.md5(f.read()).hexdigest()\n        else:\n            # If not a file, set hash to None\n            file_hash = None\n        # Append the component and its hash to the result list\n        result.append((component, file_hash))\n    \n    return result"}
{"task_id": "BigCodeBench/1128", "solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\ndef task_func(file_path, unknown_key):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    value = data['A'][unknown_key]['maindata'][0]['Info']\n    \n    hash_object = hashlib.sha256(value.encode())\n    hash_value = hash_object.hexdigest()\n    \n    encoded_hash = base64.b64encode(hash_value.encode()).decode()\n    \n    timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n    new_file_name = f\"hashed_value_{timestamp}.txt\"\n    new_file_path = os.path.join(os.getcwd(), new_file_name)\n    \n    with open(new_file_path, 'w') as new_file:\n        new_file.write(encoded_hash)\n    \n    return new_file_path"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON string to find the URL associated with the specified key\n    data = json.loads(json_data)\n    url = data.get(unknown_key)\n    \n    if url is None:\n        return \"Error: URL not found for the specified key.\"\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    \n    if response.status_code != 200:\n        return f\"Error: Failed to download file from {url}. Status code: {response.status_code}\"\n    \n    # Generate a timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    else:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n    \n    # Save the file with the timestamped filename\n    file_path = os.path.join(save_dir, filename)\n    with open(file_path, 'wb') as f:\n        f.write(response.content)\n    \n    return file_path"}
{"task_id": "BigCodeBench/1130", "solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\ndef task_func(directory: str) -> str:\n    # Create a dictionary to store the hashes\n    hashes = {}\n    \n    # Walk through all files in the directory and its subdirectories\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            # Construct the full path of the file\n            filepath = os.path.join(dirpath, filename)\n            # Open the file in binary mode and read its content\n            with open(filepath, 'rb') as f:\n                # Create a SHA256 hash object\n                sha256_hash = hashlib.sha256()\n                # Read the file in chunks and update the hash object\n                for byte_block in iter(lambda: f.read(4096), b\"\"):\n                    sha256_hash.update(byte_block)\n                # Store the hash in the dictionary with the file path as the key\n                hashes[filepath] = sha256_hash.hexdigest()\n    \n    # Create a Path object for the directory\n    dir_path = Path(directory)\n    # Create a Path object for the 'hashes.json' file\n    json_file_path = dir_path / 'hashes.json'\n    \n    # Write the hashes dictionary to the JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(hashes, f, indent=4)\n    \n    # Return the absolute path of the JSON file\n    return str(json_file_path.resolve())"}
{"task_id": "BigCodeBench/1131", "solution": "import hashlib\nimport binascii\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string\")\n    \n    cursor.execute(\"SELECT id, password FROM users\")\n    users = cursor.fetchall()\n    \n    updated_count = 0\n    for user in users:\n        user_id = user[0]\n        password = user[1]\n        \n        # Hash the password with SHA256 using the provided salt\n        salted_password = password + salt\n        hashed_password = hashlib.sha256(salted_password.encode()).hexdigest()\n        \n        # Update the user's password in the database\n        cursor.execute(\"UPDATE users SET password = ? WHERE id = ?\", (hashed_password, user_id))\n        updated_count += 1\n    \n    return updated_count"}
{"task_id": "BigCodeBench/1132", "solution": "import os\nimport hashlib\nimport base64\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"SALT_LENGTH cannot be negative\")\n    \n    salt = os.urandom(SALT_LENGTH)\n    salted_password = PREFIX + password + salt.decode('latin1')\n    hash_object = hashlib.sha256(salted_password.encode('utf-8'))\n    hashed_password = base64.b64encode(hash_object.digest()).decode('utf-8')\n    return hashed_password"}
{"task_id": "BigCodeBench/1133", "solution": "import json\nimport requests\ndef task_func(API_URL, endpoint, PREFIX):\n    try:\n        response = requests.get(f\"{API_URL}/{endpoint}\")\n        response.raise_for_status()\n        data = response.json()\n        filename = f\"{PREFIX}_{endpoint}.json\"\n        with open(filename, 'w') as f:\n            json.dump(data, f)\n        return filename\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")\n    except json.JSONDecodeError as e:\n        raise RuntimeError(f\"Error decoding JSON: {e}\")\n    except Exception as e:\n        raise RuntimeError(f\"An unexpected error occurred: {e}\")"}
{"task_id": "BigCodeBench/1134", "solution": "import os\nimport glob\nimport hashlib\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory {source_dir} does not exist.\")\n    \n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    files = glob.glob(os.path.join(source_dir, '*'))\n    new_files = []\n    \n    for file in files:\n        if os.path.isfile(file):\n            with open(file, 'rb') as f:\n                content = f.read()\n                md5_hash = hashlib.md5(content).hexdigest()\n                new_content = prefix + md5_hash + '\\n' + content.decode('utf-8')\n                new_file_path = os.path.join(target_dir, os.path.basename(file))\n                with open(new_file_path, 'w') as nf:\n                    nf.write(new_content)\n                new_files.append(new_file_path)\n    \n    return new_files"}
{"task_id": "BigCodeBench/1135", "solution": "import collections\nimport json\nimport requests\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    # Construct the API URL for the user's repositories\n    url = f\"{API_URL}{user}/repos\"\n    \n    # Make a GET request to the GitHub API\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the JSON response\n        data = json.loads(response.text)\n        \n        # Create a list to hold repository names and their creation dates\n        repos = []\n        \n        # Iterate over the repositories\n        for repo in data:\n            # Extract the name and creation date\n            name = repo['name']\n            created_at = repo['created_at']\n            \n            # Append to the list\n            repos.append((name, created_at))\n        \n        # Sort the repositories by creation date\n        repos_sorted = sorted(repos, key=lambda x: x[1])\n        \n        # Extract only the repository names\n        sorted_repo_names = [repo[0] for repo in repos_sorted]\n        \n        # Return the list of repository names\n        return sorted_repo_names\n    else:\n        # If the request was not successful, return an empty list\n        return []"}
{"task_id": "BigCodeBench/1136", "solution": "import bs4\nimport requests\nimport re\nimport csv\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n        return None\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    \n    # Find all text in the HTML\n    text = soup.get_text()\n    \n    # Find all email addresses using the provided regex pattern\n    emails = re.findall(regex, text)\n    \n    # Write the emails to a CSV file\n    with open(csv_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Emails\"])\n        for email in emails:\n            writer.writerow([email])\n    \n    # Return the path to the CSV file\n    return csv_path"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    # Fetch the content from the URL\n    response = requests.get(url)\n    content = response.text\n\n    # Use BeautifulSoup to parse the HTML content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n\n    # Find all script tags\n    script_tags = soup.find_all('script')\n\n    # Initialize a list to hold phone numbers\n    phone_numbers = []\n\n    # Regular expression pattern for matching phone numbers\n    pattern = re.compile(r'(\\+?\\d{1,3}[-\\.\\s]??\\d{1,4}[-\\.\\s]??\\d{1,4}[-\\.\\s]??\\d{1,9})')\n\n    # Iterate through each script tag\n    for script in script_tags:\n        # Extract text from the script tag\n        text = script.string\n        if text:\n            # Find all matches of the phone number pattern in the text\n            matches = pattern.findall(text)\n            # Extend the list with the found phone numbers\n            phone_numbers.extend(matches)\n\n    # Write the list of phone numbers to a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    # Return the list of phone numbers\n    return phone_numbers"}
{"task_id": "BigCodeBench/1138", "solution": "import numpy as np\nimport itertools\ndef task_func(matrix):\n    # Sort the 2D numpy array in ascending order\n    sorted_array = np.sort(matrix, axis=None)\n    \n    # Find all unique combinations of two elements from the sorted array\n    unique_combinations = list(itertools.combinations(sorted_array, 2))\n    \n    # Return the sorted array and the list of unique combinations\n    return (sorted_array, unique_combinations)"}
{"task_id": "BigCodeBench/1139", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Hours', 'Scores'])\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df[['Hours']], df['Scores'], test_size=0.2, random_state=42)\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n    \n    # Calculate the mean squared error\n    mse = np.mean((y_test - y_pred) ** 2)\n    \n    return mse"}

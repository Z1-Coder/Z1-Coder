{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # List all files in the directory\n    files = []\n    try:\n        files = ftp.nlst()\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # Download each file using wget\n    downloaded_files = []\n    for file in files:\n        try:\n            # Construct the wget command\n            wget_command = f\"wget -O {file} ftp://{ftp_user}:{ftp_password}@{ftp_server}/{ftp_dir}/{file}\"\n            # Execute the wget command using subprocess\n            subprocess.run(wget_command, shell=True, check=True)\n            downloaded_files.append(file)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {file}: {str(e)}\")\n    \n    return downloaded_files"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        commands = [row[0] for row in reader]\n    \n    output_files = []\n    for index, command in enumerate(commands):\n        output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n        with open(output_file_path, 'w') as output_file:\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n                output_file.write(result.stdout.decode())\n            except subprocess.CalledProcessError as e:\n                output_file.write(f\"Command failed with exit code {e.returncode}:\\n{e.output.decode()}\")\n            output_files.append(output_file_path)\n    \n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, terminate and restart it\n            proc.terminate()\n            time.sleep(2)  # Wait for the process to terminate\n            # Start the process again\n            subprocess.Popen(process_name)\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # Process is not running, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]\n    \n    if not files:\n        return None\n    \n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Check if there are any words left after removing URLs\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud().generate(' '.join(words))\n    \n    # Display the word cloud\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    # Split the dataframe into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the Random Forest Classifier\n    clf = RandomForestClassifier()\n    clf.fit(X, y)\n    \n    # Get feature importances\n    importances = clf.feature_importances_\n    \n    # Create a dataframe to hold feature names and their importances\n    feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n    \n    # Sort the dataframe by importance in descending order\n    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n    \n    # Plot the bar plot\n    plt.figure(figsize=(10,6))\n    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n    \n    # Return the trained classifier and the axes object of the plot\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = password\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    login_manager = LoginManager()\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    users = [User(1, 'user1', generate_password_hash('password1')),\n             User(2, 'user2', generate_password_hash('password2'))]\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        for user in users:\n            if user.id == int(user_id):\n                return user\n        return None\n\n    @app.route('/')\n    def home():\n        return render_template('home.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = None\n            for u in users:\n                if u.username == form.username.data:\n                    user = u\n                    break\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('home'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('home'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', user=current_user)\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Standardize the column\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data_scaled))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)\n    \n    # Remove outliers\n    data_no_outliers = data.drop(data.index[outliers])\n    \n    # Plot original data with outliers\n    plt.figure(figsize=(10,5))\n    plt.scatter(data.index, data_scaled, color='blue', label='Original Data')\n    plt.scatter(data.index[outliers], data_scaled[outliers], color='red', label='Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.show()\n    \n    # Plot data without outliers\n    plt.figure(figsize=(10,5))\n    plt.scatter(data_no_outliers.index, scaler.transform(data_no_outliers[[column]]), color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.show()\n    \n    # Return original data, data without outliers, and outliers indices\n    return (data, data_no_outliers, outliers)"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters < 2:\n        raise ValueError(\"n_clusters must be an integer greater than 1\")\n    \n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.predict(data)\n    centroids = kmeans.cluster_centers_\n    \n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n    ax.set_title('K-means Clustering')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    \n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n    \n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df[\"PC1\"], transformed_df[\"PC2\"])\n    ax.set_xlabel(\"Principal Component 1\")\n    ax.set_ylabel(\"Principal Component 2\")\n    ax.set_title(\"PCA Scatter Plot\")\n    \n    return transformed_df, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    pairplot = sns.pairplot(df, hue='species')\n\n    # Set the title\n    pairplot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n\n    # Label the axes\n    for ax in pairplot.axes.flatten():\n        ax.set_xlabel(ax.get_xlabel(), fontsize=10)\n        ax.set_ylabel(ax.get_ylabel(), fontsize=10)\n\n    # Show the plot\n    plt.show()\n\n    # Return the figure object\n    return pairplot.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        today = datetime.today()\n        dates = [today - pd.DateOffset(days=x) for x in range(30)]\n        \n        # Generate random values for the time series\n        values = [random.randint(0, 100) for _ in range(30)]\n        \n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        \n        # Plot the data\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], label='Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n        ax.legend()\n        \n        # Set the font to Arial\n        plt.rcParams['font.family'] = 'Arial'\n        \n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    \"\"\"\n    Draw the correlation heatmap of the Boston Housing dataset using Seaborn, with an option to save it to a specified file.\n    \n    Parameters:\n    - data_url (str): URL to the Boston Housing dataset.\n    - seed (int): Seed for reproducibility.\n    - save_path (str, optional): Path to save the heatmap plot. If None, the plot is not saved.\n    \n    Returns:\n    - matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n    \n    Raises:\n    - ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    # Load the dataset\n    try:\n        df = pd.read_csv(data_url, header=None)\n    except Exception as e:\n        raise ValueError(f\"Error loading the dataset: {e}\")\n    \n    # Set the column names\n    column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n    df.columns = column_names\n    \n    # Set the random seed\n    np.random.seed(seed)\n    \n    # Compute the correlation matrix\n    corr = df.corr()\n    \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    \n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n    \n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    \n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    \n    # If save_path is provided, save the plot\n    if save_path:\n        try:\n            plt.savefig(save_path)\n        except Exception as e:\n            raise ValueError(f\"Error saving the plot: {e}\")\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Parameters:\n    - df (pandas.DataFrame): DataFrame containing the time series data.\n    - freq (str): Frequency string for the time series.\n    - decomposition_model (str): Model for decomposition, either 'additive' or 'multiplicative'.\n\n    Returns:\n    - tuple: A tuple containing the decomposition result (DecomposeResult object) and the matplotlib Axes object.\n\n    Raises:\n    - ValueError: If 'df' is not a DataFrame, lacks required columns, or contains invalid data types.\n    - ValueError: If 'freq' is not a valid frequency string.\n    - ValueError: If 'decomposition_model' is not 'additive' or 'multiplicative'.\n    \"\"\"\n\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df must be a pandas DataFrame\")\n\n    # Check if 'value' column exists in df\n    if 'value' not in df.columns:\n        raise ValueError(\"df must contain a 'value' column\")\n\n    # Check if 'value' column contains numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data\")\n\n    # Check if 'freq' is a valid frequency string\n    valid_freqs = ['D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S']\n    if freq not in valid_freqs:\n        raise ValueError(f\"Invalid frequency string. Valid frequencies are: {', '.join(valid_freqs)}\")\n\n    # Check if 'decomposition_model' is either 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"decomposition_model must be either 'additive' or 'multiplicative'\")\n\n    # Set the index of the DataFrame to datetime with the specified frequency\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    df.index.freq = freq\n\n    # Perform decomposition\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model)\n\n    # Plot the decomposition\n    fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(10, 12))\n    decomposition.plot(ax=axes)\n    plt.tight_layout()\n\n    return decomposition, axes"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date\")\n    \n    random_seed(seed)\n    delta = end_date - start_date\n    days = delta.days + 1  # inclusive of both start and end dates\n    random_dates = [start_date + timedelta(days=randint(0, days-1)) for _ in range(days)]\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(\"Directory does not exist\")\n    \n    # Add '12' to the list\n    my_list.append('12')\n    \n    # Determine the number of files to concatenate\n    num_files = sum(map(int, my_list))\n    \n    # Get the list of CSV files in the directory\n    files = glob.glob(os.path.join(file_dir, '*' + file_ext))\n    \n    if not files:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n    \n    # Read the first 'num_files' CSV files into a list of DataFrames\n    dfs = []\n    for file in files[:num_files]:\n        dfs.append(pd.read_csv(file))\n    \n    # Concatenate the DataFrames\n    result_df = pd.concat(dfs, ignore_index=True)\n    \n    return result_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric values (int or float)\")\n    \n    # Enhance 'my_list' by appending the number 12\n    my_list.append(12)\n    \n    # Calculate the sum of elements in 'my_list'\n    total_sum = sum(my_list)\n    \n    # Generate a list of random integers based on the sum of elements in 'my_list', limited by 'size'\n    random_numbers = [randint(1, 100) for _ in range(min(total_sum, size))]\n    \n    # Measure the time taken for this process\n    start_time = time.time()\n    # Generate the list of random numbers\n    random_numbers = [randint(1, 100) for _ in range(min(total_sum, size))]\n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    # Plot a histogram of the generated random numbers\n    plt.hist(random_numbers, bins=range(1, 101), align='left', rwidth=0.8)\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    plt.show()\n    \n    # Return the time taken and the matplotlib Axes object for the histogram\n    return (time_taken, plt.gca())"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.ConnectionError as e:\n        raise ConnectionError(f\"Connection error: {e}\")\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP error: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    tables = soup.find_all('table')\n\n    if not tables:\n        raise ValueError(\"No table found on the page\")\n\n    table = tables[0]\n    headers = []\n    rows = []\n\n    header_row = table.find('tr')\n    if header_row:\n        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n    else:\n        # If no headers, use default column names\n        headers = [f\"Column {i+1}\" for i in range(len(table.find('tr').find_all('td')))]\n\n    for row in table.find_all('tr'):\n        cells = row.find_all(['th', 'td'])\n        if cells:\n            rows.append([cell.get_text(strip=True) for cell in cells])\n\n    df = pd.DataFrame(rows, columns=headers)\n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if numeric_cols.empty:\n        raise ValueError(\"No numeric columns found in the DataFrame.\")\n    \n    axes = []\n    for col in numeric_cols:\n        ax = plt.subplot(len(numeric_cols), 1, numeric_cols.get_loc(col) + 1)\n        df[col].hist(ax=ax, bins=30)\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans a specified IP address range and checks if a specified port is open on each IP.\n    The function returns a dictionary with IP addresses as keys and a boolean indicating the port's status (True if open, False otherwise).\n\n    Args:\n    ip_range (str): The IP address range to scan, in CIDR notation (e.g., '192.168.1.0/24').\n    port (int): The port to check on each IP address.\n\n    Returns:\n    dict: A dictionary mapping IP addresses to their port status (True if open, False otherwise).\n    \"\"\"\n    ip_network = IPv4Network(ip_range)\n    ip_list = list(ip_network.hosts())\n    results = {}\n\n    def scan_ip(ip):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        results[str(ip)] = result == 0\n        sock.close()\n\n    threads = []\n    for ip in ip_list:\n        thread = Thread(target=scan_ip, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n\n    # Initialize lists to hold extracted data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read its contents\n    with open(log_file, 'r') as file:\n        lines = file.readlines()\n\n    # Iterate through each line in the log file\n    for line in lines:\n        match = re.match(pattern, line)\n        if match:\n            # Extract the type, timestamp, and message from the log entry\n            log_type = match.group(1)\n            timestamp_str = match.group(2)\n            message = match.group(3)\n\n            # Validate the timestamp\n            try:\n                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n            except ValueError:\n                raise ValueError(f\"Invalid timestamp: {timestamp_str}\")\n\n            # Append the extracted data to the lists\n            types.append(log_type)\n            timestamps.append(timestamp)\n            messages.append(message)\n\n    # Check if any valid log entries were found\n    if not types:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Define the output CSV file path\n    csv_file = 'structured_logs.csv'\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file, index=False)\n\n    # Return the file path to the newly created CSV file\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Remove punctuation and split text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    if word_lengths:\n        plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), rwidth=rwidth)\n        plt.xlabel('Word Length')\n        plt.ylabel('Frequency')\n        plt.title('Distribution of Word Lengths')\n        plt.show()\n    else:\n        print(\"No words found in the text.\")\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\ndef task_func(df):\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or missing necessary columns 'Title' and 'Content'\")\n\n    # Extract articles with titles containing 'like' or 'what' (case-insensitive)\n    keywords = ['like', 'what']\n    mask = df['Title'].str.contains('|'.join(keywords), case=False)\n    articles = df[mask]\n\n    # Tokenize and count word frequencies in the content, excluding punctuation\n    all_words = []\n    for content in articles['Content']:\n        # Remove punctuation\n        content = content.translate(str.maketrans('', '', punctuation))\n        # Tokenize\n        words = nltk.word_tokenize(content)\n        all_words.extend(words)\n\n    # Count word frequencies\n    word_counts = Counter(all_words)\n\n    return dict(word_counts)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Preprocess the text data\n    dataframe[text_column] = dataframe[text_column].str.lower()\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n    \n    # Vectorize the text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert the vectorized data into a DataFrame\n    vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return vectorized_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Missing 'Lon' or 'Lat' keys in the dictionary.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values must be tuples.\")\n    \n    lon_range = dic['Lon']\n    lat_range = dic['Lat']\n    \n    city_coords = []\n    for city in cities:\n        lon = np.random.uniform(lon_range[0], lon_range[1])\n        lat = np.random.uniform(lat_range[0], lat_range[1])\n        city_coords.append((city, Point(lon, lat)))\n    \n    gdf = gpd.GeoDataFrame(city_coords, columns=['City', 'Coordinates'])\n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions must be strings\")\n    if not all(isinstance(tz, str) for tz in timezones.values()):\n        raise ValueError(\"All timezones must be strings\")\n    if not isinstance(seed, int):\n        raise ValueError(\"Seed must be an integer\")\n    \n    set_seed(seed)\n    report = []\n    for city in cities:\n        local_tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(local_tz)\n        weather = weather_conditions[randint(0, len(weather_conditions)-1)]\n        report.append({\n            'City': city,\n            'Local Time': local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather\n        })\n    return pd.DataFrame(report)"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    stats = {\n        'count': len(walk),\n        'mean': np.mean(walk),\n        'std': np.std(walk),\n        'min': np.min(walk),\n        '5th percentile': np.percentile(walk, 5),\n        '25th percentile': np.percentile(walk, 25),\n        'median': np.median(walk),\n        '75th percentile': np.percentile(walk, 75),\n        '95th percentile': np.percentile(walk, 95),\n        'max': np.max(walk)\n    }\n    \n    fig, ax = plt.subplots()\n    ax.plot(walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    \n    return stats, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Download the zip file from the URL\n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download file from {url}\")\n\n    # Save the zip file to a temporary location\n    with open(\"temp.zip\", \"wb\") as f:\n        f.write(response.content)\n\n    # Extract the contents to the specified directory\n    with zipfile.ZipFile(\"temp.zip\", \"r\") as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Get the list of extracted files\n    extracted_files = os.listdir(destination_directory)\n\n    # Remove the temporary zip file\n    os.remove(\"temp.zip\")\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Parameters:\n    - seed (int): Seed for random number generation.\n    - image_size (tuple): Size of the image (height, width, channels).\n    - range_low (int): Lower bound for random values.\n    - range_high (int): Upper bound for random values.\n\n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object of the plot.\n    - image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n    - ValueError: If range_low is not less than range_high.\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size, dtype=np.uint8)\n    ax = plt.imshow(image)\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file {audio_file} does not exist.\")\n    \n    # Read the audio file\n    audio_data, sample_rate = sf.read(audio_file)\n    \n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(audio_data**2)))\n    \n    # Normalize the list L to MxN matrix\n    L = np.array(L)\n    L = L.reshape(M, N)\n    L_normalized = L / np.max(np.abs(L))\n    \n    # Generate the spectrogram\n    f, t, Sxx = signal.stft(L_normalized, fs=sample_rate)\n    Sxx_db = 20 * np.log10(np.abs(Sxx) + 1e-10)\n    \n    # Adjust the amplitude based on SPL\n    Sxx_db_adjusted = Sxx_db + spl\n    \n    # Create a figure for the spectrogram\n    fig, ax = plt.subplots()\n    img = ax.imshow(Sxx_db_adjusted, aspect='auto', origin='lower', extent=[t.min(), t.max(), f.min(), f.max()], cmap='viridis')\n    ax.set_xlabel('Time [s]')\n    ax.set_ylabel('Frequency [Hz]')\n    ax.set_title('Spectrogram with SPL Adjustment')\n    plt.colorbar(img, ax=ax)\n    \n    return L_normalized, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = []\n    for item in original:\n        if isinstance(item, tuple):\n            for sub_item in item:\n                if isinstance(sub_item, (int, float)):\n                    numeric_values.append(sub_item)\n        elif isinstance(item, (int, float)):\n            numeric_values.append(item)\n    \n    # Convert the list of numeric values to a numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Compute basic statistics\n    mean = np.mean(numeric_array)\n    std_dev = np.std(numeric_array)\n    minimum = np.min(numeric_array)\n    maximum = np.max(numeric_array)\n    statistics = {\n        'mean': mean,\n        'standard_deviation': std_dev,\n        'minimum': minimum,\n        'maximum': maximum\n    }\n    \n    # Generate a histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, density=True, alpha=0.6, bins='auto')\n    # Fit a normal distribution to the data\n    mu, std = stats.norm.fit(numeric_array)\n    # Generate the PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return numeric_array, statistics, ax\noriginal = [(1, 2), 3, (4, 5), 6]"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array])\n    \n    # Plot the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array[0], label='Normalized')\n    ax.legend()\n    \n    return original_array, normalized_array[0], ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Adds a new key \"a\" with the value 1 to the dictionary.\n    data['a'] = 1\n\n    # 2. Generates a signal based on the values in \"data\".\n    # Assuming the signal is a simple sine wave for demonstration\n    time = np.arange(0, len(data), 1)\n    signal = np.sin(2 * np.pi * 5 * time / sample_rate)\n\n    # 3. Runs a Fast Fourier Transform (FFT) on the signal.\n    fft_result = fftpack.fft(signal)\n\n    # 4. Plots and returns the FFT of the signal.\n    freqs = fftpack.fftfreq(len(signal)) * sample_rate\n    plt.plot(freqs, np.abs(fft_result))\n    plt.xlabel('Frequency')\n    plt.ylabel('Amplitude')\n    plt.title('FFT of the Signal')\n    plt.show()\n\n    return fft_result, plt\ndata = {'key1': 10, 'key2': 20}"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nclass RequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_type = self.headers.get('Content-Type')\n        if content_type != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Content-Type header is not application/json')\n            return\n        length = int(self.headers.get('Content-Length', 0))\n        data = self.rfile.read(length)\n        try:\n            json_data = json.loads(data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n            return\n        if 'data' not in json_data:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'No data key in request')\n            return\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\ndef task_func():\n    http.server.test(HandlerClass=RequestHandler, port=8000)"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            try:\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(b'Invalid JSON')\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            for key in required_keys:\n                if key not in email_data:\n                    self.send_response(400)\n                    self.end_headers()\n                    self.wfile.write(f'Missing {key} in email data'.encode())\n                    return\n\n            try:\n                server = smtplib.SMTP(smtp_server, smtp_port)\n                server.starttls()\n                server.login(smtp_username, smtp_password)\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n                server.sendmail(smtp_username, [email_data['to']], msg.as_string())\n                server.quit()\n                self.send_response(200)\n                self.end_headers()\n                self.wfile.write(b'Email sent successfully')\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.end_headers()\n                self.wfile.write(b'Authentication Failed')\n            except Exception as e:\n                self.send_response(500)\n                self.end_headers()\n                self.wfile.write(f'Error: {str(e)}'.encode())\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    total_words = 0\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".txt\"):\n                with open(os.path.join(root, file), 'r') as f:\n                    words = f.read().split()\n                    total_words += len(words)\n    with open(filename, 'w') as f:\n        json.dump({'total_words': total_words}, f)\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns,\n    and optionally visualizes the correlation matrix using a heatmap.\n\n    Parameters:\n    - df: pandas DataFrame with columns 'Date' and 'Value' where 'Value' contains lists of numbers.\n    - plot: boolean, if True, returns a matplotlib Axes object containing the heatmap plot.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n    - Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n\n    Raises:\n    - ValueError: If the DataFrame is empty or 'Value' column contains invalid lists.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    if not all(isinstance(x, list) and all(isinstance(y, (int, float)) for y in x) for x in df['Value']):\n        raise ValueError(\"The 'Value' column must contain lists of numbers.\")\n    \n    # Split the 'Value' column into separate columns\n    df = pd.concat([df[COLUMNS[0]], pd.DataFrame(df[COLUMNS[1]].tolist())], axis=1)\n    \n    # Calculate the Pearson correlation matrix\n    corr_matrix = df.corr(method='pearson')\n    \n    if plot:\n        # Create a heatmap of the correlation matrix\n        plt.figure(figsize=(10, 8))\n        heatmap = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16)\n        return corr_matrix, heatmap\n    else:\n        return corr_matrix"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Create a dictionary to hold the data\n    data = {field: [] for field in FIELDS}\n    for student in STUDENTS:\n        for field in FIELDS:\n            grade = random.randint(0, 100)\n            data[field].append(grade)\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject\n    avg_subjects = df.mean()\n    avg_subjects.name = 'Average'\n    \n    # Append the average grades to the DataFrame\n    df = df.append(avg_subjects)\n    \n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Create a list to hold the data for all people\n    people_data = []\n    \n    # Generate data for each person\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{random.randint(1, 1000)}\"\n        age = random.randint(18, 80)\n        height = random.uniform(1.5, 2.0)\n        weight = random.uniform(50, 100)\n        people_data.append([name, age, height, weight])\n    \n    # Calculate averages\n    ages = [person[1] for person in people_data]\n    heights = [person[2] for person in people_data]\n    weights = [person[3] for person in people_data]\n    avg_age = mean(ages)\n    avg_height = mean(heights)\n    avg_weight = mean(weights)\n    \n    # Append averages to the data\n    people_data.append([f\"Average\", avg_age, avg_height, avg_weight])\n    \n    # Write to CSV\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(people_data)\n    \n    return filename"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Create a dictionary to hold the files organized by their first text outside of square brackets\n    organized_files = {}\n    \n    # Walk through all files in the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Read the first line of the file\n            with open(file_path, 'r') as f:\n                first_line = f.readline()\n            # Use regular expression to find the first text not enclosed in square brackets\n            match = re.search(r'(?<=\\[).+?(?=\\])', first_line)\n            if match:\n                # Extract the text\n                text = match.group().strip()\n                # If the text is not already a key in the dictionary, add it\n                if text not in organized_files:\n                    organized_files[text] = []\n                # Move the file to the corresponding subdirectory\n                sub_dir = os.path.join(directory, text)\n                if not os.path.exists(sub_dir):\n                    os.makedirs(sub_dir)\n                shutil.move(file_path, os.path.join(sub_dir, file))\n                organized_files[text].append(file)\n            else:\n                # If no matching text is found, do not move the file\n                pass\n    \n    # Return the directory path with organized files and the dictionary of organized files\n    return (directory, organized_files)"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    exit_codes = []\n    threads = []\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n    for thread in threads:\n        thread.join()\n    return exit_codes\ndef run_file(file):\n    try:\n        result = subprocess.run(file, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        exit_code = result.returncode\n        print(f\"File {file} exited with code {exit_code}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"File {file} failed with error: {e}\")\n        exit_code = e.returncode\n    return exit_code"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    # List to store the results\n    results = []\n    \n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    # Iterate over each .bat file\n    for file in bat_files:\n        try:\n            # Execute the .bat file\n            result = subprocess.run(file, capture_output=True, text=True)\n            # Append the file name and its exit code to the results list\n            results.append((os.path.basename(file), result.returncode))\n        except Exception as e:\n            # If there is an error, append the file name and None\n            results.append((os.path.basename(file), None))\n    \n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' is not in the DataFrame.\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    if df[col].dtype in [int, float]:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n        axes[0].set_title('Histogram with KDE')\n    else:\n        raise ValueError(\"The specified column must be numerical for a histogram.\")\n    \n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title('Box Plot')\n    \n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n    \n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr)"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    try:\n        # Load data from Excel spreadsheet\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The specified Excel file does not exist at the given path.\")\n    except KeyError:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n    \n    # Calculate mean and standard deviation of each column\n    mean_values = df.mean().to_dict()\n    std_dev_values = df.std().to_dict()\n    \n    # Create a dictionary to hold the results\n    results = {\n        'mean': mean_values,\n        'std_dev': std_dev_values\n    }\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    x = np.arange(len(mean_values))  # the label locations\n    width = 0.35  # the width of the bars\n    \n    # Create bars for mean and standard deviation\n    rects1 = ax.bar(x - width/2, mean_values.values(), width, label='Mean')\n    rects2 = ax.bar(x + width/2, std_dev_values.values(), width, label='Standard Deviation')\n    \n    # Add some text for labels, title, and custom x-axis tick labels, etc.\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(mean_values.keys())\n    ax.legend()\n    \n    # Attach a text label above each bar in *both* means and standard deviations\n    def autolabel(rects):\n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate('{}'.format(height),\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')\n    \n    autolabel(rects1)\n    autolabel(rects2)\n    \n    # Show the plot\n    plt.show()\n    \n    return results, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n    \n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n    \n    days = list(day_counts.keys())\n    counts = list(day_counts.values())\n    \n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    random.seed(seed)\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        return \"No files to move.\"\n    file_to_move = random.choice(files)\n    shutil.move(os.path.join(src_dir, file_to_move), os.path.join(dest_dir, file_to_move))\n    return file_to_move"}
{"task_id": "BigCodeBench/374", "solution": "import re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Initialize a counter for the number of Excel files processed\n    file_count = 0\n\n    # Use glob to find all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Iterate over each file found\n    for file in xlsx_files:\n        # Load the workbook\n        wb = load_workbook(file, data_only=True)\n\n        # Iterate over each sheet in the workbook\n        for sheet in wb.sheetnames:\n            ws = wb[sheet]\n\n            # Iterate over each cell in the sheet\n            for row in ws.iter_rows():\n                for cell in row:\n                    # Check if the cell contains a string\n                    if cell.data_type == 's':\n                        # Use regex to find all double quotes and prefix them with a double backslash\n                        cell.value = re.sub(r'(\")', r'\\\\\\\"', str(cell.value))\n\n        # Save the changes back to the file\n        wb.save(file)\n\n        # Increment the file counter\n        file_count += 1\n\n    # Return the number of Excel files processed\n    return file_count"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    t = np.linspace(0, 1, sample_size)\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves')\n    ax.legend()\n    \n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.getenv('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = bool(os.getenv('MAIL_USE_TLS', False))\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', None)\n    mail = Mail(app)\n    return (mail, app.config)"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the path {excel_file_path}\")\n    \n    # Read the Excel file\n    df = pd.read_excel(full_path)\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file\")\n    \n    # Calculate mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n    \n    # Return the results in a dictionary\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"standard_deviation\": std_dev\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Create a Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n    \n    # Compile the model\n    optimizer = SGD(learning_rate=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=10, validation_data=(X_test, Y_test), verbose=0)\n    \n    # Plot the model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.show()\n    \n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Create a Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid')\n    ])\n    \n    # Compile the model with binary cross-entropy loss and an SGD optimizer specifying a learning rate\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=learning_rate), metrics=['accuracy'])\n    \n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n    \n    # Predict probabilities on the test set\n    Y_pred = model.predict(X_test)\n    \n    # Compute ROC curve and ROC area\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    # Return the trained model and the axes object for the ROC plot\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    \n    # Check if the image was read successfully\n    if image is None:\n        raise FileNotFoundError(f\"Failed to read the image file {image_path}.\")\n    \n    # Reshape the image to a 2D array of pixels\n    pixels = image.reshape((-1, 3))\n    \n    # Initialize KMeans\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    \n    # Fit the model and predict the cluster labels\n    labels = kmeans.fit_predict(pixels)\n    \n    # Replace each pixel with its centroid\n    segmented_pixels = kmeans.cluster_centers_[labels]\n    \n    # Reshape back to the original image shape\n    segmented_image = segmented_pixels.reshape(image.shape)\n    \n    return image, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', flatten the result,\n    apply KMeans clustering to the flattened data, and visualize it.\n\n    Parameters:\n    - P (np.ndarray): A matrix.\n    - T (np.ndarray): A 3D tensor.\n    - n_clusters (int): Number of clusters for KMeans.\n    - random_state (int): Random state for KMeans.\n    - n_init (int): Number of times the KMeans algorithm will be run with different centroid seeds.\n\n    Returns:\n    - cluster_result (np.ndarray): The result of KMeans clustering.\n    - ax (plt.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n    \n    # Flatten the result\n    flattened = product.flatten()\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n    \n    # Visualization\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax\nP = np.random.rand(10, 10)\nT = np.random.rand(10, 10, 10)"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n\n    Parameters:\n    - points: A numpy array of shape (n, 2) representing n points in 2D space.\n    - seed: An integer seed for the random number generator to ensure reproducibility.\n\n    Returns:\n    - A tuple (vor, ax) where:\n        - vor is a Voronoi object representing the Voronoi diagram of the points.\n        - ax is the axes of the plotted Voronoi diagram.\n\n    Raises:\n    - TypeError: If points is not a numpy array or does not have the correct shape.\n    - ValueError: If points do not have exactly 2 columns.\n    \"\"\"\n    # Check if points is a numpy array\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Points must be a numpy array.\")\n    \n    # Check if points has the correct shape\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Points must be a 2D array with shape (n, 2).\")\n    \n    # Apply jittering to the points\n    np.random.seed(seed)\n    jitter = 0.1 * np.random.rand(points.shape[0], 2)\n    points_jittered = points + jitter\n    \n    # Calculate the Voronoi diagram\n    vor = Voronoi(points_jittered)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    return vor, ax\npoints = np.array([[0, 0], [1, 1], [2, 0], [0, 2], [1, 0]])"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n    \n    files = glob.glob(os.path.join(src_dir, f'*{ext}'))\n    moved_files = []\n    for file in files:\n        file_name = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, file_name)\n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_file)\n            moved_files.append(dest_file)\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    \n    if not data:\n        return pd.DataFrame()\n    \n    def normalize_value(val):\n        if isinstance(val, list):\n            return [normalize_value(item) for item in val]\n        elif isinstance(val, str):\n            try:\n                return float(val)\n            except ValueError:\n                return val\n        elif isinstance(val, (int, float)):\n            return val * 2\n        else:\n            return val\n    \n    normalized_data = {key: normalize_value(val) for key, val in data.items()}\n    \n    return pd.DataFrame(normalized_data)"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n    \n    try:\n        process = subprocess.Popen(script_path, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to execute the script: {e}\")\n    \n    start_time = time.time()\n    cpu_usage = 0\n    memory_usage = 0\n    while True:\n        if process.poll() is not None:\n            break\n        if time.time() - start_time > timeout:\n            process.terminate()\n            raise TimeoutError(\"Script execution timed out.\")\n        \n        try:\n            current_process = psutil.Process(process.pid)\n            cpu_usage += current_process.cpu_percent(interval=0.1)\n            memory_usage += current_process.memory_info().rss\n        except psutil.NoSuchProcess:\n            raise RuntimeError(\"The process was not found.\")\n        except psutil.AccessDenied:\n            raise PermissionError(\"Access denied to the process.\")\n        \n        time.sleep(0.1)\n    \n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    if N < len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n    else:\n        categories = np.random.choice(CATEGORIES, size=N, replace=True)\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    df = pd.DataFrame({\"x\": x, \"y\": y, \"category\": categories})\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(x, y, c=categories, cmap='viridis')\n    return df, ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate timestamps\n    start = datetime.fromisoformat(start_time)\n    end = datetime.fromisoformat(end_time)\n    timestamps = pd.date_range(start, end, freq=f'{step}S')\n    \n    # Generate random values from a normal distribution\n    values = np.random.normal(loc=0, scale=1, size=len(timestamps))\n    \n    # Add a linear trend\n    values += trend * np.arange(len(timestamps))\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n    \n    # Plot the time series\n    ax = df.plot(x='Time', y='Value', figsize=(10, 5))\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    \n    return ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(epoch_milliseconds, random_seed=0, products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"]):\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_date = datetime.now()\n    \n    # Check if start_date is in the future\n    if start_date > current_date:\n        raise ValueError(\"Start date is in the future.\")\n    \n    # Set random seed\n    random.seed(random_seed)\n    \n    # Initialize list to hold sales data\n    sales_data = []\n    \n    # Generate sales data for each day between start_date and current_date\n    current = start_date\n    while current <= current_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({'Product': product, 'Date': current, 'Sales': sales})\n        current += timedelta(days=1)\n    \n    # Create DataFrame from sales data\n    df = pd.DataFrame(sales_data)\n    \n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import json\nimport pandas as pd\nimport os"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    dates = [datetime.today() - timedelta(days=i) for i in range(days_in_past)]\n    data = []\n    for date in dates:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    df = df.sort_values(by=\"Date\")\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Create a list to hold the data for each stock\n    data = []\n    \n    # Get the current date\n    end_date = datetime.now()\n    \n    # Generate data for each stock\n    for stock in stock_names:\n        # Create a list to hold the prices for this stock\n        prices = []\n        \n        # Generate random prices for the specified number of days\n        for _ in range(days_in_past):\n            # Generate a random price between 0.0 and 1.0\n            price = np.random.rand()\n            prices.append(price)\n        \n        # Reverse the list to have the most recent price first\n        prices.reverse()\n        \n        # Create a list of dates for the past days\n        dates = [end_date - timedelta(days=i) for i in range(days_in_past)]\n        \n        # Append the data for this stock to the overall data list\n        data.append({\n            \"stock\": [stock] * days_in_past,\n            \"date\": dates,\n            \"price\": prices\n        })\n    \n    # Combine all the data into a single DataFrame\n    combined_data = pd.concat([pd.DataFrame(d) for d in data], ignore_index=True)\n    \n    # Sort the DataFrame by date\n    combined_data = combined_data.sort_values(by=\"date\")\n    \n    # Reset the index\n    combined_data = combined_data.reset_index(drop=True)\n    \n    return combined_data"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r', newline='', encoding='utf-8') as f1, open(file_path2, 'r', newline='', encoding='utf-8') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            lines1 = list(reader1)\n            lines2 = list(reader2)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both files not found.\")\n    except csv.Error as e:\n        raise ValueError(f\"Error reading CSV files: {e}\")\n    \n    if not lines1 or not lines2:\n        raise ValueError(\"One or both files are empty.\")\n    \n    differences = []\n    max_lines = max(len(lines1), len(lines2))\n    \n    for i in range(max_lines):\n        line1 = lines1[i] if i < len(lines1) else []\n        line2 = lines2[i] if i < len(lines2) else []\n        \n        if line1 == line2:\n            status = ' '\n            content = ' '.join(line1)\n        else:\n            diff = ndiff(line1, line2)\n            status = ''.join(['+' if c[0] == '+' else '-' if c[0] == '-' else ' ' for c in diff])\n            content = ' '.join(line1 + line2)\n        \n        differences.append({'Line Number': i+1, 'Status': status, 'Content': content})\n    \n    df = pd.DataFrame(differences)\n    return df"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None)\n    \n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None)\n    \n    stats = df[column].agg(['sum', 'mean', 'min', 'max']).to_dict()\n    plt.pie(df[column], labels=df['Age'], autopct='%1.1f%%')\n    plt.title(f'Pie chart of {column}')\n    plt.show()\n    \n    return (stats, plt.gca())"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise KeyError(\"The specified column is not valid.\")\n    \n    if not df[column].dtype in [np.int64, np.float64]:\n        raise TypeError(\"The specified column is not numeric.\")\n    \n    if any(df[column] < 0):\n        raise ValueError(\"Negative values are not allowed in the specified column.\")\n    \n    summary = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    plt.plot(df['Date'], df[column])\n    plt.title(f'Line Chart of {column}')\n    plt.xlabel('Date')\n    plt.ylabel(column)\n    plt.show()\n    \n    return (summary, plt.gca())"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    results = defaultdict(list)\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                results[key].append(value)\n    \n    means = {}\n    medians = {}\n    for key, values in results.items():\n        if values:\n            means[key] = np.mean(values)\n            medians[key] = np.median(values)\n        else:\n            means[key] = np.nan\n            medians[key] = np.nan\n    \n    df = pd.DataFrame({\n        'mean': means,\n        'median': medians\n    }).sort_index()\n    \n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be a CSV file with a .csv extension.\")\n    \n    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n        reader = csv.DictReader(file)\n        rows = list(reader)\n    \n    # Convert list of dictionaries to a DataFrame\n    df = pd.DataFrame(rows)\n    \n    # Identify duplicate rows\n    duplicate_rows = df.duplicated(keep=False)\n    \n    # Count duplicates\n    duplicate_counts = Counter()\n    for index, row in df.iterrows():\n        if duplicate_rows[index]:\n            duplicate_counts[tuple(row)] += 1\n    \n    # Convert duplicated rows into a pandas DataFrame\n    duplicated_df = pd.DataFrame([dict(row) for row in duplicate_counts if duplicate_counts[row] > 1])\n    \n    # Plotting\n    counts = [duplicate_counts[tuple(row)] for row in duplicated_df.to_dict('records')]\n    labels = [str(row) for row in duplicated_df.to_dict('records')]\n    \n    fig, ax = plt.subplots()\n    ax.bar(labels, counts)\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Duplicate Rows in CSV File')\n    \n    return dict(duplicate_counts), ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Ensure 'age' is not negative\n    if df['age'].min() < 0:\n        raise ValueError(\"Age cannot be negative\")\n    \n    # Round down ages to nearest integer\n    df['age'] = df['age'].apply(np.floor)\n    \n    # Identify duplicates in 'name' column\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    \n    if duplicates.empty:\n        return Counter(), None\n    \n    # Record age distribution for duplicates\n    age_distribution = Counter(duplicates['age'])\n    \n    # Calculate bins for histogram\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age, max_age + 1.5, 1) - 0.5\n    \n    # Plot histogram\n    plt.figure()\n    sns.histplot(duplicates, x='age', bins=bins, stat='count')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution for Duplicate Names')\n    plt.show()\n    \n    return age_distribution, plt.gca()"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Count duplicates\n    value_counts = df['value'].value_counts()\n    duplicates = value_counts[value_counts > 1]\n    counter = Counter(duplicates.index)\n    \n    # Plot histogram\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6, label='Data')\n    mu, std = norm.fit(df['value'])\n    y = norm.pdf(bins, mu, std)\n    ax.plot(bins, y, 'k-', linewidth=2, label='Normal Distribution')\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    return (counter, ax)"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Generate random values for the DataFrame\n    data = np.random.rand(len(a), len(b))\n    \n    # Create the DataFrame with the given row indices and column names\n    df = pd.DataFrame(data, index=a, columns=b)\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object of the plotted bar chart\n    return ax"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Convert the 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Extract year and month from the 'date' column\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    \n    # Filter the data for the specified year\n    year = df['year'].unique()[0]\n    monthly_data = df[df['year'] == year]\n    \n    # Plot the bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(monthly_data['month'], monthly_data['value'], color='skyblue')\n    plt.title(f'Monthly Data for {year}')\n    plt.xlabel('Month')\n    plt.ylabel('Value')\n    plt.xticks(monthly_data['month'], monthly_data['month'].map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}))\n    plt.grid(axis='y')\n    \n    # Return the axes object\n    return plt.gca()\ndata = [\n    {'date': '2021-01-01', 'value': 10},\n    {'date': '2021-02-01', 'value': 20},\n    {'date': '2021-03-01', 'value': 30},\n    {'date': '2021-04-01', 'value': 40},\n    {'date': '2021-05-01', 'value': 50},\n    {'date': '2021-06-01', 'value': 60},\n    {'date': '2021-07-01', 'value': 70},\n    {'date': '2021-08-01', 'value': 80},\n    {'date': '2021-09-01', 'value': 90},\n    {'date': '2021-10-01', 'value': 100},\n    {'date': '2021-11-01', 'value': 110},\n    {'date': '2021-12-01', 'value': 120}\n]"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the input string to a list of floats\n    data_list = list(map(float, data.split()))\n    \n    # Create a pandas DataFrame from the list\n    df = pd.DataFrame(data_list, columns=['Value'])\n    \n    # Calculate the frequency distribution\n    frequency = df['Value'].value_counts().sort_index()\n    \n    # Create a histogram\n    ax = plt.hist(data_list, bins=np.arange(data_list.min(), data_list.max()+2) - 0.5, edgecolor='black')\n    \n    # Set the title and labels\n    plt.title('Histogram of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 10, array_length)\n    \n    # Generate y values as a sine wave with noise\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n    \n    # Define the function to fit\n    def func(x, a, b, c):\n        return a * np.sin(b * x + c)\n    \n    # Fit the function to the data\n    popt, pcov = curve_fit(func, x, y)\n    \n    # Generate y values for the fitted function\n    y_fit = func(x, *popt)\n    \n    # Plot the original data and the fitted function\n    plt.plot(x, y, 'o', label='data')\n    plt.plot(x, y_fit, '-', label='fit')\n    plt.legend()\n    plt.show()\n    \n    return plt.gca()"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text = ' '.join([row[0] for row in reader])\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The CSV file was not found.\")\n    except IOError:\n        raise IOError(\"An error occurred while reading the file.\")\n    \n    normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n    words = normalized_text.lower().split()\n    word_counts = Counter(words)\n    most_common = word_counts.most_common(10)\n    \n    words, counts = zip(*most_common)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_title('10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    \n    return (ax, most_common)"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n    \n    # Plot the histogram\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    \n    # Plot the probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, loc=0, scale=1)\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    \n    # Add labels and legend\n    ax.set_title('Normal Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n    \n    # Generate a random password for AES encryption\n    password = get_random_bytes(16)\n    \n    # Generate a random nonce for AES encryption\n    nonce = get_random_bytes(16)\n    \n    # Encrypt the private key with AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, _ = cipher.encrypt_and_digest(privkey.save_pkcs1())\n    \n    # Generate a filename based on 8 random bytes\n    random_bytes = os.urandom(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n    \n    # Save the encrypted private key to a file\n    with open(filename, 'wb') as f:\n        f.write(b64encode(nonce + cipher.nonce + ciphertext))\n    \n    # Return the public key, filename, password, and nonce\n    return pubkey, filename, password, nonce\ndecoded_data = b64decode(encrypted_data)\nreceived_nonce = decoded_data[:16]\ncipher = AES.new(password, AES.MODE_GCM, nonce=received_nonce)"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n    \n    # Generate AES key\n    aes_key = os.urandom(32)\n    \n    # Encrypt the file using AES\n    with open(file_path, 'rb') as f:\n        data = f.read()\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(os.urandom(16)), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_data)\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n    \n    # Save the encrypted AES key\n    encrypted_aes_key_path = file_path + '.key.enc'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(encrypted_aes_key)\n    \n    return pubkey, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL cannot be empty\")\n    try:\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Error fetching URL: \" + str(e.reason))\n    doc = pq(html)\n    anchors = doc('a')\n    data = []\n    for anchor in anchors.items():\n        text = anchor.text()\n        href = anchor.attr('href')\n        data.append({'text': text, 'href': href})\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Calculate the end time\n    end_time = datetime.now()\n    start_time = end_time - timedelta(hours=hours)\n    \n    # Generate data for each sensor\n    data = []\n    current_time = start_time\n    while current_time < end_time:\n        row = {'Time': current_time.strftime('%Y-%m-%d %H:%M:%S')}\n        for sensor in SENSORS:\n            # Generate random values for each sensor\n            if sensor == 'Temperature':\n                value = randint(20, 30)\n            elif sensor == 'Humidity':\n                value = randint(30, 70)\n            elif sensor == 'Pressure':\n                value = randint(1000, 1020)\n            row[sensor] = value\n        data.append(row)\n        current_time += timedelta(minutes=1)\n    \n    # Write data to CSV file\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time'] + SENSORS)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    print(f\"Data has been saved to {filename}\")"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for hour in range(hours):\n        row = {'Time': current_time + timedelta(hours=hour)}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(0, 100)\n        data.append(row)\n    \n    # Save to CSV\n    filename = os.path.join(output_dir, 'traffic_data.csv')\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time'] + VEHICLE_TYPES)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Plot data\n    df = pd.DataFrame(data)\n    df['Time'] = pd.to_datetime(df['Time'])\n    df.set_index('Time', inplace=True)\n    \n    ax = df.plot(kind='line')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    \n    return filename, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Create backup directory if it doesn't exist\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n    \n    # Generate current time\n    current_time = datetime.now()\n    \n    # Generate weather data\n    weather_data = []\n    for _ in range(hours):\n        time = current_time + timedelta(hours=_)\n        condition = choice(WEATHER_CONDITIONS)\n        weather_data.append({'Time': time.strftime('%Y-%m-%d %H:%M:%S'), 'Condition': condition})\n    \n    # Create CSV file\n    file_name = f'weather_{current_time.strftime(\"%Y%m%d%H%M%S\")}.csv'\n    file_path = os.path.join(output_dir, file_name)\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Condition'])\n        writer.writeheader()\n        for data in weather_data:\n            writer.writerow(data)\n    \n    # Backup the file\n    backup_file_path = os.path.join(BACKUP_DIR, file_name)\n    shutil.copy2(file_path, backup_file_path)\n    \n    return file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Generate random goals and penalties for each team\n    team_goals = {team: randint(0, goals) for team in TEAMS}\n    team_penalties = {team: randint(0, penalties) for team in TEAMS}\n    \n    # Convert penalties to fines\n    team_fines = {team: penalty * PENALTY_COST for team, penalty in team_penalties.items()}\n    \n    # Create a DataFrame for match results\n    data = {\n        'Team': list(team_goals.keys()),\n        'Goals': list(team_goals.values()),\n        'Penalty Cost': list(team_fines.values())\n    }\n    df = pd.DataFrame(data)\n    \n    # Create plots\n    plot1 = sns.barplot(x='Team', y='Goals', data=df)\n    plot2 = sns.barplot(x='Team', y='Penalty Cost', data=df)\n    \n    # Return the DataFrame and the list of plots\n    return df, [plot1, plot2]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count the non-zero values in each column\n    non_zero_counts = df.replace(0, np.nan).count().dropna()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(non_zero_counts.index, non_zero_counts.values)\n    ax.set_title('Non-zero values in each column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of non-zero values')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Generate a list of courses\n    courses = ['Math', 'Science', 'History', 'English', 'Art']\n    \n    # Generate a list of students\n    students = [f'Student {i+1}' for i in range(num_students)]\n    \n    # Generate random grades for each student in each course\n    grades = {course: [np.random.randint(0, 101) for _ in range(num_students)] for course in courses}\n    \n    # Create a DataFrame from the grades\n    df = pd.DataFrame(grades, index=students)\n    \n    # Calculate the average grade in each course\n    avg_grades = df.mean()\n    \n    # Calculate the number of students with a passing grade (>= 60) in each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create a DataFrame to hold the average grades and passing counts\n    summary_df = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Students': passing_counts\n    }, index=courses)\n    \n    # Create a bar plot for the summary DataFrame\n    ax = summary_df.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Course')\n    ax.set_ylabel('Value')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Filter the array to get indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n    \n    # Extract the second column for fitting\n    y = array[indices, 1]\n    \n    # Generate x values (indices)\n    x = np.arange(len(y))\n    \n    # Define the exponential decay function\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Fit the function to the data\n    popt, pcov = optimize.curve_fit(func, x, y)\n    \n    # Plot the original data and the fitted function\n    plt.plot(x, y, 'o', label='data')\n    plt.plot(x, func(x, *popt), 'r', label='fit')\n    plt.legend()\n    plt.show()\n    \n    return popt, plt.gca()"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    def preprocess(text):\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n    processed_texts = [preprocess(text) for text in texts]\n    \n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=1000)\n    X = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    W = nmf.fit_transform(X)\n    H = nmf.components_\n    \n    # Get the most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(H):\n        print(\"Topic %d:\" % (topic_idx + 1))\n        top_features = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(top_features)\n        print(\" \".join(top_features))\n    \n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # Download stopwords if not already downloaded\n    nltk.download('stopwords')\n    # Get the list of stopwords\n    stop_words = set(nltk.corpus.stopwords.words('english'))\n    # If custom stopwords are provided, add them to the stop_words set\n    if stopwords:\n        stop_words.update(stopwords)\n    # Clean and preprocess the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase the text\n        cleaned_text = cleaned_text.lower()\n        # Tokenize the text\n        tokens = nltk.word_tokenize(cleaned_text)\n        # Remove stop words\n        filtered_tokens = [word for word in tokens if word not in stop_words]\n        # Join the tokens back into a string\n        cleaned_text = ' '.join(filtered_tokens)\n        cleaned_texts.append(cleaned_text)\n    # Tokenize the cleaned texts\n    tokenized_texts = [nltk.word_tokenize(text) for text in cleaned_texts]\n    # Train the Word2Vec model\n    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Create a list to hold all the dataframes\n    dfs = []\n    \n    # List all files in the directory\n    files = os.listdir(path)\n    \n    # Filter only JSON files\n    json_files = [f for f in files if f.endswith('.json')]\n    \n    # Sort the JSON files alphabetically\n    json_files.sort()\n    \n    # Process each JSON file\n    for file in json_files:\n        # Read the JSON file\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n        \n        # Convert the JSON data to a DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add a \"Source\" column with the filename\n        df['Source'] = file\n        \n        # Append the DataFrame to the list\n        dfs.append(df)\n    \n    # Concatenate all DataFrames into one\n    df = pd.concat(dfs, ignore_index=True)\n    \n    # Create a \"processed\" subdirectory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n    \n    # Move each JSON file to the \"processed\" subdirectory\n    for file in json_files:\n        shutil.move(os.path.join(path, file), processed_path)\n    \n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the file path\n    file_path = \"task_func_data/Output.txt\"\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    \n    # Open the file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Generate random temperature and humidity data\n        temperature = random.uniform(20, 30)\n        humidity = random.uniform(30, 60)\n        \n        # Get current date and time\n        now = datetime.now()\n        date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n        \n        # Write the data to the CSV file\n        writer.writerow([date_time, temperature, humidity])\n    \n    # Return the path to the CSV file\n    return file_path"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n    # Read the HTML content\n    html_content = response.read()\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html_content, 'html.parser')\n    # Find all the paragraphs in the HTML\n    paragraphs = soup.find_all('p')\n    # Open a CSV file to write the scraped data\n    with open(CSV_FILE_PATH, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Paragraph'])\n        # Write each paragraph to the CSV file\n        for paragraph in paragraphs:\n            writer.writerow([paragraph.get_text()])\n    # Return the path of the CSV file\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data is not a DataFrame\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"data is empty\")\n    \n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(\"target_column is not a column of data\")\n    \n    # Check if data contains only numeric values\n    if not np.issubdtype(data.dtypes[0], np.number):\n        raise ValueError(\"data contains values that are not numeric\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state is not an integer\")\n    \n    # Check if test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"test_size is not between 0 and 1\")\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(data.drop(target_column, axis=1), data[target_column], test_size=test_size, random_state=random_state)\n    \n    # Initialize and train the model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model score of the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nfrom datetime import datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n              rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Generate IDs\n    ids = np.arange(1, 101)\n    \n    # Generate Names\n    names = []\n    for _ in range(100):\n        if np.random.rand() < 0.5:\n            name = np.random.choice(latin_names)\n        else:\n            name = np.random.choice(other_names)\n        # Correct improper encoding\n        name = name.encode('utf-8', 'replace').decode('utf-8')\n        names.append(name)\n    \n    # Generate Date of Birth\n    dates = []\n    for _ in range(100):\n        year = np.random.randint(start_year, end_year + 1)\n        month = np.random.randint(1, 13)\n        day = np.random.randint(1, 28)  # Assuming all months have 28 days for simplicity\n        date = datetime(year, month, day)\n        dates.append(date.strftime('%y-%m-%d %H:%M:%S'))\n    \n    # Generate Email\n    emails = []\n    for name, date in zip(names, dates):\n        # Extract year from date\n        year = date.split('-')[0]\n        # Construct email\n        email = re.sub(r'[^a-zA-Z0-9]', '', name.lower()) + year + '@' + email_domain\n        emails.append(email)\n    \n    # Create DataFrame\n    data = {\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates,\n        'Email': emails\n    }\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a dictionary to hold the means and medians\n    results = defaultdict(lambda: {'mean': None, 'median': None})\n    \n    # Iterate over each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            # Convert the value to a float\n            value = float(value)\n            # Calculate mean and median\n            if results[key]['mean'] is None:\n                results[key]['mean'] = value\n                results[key]['median'] = value\n            else:\n                results[key]['mean'] = (results[key]['mean'] + value) / 2\n                results[key]['median'] = np.median([results[key]['median'], value])\n    \n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=['key', 'mean', 'median'])\n        writer.writeheader()\n        for key, values in results.items():\n            writer.writerow({'key': key, 'mean': values['mean'], 'median': values['median']})"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    for file_path in kwargs.values():\n        if os.path.exists(file_path):\n            if os.path.getsize(file_path) > 0:\n                target_path = Path(target_dir) / Path(file_path).name\n                shutil.copy(file_path, target_path)\n                copied_files.append(str(target_path))\n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a specified directory based on a given pattern and creates new files with altered names while preserving the content.\n\n    Parameters:\n    - directory (str): The directory path where the CSV files are located.\n    - pattern (str): The regex pattern to match the filenames. Default is r'^(.*?)-\\d+\\.csv$'.\n\n    Returns:\n    - new_files (list): A list of strings, where each string is the filename of a new CSV file created by the function.\n    \"\"\"\n    # Define the pattern for matching filenames\n    file_pattern = re.compile(pattern)\n\n    # List to hold the names of the new files\n    new_files = []\n\n    # Get the list of files in the directory\n    files = os.listdir(directory)\n\n    # Iterate over each file in the directory\n    for file in files:\n        # Check if the file matches the pattern\n        match = file_pattern.match(file)\n        if match:\n            # Extract the base name and the number from the filename\n            base_name = match.group(1)\n            number = match.group(2)\n\n            # Create a new filename with the number incremented by 1\n            new_number = str(int(number) + 1)\n            new_filename = f\"{base_name}-{new_number}.csv\"\n\n            # Construct the full paths for the original and new files\n            original_path = os.path.join(directory, file)\n            new_path = os.path.join(directory, new_filename)\n\n            # Copy the content from the original file to the new file\n            with open(original_path, 'r') as original_file, open(new_path, 'w', newline='') as new_file:\n                csv_reader = csv.reader(original_file)\n                csv_writer = csv.writer(new_file)\n                for row in csv_reader:\n                    csv_writer.writerow(row)\n\n            # Append the new filename to the list\n            new_files.append(new_filename)\n\n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    # Compile the regular expression pattern\n    regex = re.compile(pattern)\n    \n    # List to store the directories where files were extracted\n    extracted_dirs = []\n    \n    # Walk through all files in the specified directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file matches the pattern\n            match = regex.match(file)\n            if match:\n                # Extract the prefix part of the filename\n                prefix = match.group(1)\n                \n                # Create the directory name using the prefix\n                dir_name = os.path.join(root, prefix)\n                \n                # Create the directory if it doesn't exist\n                if not os.path.exists(dir_name):\n                    os.makedirs(dir_name)\n                \n                # Construct the full path to the zip file\n                zip_path = os.path.join(root, file)\n                \n                # Extract the zip file to the created directory\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(dir_name)\n                \n                # Add the directory to the list\n                extracted_dirs.append(dir_name)\n    \n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n    # Find files matching the pattern\n    files = glob.glob(pattern)\n    if not files:\n        print(\"No files found matching the pattern.\")\n        return None\n    # Create archive file name\n    archive_name = os.path.join(ARCHIVE_DIR, 'archive_{}.tar.gz'.format(os.getpid()))\n    # Create archive\n    with open(archive_name, 'wb') as f:\n        subprocess.run(['tar', '-czf', '-', *files], stdout=f)\n    # Delete original files\n    for file in files:\n        os.remove(file)\n    return archive_name"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize a Counter to hold the total counts of goals and penalties\n    total_counts = Counter()\n\n    # Check if the CSV file exists\n    if os.path.exists(csv_file_path):\n        with open(csv_file_path, mode='r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                # Convert goals and penalties to integers and add to the total counts\n                total_counts['goals'] += int(row['goals'])\n                total_counts['penalties'] += int(row['penalties'])\n    else:\n        print(f\"CSV file not found: {csv_file_path}\")\n\n    # Add the given goals and penalties to the total counts\n    total_counts['goals'] += goals\n    total_counts['penalties'] += penalties\n\n    return total_counts"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    moved_files_count = 0\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), os.path.join(target_dir, filename))\n            moved_files_count += 1\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean the texts\n    text1_clean = ALPHANUMERIC.sub(' ', text1).lower()\n    text2_clean = ALPHANUMERIC.sub(' ', text2).lower()\n    \n    # Tokenize the texts\n    words1 = text1_clean.split()\n    words2 = text2_clean.split()\n    \n    # Compute term frequencies\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n    \n    # Get the unique terms\n    unique_terms = set(tf1.keys()) | set(tf2.keys())\n    \n    # Create vectors for each text\n    vector1 = [tf1.get(term, 0) for term in unique_terms]\n    vector2 = [tf2.get(term, 0) for term in unique_terms]\n    \n    # Compute cosine similarity\n    dot_product = sum(a*b for a, b in zip(vector1, vector2))\n    magnitude1 = np.sqrt(sum(a*a for a in vector1))\n    magnitude2 = np.sqrt(sum(b*b for b in vector2))\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 != 0 and magnitude2 != 0 else 0\n    \n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(text1_clean, text2_clean)\n    \n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of integers.\")\n    if not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"All elements in the list must be integers.\")\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"All numbers must be non-negative.\")\n    \n    # Calculate factorial of each number\n    factorials = [math.factorial(num) for num in numbers]\n    \n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n    \n    # Calculate the sum of factorials for each permutation\n    sum_fac_perms = [sum(factorials) for _ in perms]\n    \n    return sum_fac_perms, perms"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n    for extension in EXTENSIONS:\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*{extension}'))\n        for file in files:\n            try:\n                shutil.copy2(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item for item, _, _ in data]\n    counts = [count for _, count, _ in data]\n    weights = [weight for _, _, weight in data]\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(np.array(weights).reshape(-1,1)).flatten()\n    \n    # Create a pandas DataFrame with the results\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize a list to hold the means for each position\n    means = []\n    \n    # Determine the number of positions based on the first tuple\n    num_positions = len(data_list[0]) if data_list else 0\n    \n    # Iterate over each position\n    for i in range(num_positions):\n        # Extract the values at the current position from all tuples\n        values = [item[i] for item in data_list if len(item) > i]\n        \n        # Filter out non-numeric values\n        numeric_values = [x for x in values if isinstance(x, (int, float))]\n        \n        # Calculate the mean of numeric values\n        if numeric_values:\n            mean_value = np.mean(numeric_values)\n        else:\n            mean_value = np.nan\n        \n        # Append the mean to the list\n        means.append(mean_value)\n    \n    # Create a DataFrame with the means\n    df = pd.DataFrame({'Mean Value': means}, index=[f'Position {i}' for i in range(num_positions)])\n    \n    return df\ndata = [('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Column(s) {col1} and/or {col2} not found in the DataFrame.\")\n    \n    # Check if both columns are categorical\n    if not data[col1].dtype.name.startswith('category') or not data[col2].dtype.name.startswith('category'):\n        raise TypeError(\"Both columns must be categorical.\")\n    \n    # Check if both columns have multiple categories\n    if len(data[col1].cat.categories) < 2 or len(data[col2].cat.categories) < 2:\n        raise ValueError(\"Both columns must have multiple categories.\")\n    \n    # Check if any category has less than 5 observations\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    if (contingency_table < 5).sum().sum() > 0:\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Perform chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    results = [random.randint(1, 6) for _ in range(rolls)]\n    frequency = np.bincount(results, minlength=7)[1:]\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=6, range=(0.5, 6.5), align='mid', rwidth=0.8)\n    ax.set_title('Histogram of Dice Rolls')\n    ax.set_xlabel('Dice Value')\n    ax.set_ylabel('Frequency')\n    return frequency, ax"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Create a zip file in the target directory\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the source directory\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                # Check if the file has the '_processed' suffix\n                if re.search(r'_processed$', file):\n                    # Construct the full path of the file\n                    file_path = os.path.join(root, file)\n                    # Add the file to the zip archive\n                    zipf.write(file_path, os.path.relpath(file_path, source_dir))\n    \n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Sort the dataframe by date\n    df = df.sort_values(by='Date')\n    \n    # Create a new column 'Day' which is the day number since the first date\n    df['Day'] = (df['Date'] - df['Date'].min()).dt.days\n    \n    # Prepare the data for the linear regression model\n    X = df[['Day']]\n    y = df['Close']\n    \n    # Fit the linear regression model\n    model = LinearRegression().fit(X, y)\n    \n    # Predict the next 7 days\n    last_day = df['Day'].max()\n    future_days = np.array(range(last_day + 1, last_day + 8)).reshape(-1, 1)\n    future_prices = model.predict(future_days)\n    \n    # Plot the data\n    plt.figure(figsize=(10,5))\n    plt.plot(df['Date'], df['Close'], label='Historical Prices')\n    plt.plot(df['Date'].max() + pd.to_timedelta(np.arange(1,8), unit='d'), future_prices, label='Predicted Prices', color='red')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.title('Stock Prices')\n    plt.legend()\n    plt.show()\n    \n    # Return the predicted prices and the plot\n    return (future_prices.tolist(), plt.gca())"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    # Calculate Z-Scores\n    z_scores = np.abs(zscore(df['closing_price']))\n    \n    # Identify outliers\n    outliers = df[z_scores > z_threshold]\n    \n    # Plot outliers\n    fig, ax = plt.subplots()\n    ax.scatter(outliers.index, outliers['closing_price'], color='red')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    This function is designed to help understand the spread, central tendency, and the distribution shape of stock closing prices.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing stock data with a 'Close' column.\n\n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot and the second for the histogram.\n    \"\"\"\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Box plot\n    sns.boxplot(x=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return axes[0], axes[1]"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Assuming the DataFrame has a column named 'Close' which contains the closing prices\n    # Convert the DataFrame to a Series for easier manipulation\n    series = df['Close']\n    \n    # Fit an ARIMA model to the data\n    model = ARIMA(series, order=(5,1,0))  # You can adjust the order parameters as needed\n    model_fit = model.fit()\n    \n    # Make predictions for the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    \n    # Plot the forecast\n    plt.figure(figsize=(10,5))\n    plt.plot(series, label='Historical Prices')\n    plt.plot(forecast, label='Forecasted Prices', color='red')\n    plt.legend()\n    plt.title('Forecasted Share Closing Prices')\n    plt.xlabel('Days')\n    plt.ylabel('Price')\n    axes = plt.gca()\n    \n    return forecast.tolist(), axes"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of the lowercase English alphabets\n    two_letter_combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Count the frequency of each two-letter combination in the given word\n    word_combinations = [word[i:i+2] for i in range(len(word)-1)]\n    combination_counts = Counter(word_combinations)\n    \n    # Create a dictionary with all possible two-letter combinations and their counts in the word\n    result = {combo: combination_counts.get(combo, 0) for combo in two_letter_combinations}\n    \n    return result"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with 'Date' and 'Category' columns\n    df = pd.DataFrame({'Date': date_range, 'Category': np.random.choice(categories, periods)})\n    \n    # Generate random sales data\n    df['Sales'] = np.random.randint(100, 1000, periods)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate a time series of sales data if sales_data is not provided\n    if sales_data is None:\n        # Create a date range\n        dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n        # Generate some sales data (random values for demonstration)\n        sales = np.random.rand(periods) * 100  # Random sales data between 0 and 100\n        sales_data = pd.DataFrame({'date': dates, 'sales': sales})\n    \n    # Convert the date column to datetime\n    sales_data['date'] = pd.to_datetime(sales_data['date'])\n    \n    # Set the date column as the index\n    sales_data.set_index('date', inplace=True)\n    \n    # Fit a linear regression model\n    X = sales_data.index.to_julian_date().values.reshape(-1, 1)\n    y = sales_data['sales'].values\n    model = LinearRegression().fit(X, y)\n    \n    # Generate future dates\n    future_dates = pd.date_range(start=sales_data.index.max(), periods=periods, freq=freq)\n    \n    # Convert future dates to julian date\n    future_X = future_dates.to_julian_date().values.reshape(-1, 1)\n    \n    # Predict future sales\n    future_sales = model.predict(future_X)\n    \n    # Create a dataframe for future sales\n    future_sales_data = pd.DataFrame({'date': future_dates, 'sales': future_sales})\n    \n    # Set the date column as the index\n    future_sales_data.set_index('date', inplace=True)\n    \n    return future_sales_data['sales'].values"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(task_list, n_tasks, employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"], seed=None):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    if seed is not None:\n        random.seed(seed)\n    current_date = datetime.today().date()\n    assigned_tasks = []\n    for _ in range(n_tasks):\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        assigned_tasks.append({\n            'Task Name': task.replace(\" \", \"_\"),\n            'Assigned To': employee,\n            'Due Date': current_date\n        })\n    return pd.DataFrame(assigned_tasks)"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Replace spaces with underscores and convert to lowercase\n    modified_text = re.sub(r'\\s+', '_', text.lower())\n    \n    # Split the text into words\n    words = modified_text.split('_')\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n    \n    # Extract words and their frequencies\n    words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency Plot')\n    \n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n    \n    # Define the valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create a list to hold the files to be zipped\n    files_to_zip = []\n    \n    # Walk through the source directory and collect files with valid extensions\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            _, ext = os.path.splitext(file)\n            if ext.lower() in valid_extensions:\n                files_to_zip.append(os.path.join(root, file))\n    \n    # Create the target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # Create the zip file path\n    zip_path = os.path.join(target_directory, zip_name + '.zip')\n    \n    # Create a ZipFile object\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Write each file to the zip\n        for file in files_to_zip:\n            zipf.write(file, os.path.relpath(file, source_directory))\n    \n    # Return the full path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if source directory exists\n    source_path = Path(source_directory)\n    if not source_path.exists():\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n    \n    # Create target directory if it doesn't exist\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # List of supported file extensions\n    supported_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Counter for successful conversions\n    success_count = 0\n    \n    # Iterate over all files in the source directory\n    for file in source_path.iterdir():\n        if file.is_file():\n            # Check if the file has a supported extension\n            if file.suffix.lower() in supported_extensions:\n                # Determine the target file path\n                target_file = target_path / file.name.replace(file.suffix, '.csv')\n                \n                # Read the file based on its extension\n                if file.suffix.lower() == '.txt':\n                    with open(file, 'r', encoding='utf-8') as f:\n                        text = f.read()\n                elif file.suffix.lower() == '.docx':\n                    doc = docx.Document(file)\n                    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n                elif file.suffix.lower() == '.xlsx':\n                    df = pd.read_excel(file)\n                    text = df.to_string(index=False)\n                elif file.suffix.lower() == '.csv':\n                    with open(file, 'r', encoding='utf-8') as f:\n                        reader = csv.reader(f)\n                        text = '\\n'.join([','.join(row) for row in reader])\n                \n                # Write the text to a CSV file\n                with open(target_file, 'w', newline='', encoding='utf-8') as f:\n                    writer = csv.writer(f)\n                    writer.writerow(['Text'])\n                    writer.writerow([text])\n                \n                # Increment the success counter\n                success_count += 1\n    \n    return success_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if df.isnull().any().any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    scaler = MinMaxScaler()\n    for col in df.columns:\n        df[col] = scaler.fit_transform(df[[col]]).flatten()\n    return df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # List all entries in the directory\n    entries = os.listdir(directory_path)\n    \n    # Initialize an empty list to store the results\n    result = []\n    \n    # Iterate over each entry\n    for entry in entries:\n        # Construct the full path of the entry\n        entry_path = os.path.join(directory_path, entry)\n        \n        # Check if the entry is a file\n        if os.path.isfile(entry_path):\n            # Get file size in bytes\n            file_size = os.path.getsize(entry_path)\n            \n            # Get creation time\n            # Note: os.path.getctime is used for creation time, but it might not be available on all platforms\n            # For more consistent behavior, we can use os.stat to get the creation time\n            try:\n                creation_time = os.path.getctime(entry_path)\n            except AttributeError:\n                # If getctime is not available, use modification time as a fallback\n                creation_time = os.path.getmtime(entry_path)\n            \n            # Get modification time\n            modification_time = os.path.getmtime(entry_path)\n            \n            # Convert times to ISO format\n            creation_time_iso = datetime.fromtimestamp(creation_time, timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, timezone.utc).isoformat()\n            \n            # Append the tuple to the result list\n            result.append((entry, file_size, creation_time_iso, modification_time_iso))\n    \n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Malformed JSON data\")\n    \n    if not data:\n        raise ValueError(\"Empty JSON data\")\n    \n    country_populations = []\n    for item in data:\n        if not isinstance(item, dict) or len(item) != 2:\n            raise ValueError(\"Invalid JSON format\")\n        country, population = list(item.items())[0]\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name\")\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Invalid population value\")\n        country_populations.append({\"Country\": country, \"Population\": math.floor(population) if isinstance(population, float) else population})\n    \n    df = pd.DataFrame(country_populations)\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    output_path = os.path.join(output_dir, file_name)\n    df.to_csv(output_path, index=False)\n    \n    return output_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if directory exists and is a directory\n    dir_path = Path(dir_path)\n    if not dir_path.is_dir():\n        raise FileNotFoundError(f\"The specified directory does not exist or is not a directory: {dir_path}\")\n\n    # Deduplicate predicates\n    unique_predicates = list(set(predicates))\n\n    # Check if any valid predicates are provided\n    if not unique_predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n\n    # Evaluate each item in the directory against the predicates\n    result = {}\n    for item in dir_path.iterdir():\n        item_name = item.name\n        for predicate in unique_predicates:\n            if re.match(predicate, item_name):\n                result[item_name] = True\n                break\n        else:\n            result[item_name] = False\n\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hex to bytes\n    try:\n        bytes_data = binascii.unhexlify(hex_string)\n    except binascii.Error:\n        return {'error': 'Invalid hexadecimal string'}\n    \n    # Convert bytes to UTF-8 string\n    try:\n        utf8_string = bytes_data.decode('utf-8')\n    except UnicodeDecodeError:\n        return {'error': 'Cannot decode bytes to UTF-8'}\n    \n    # Encode UTF-8 string to various formats\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        'ASCII': 'Not representable in ASCII' if not utf8_string.isascii() else utf8_string,\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot_13')\n    }\n    \n    return encodings"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate MD5 checksum\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        md5 = hashlib.md5(f.read()).hexdigest()\n\n    # Validate MD5 checksum\n    if md5 != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match the expected value.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Extraction failed: {e}\")\n        return False\n\n    return True"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Open the CSV file and read its contents\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            \n            # Check if the specified column_name exists in the CSV file\n            if column_name not in reader.fieldnames:\n                # Delete the downloaded file\n                os.remove(csv_file_path)\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count the occurrences of each value in the specified column\n            value_counts = collections.Counter()\n            for row in reader:\n                value = row.get(column_name)\n                if value is not None:\n                    value_counts[value] += 1\n            \n            # Delete the downloaded file\n            os.remove(csv_file_path)\n            \n            # Return the dictionary of value counts\n            return dict(value_counts)\n    \n    except Exception as e:\n        # Delete the downloaded file if it exists\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n        # Re-raise the exception\n        raise e"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch the XML file from {url}: {e.reason}\")\n\n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the root element is 'root' and contains 'item' elements\n    if root.tag != 'root':\n        raise ValueError(\"XML structure does not match expected format. Root element should be 'root'.\")\n\n    items = root.findall('item')\n    if not items:\n        raise ValueError(\"No 'item' elements found in the XML.\")\n\n    # Extract data from 'item' elements\n    data = []\n    for item in items:\n        row = {}\n        for child in item:\n            row[child.tag] = child.text\n        data.append(row)\n\n    # Create DataFrame from the extracted data\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the specified URL\n    with urllib.request.urlopen(url) as response:\n        text = response.read().decode('utf-8')\n    \n    # Use a basic regular expression to find all words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    most_common = word_counts.most_common(10)\n    \n    # Extract words and their counts for plotting\n    words, counts = zip(*most_common)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Words')\n    \n    # Return the word counts and the axes object\n    return word_counts, ax\nurl = 'https://raw.githubusercontent.com/yourusername/yourrepo/main/yourfile.txt'"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Check if the download_path exists, if not, create it\n        if not os.path.exists(download_path):\n            os.makedirs(download_path)\n        \n        # Send a GET request to the URL\n        response = requests.get(url)\n        \n        # Check if the request was successful\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Check if the content type is 'application/zip'\n        if 'application/zip' not in response.headers.get('Content-Type', ''):\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Define the path to save the downloaded ZIP file\n        zip_path = os.path.join(download_path, 'downloaded.zip')\n        \n        # Save the content to a file\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n        \n        # Try to open the ZIP file\n        try:\n            with ZipFile(zip_path, 'r') as zip_ref:\n                # Extract all contents to the download_path\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            os.remove(zip_path)  # Remove the corrupted ZIP file\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        # Remove the ZIP file after extraction\n        os.remove(zip_path)\n        \n        # Return the path to the directory containing the extracted contents\n        return download_path\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.exceptions.HTTPError as e:\n        raise ValueError(f\"HTTP request failed: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()\n\n    headers = []\n    for th in rows[0].find_all('th'):\n        headers.append(th.text.strip())\n\n    data = []\n    for row in rows[1:]:\n        cells = row.find_all(['th', 'td'])\n        row_data = []\n        for cell in cells:\n            row_data.append(cell.text.strip())\n        data.append(row_data)\n\n    df = pd.DataFrame(data, columns=headers)\n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    # Create download directory if it doesn't exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    # Create unzip directory if it doesn't exist\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    try:\n        # Download the zip file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception occurred - {}\".format(e), [])\n    \n    try:\n        # Save the zip file\n        zip_path = DOWNLOAD_DIR / filename\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n    except IOError as e:\n        return (\"Error: File-related exception occurred - {}\".format(e), [])\n    \n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        # List files in the unzipped directory\n        files = [f.name for f in zip_ref.infolist()]\n        return (\"Success\", files)\n    except zipfile.BadZipFile as e:\n        return (\"Error: Invalid zip file - {}\".format(e), [])\n    except Exception as e:\n        return (\"Error: An unexpected error occurred - {}\".format(e), [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n        return 0\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find all <a> tags\n    links = soup.find_all('a')\n    \n    # Set to store unique absolute URLs\n    unique_links = set()\n    \n    # Iterate through each <a> tag\n    for link in links:\n        # Extract the href attribute\n        href = link.get('href')\n        \n        # If href is not None, join it with the base URL to get the absolute URL\n        if href is not None:\n            absolute_url = urljoin(base_url, href)\n            unique_links.add(absolute_url)\n    \n    # Write the unique absolute URLs to a CSV file\n    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        for url in unique_links:\n            writer.writerow([url])\n    \n    # Return the number of unique absolute links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content from the URL\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    try:\n        # Parse the HTML content\n        tree = html.fromstring(response.content)\n        # Find the table in the HTML\n        table = tree.xpath('//table')[0]\n    except Exception as e:\n        print(f\"Error parsing the HTML: {e}\")\n        return 0\n\n    try:\n        # Convert the table to a pandas DataFrame\n        df = pd.read_html(html.tostring(table), flavor='lxml')[0]\n    except Exception as e:\n        print(f\"Error converting table to DataFrame: {e}\")\n        return 0\n\n    if df.empty:\n        print(\"The table is empty.\")\n        return 0\n\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        # Replace the table with the new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error connecting to or writing to the database: {e}\")\n        return 0\n    finally:\n        conn.close()\n\n    return len(df)"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nimport io\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n        # Extract text using OCR\n        text = pytesseract.image_to_string(image)\n        # Convert the text encoding\n        try:\n            text = text.encode(from_encoding).decode(to_encoding)\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\") from e\n        return text\n    except:\n        # If OCR extraction fails, try to extract from image comment\n        try:\n            with open(filename, 'rb') as f:\n                image_data = f.read()\n            # Extract comment from image\n            comment = Image.open(io.BytesIO(image_data))._getexif()\n            # Convert the comment encoding\n            try:\n                comment = comment.encode(from_encoding).decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\") from e\n            return comment\n        except:\n            # If both OCR extraction and comment processing fail, return an empty string\n            return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Failed to fetch data from {url}: {e}\")\n\n    content = response.content\n    if not content:\n        return {}\n\n    if from_encoding is None:\n        try:\n            encoding = chardet.detect(content)['encoding']\n            if encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n        except Exception as e:\n            raise Exception(f\"Error detecting encoding: {e}\")\n    else:\n        encoding = from_encoding\n\n    try:\n        decoded_content = content.decode(encoding)\n    except UnicodeDecodeError as e:\n        raise Exception(f\"Decoding error: {e}\")\n\n    try:\n        json_data = json.loads(decoded_content)\n    except json.JSONDecodeError as e:\n        raise Exception(f\"JSON parsing error: {e}\")\n\n    if to_encoding != \"utf8\":\n        try:\n            json_data = json.dumps(json_data, ensure_ascii=False).encode(to_encoding)\n            json_data = json_data.decode(to_encoding)\n        except Exception as e:\n            raise Exception(f\"Encoding error: {e}\")\n\n    return json.loads(json_data)"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n    \n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    \n    current_date = datetime.now().date()\n    filtered_df = df[df[column_name].dt.date == current_date]\n    \n    sorted_df = filtered_df.sort_values(by=column_name)\n    \n    return sorted_df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL/TLS\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode()\n        \n        # Check if the file exists\n        if not os.path.exists(file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n        \n        # Open the file in binary mode\n        with open(file_path, 'rb') as file:\n            # Read the file in chunks and compute SHA256 hash\n            sha256_hash = hashlib.sha256()\n            while chunk := file.read(buffer_size):\n                sha256_hash.update(chunk)\n        \n        # Send the SHA256 hash back to the client\n        hash_hex = sha256_hash.hexdigest().encode()\n        secure_socket.sendall(hash_hex)\n        \n        return hash_hex.decode()\n    \n    except Exception as e:\n        # Handle any exceptions that occur during processing\n        error_message = f'An error occurred: {str(e)}'\n        secure_socket.sendall(error_message.encode())\n        return error_message"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5):\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server.bind((server_address, server_port))\n    server.listen(5)\n    server.setblocking(0)\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n    while datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 1)\n        for s in readable:\n            if s is server:\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    message_queues[s].put(data)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                s.send(next_msg + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").encode())\n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n    server.close()\n    return f\"Server ran for {run_duration} seconds.\""}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Receive the message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode()\n    # Ask for sender's email\n    sender_email = input(\"Enter your email: \")\n    # Ask for recipient's email\n    recipient_email = input(\"Enter recipient's email: \")\n    # Ask for sender's email password\n    sender_password = getpass.getpass(\"Enter your email password: \")\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(message)\n    msg['Subject'] = 'Test Email'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    # Connect to the SMTP server\n    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n    server.starttls()\n    # Login to the SMTP server\n    server.login(sender_email, sender_password)\n    # Send the email\n    server.send_message(msg)\n    # Close the connection\n    server.quit()\n    # Output None\n    return None"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if the CSV has a header\n        if 'Text' in df.columns:\n            text_data = df['Text']\n        else:\n            text_data = df.iloc[:, 0]\n        \n        # Initialize the CountVectorizer with the predefined stopwords\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the text data\n        X = vectorizer.fit_transform(text_data)\n        \n        # Get the feature names (words)\n        feature_names = vectorizer.get_feature_names_out()\n        \n        # Sum the counts across all documents\n        counts = X.sum(axis=0)\n        \n        # Create a DataFrame with words and their counts\n        word_counts = pd.DataFrame(counts.T, index=feature_names, columns=['Count'])\n        \n        # Sort the DataFrame by count in descending order\n        word_counts = word_counts.sort_values(by='Count', ascending=False)\n        \n        # Get the top 10 words\n        top_words = word_counts.head(10)\n        \n        # Plot the histogram\n        plt.figure(figsize=(10, 6))\n        plt.barh(top_words.index, top_words['Count'], color='skyblue')\n        plt.xlabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        \n        if save_path:\n            plt.savefig(save_path)\n            print(f\"Plot saved to {save_path}\")\n            return None\n        else:\n            return plt.gca()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['dog', 'cat', 'bird']\n    if foods is None:\n        foods = ['meat', 'fish', 'vegetables']\n    \n    if not animals or not foods:\n        return pd.DataFrame()\n    \n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n    \n    df = pd.DataFrame(combinations, columns=['animal', 'food'])\n    df['combined'] = df['animal'] + ':' + df['food']\n    \n    return df[['combined']]"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert the timezone\n    tz = pytz.timezone(timezone)\n    time_list = []\n    for time_string in time_strings:\n        time = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        time = tz.localize(time)\n        time_list.append(time)\n\n    # Calculate the time differences\n    time_diffs = []\n    for i in range(len(time_list)-1):\n        diff = time_list[i+1] - time_list[i]\n        time_diffs.append(diff.total_seconds())\n\n    # Calculate the mean of the time differences\n    if len(time_diffs) == 0:\n        return 0.0\n    else:\n        return np.mean(time_diffs)"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation using regular expression\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_words)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return list(top_words), ax\ntext = \"This is a test text. This text is just a test.\""}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n    urls = url_pattern.findall(myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    # Try to fetch the first URL\n    url = urls[0]\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the content with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find the title tag\n    title = soup.title\n    if title is None:\n        return \"No title tag found in the webpage.\"\n    \n    return title.string"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON string to find the URL associated with the specified key\n    data = json.loads(json_data)\n    url = data.get(unknown_key)\n    \n    if url is None:\n        return \"Error: URL not found for the specified key.\"\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    \n    if response.status_code != 200:\n        return f\"Error: Failed to download file from {url}. Status code: {response.status_code}\"\n    \n    # Generate a timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    else:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n    \n    # Save the file with the timestamped filename\n    file_path = os.path.join(save_dir, filename)\n    with open(file_path, 'wb') as f:\n        f.write(response.content)\n    \n    return file_path"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    # Fetch the content from the URL\n    response = requests.get(url)\n    content = response.text\n\n    # Use BeautifulSoup to parse the HTML content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n\n    # Find all script tags\n    script_tags = soup.find_all('script')\n\n    # Initialize a list to hold phone numbers\n    phone_numbers = []\n\n    # Regular expression pattern for matching phone numbers\n    pattern = re.compile(r'(\\+?\\d{1,3}[-\\.\\s]??\\d{1,4}[-\\.\\s]??\\d{1,4}[-\\.\\s]??\\d{1,9})')\n\n    # Iterate through each script tag\n    for script in script_tags:\n        # Extract text from the script tag\n        text = script.string\n        if text:\n            # Find all matches of the phone number pattern in the text\n            matches = pattern.findall(text)\n            # Extend the list with the found phone numbers\n            phone_numbers.extend(matches)\n\n    # Write the list of phone numbers to a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    # Return the list of phone numbers\n    return phone_numbers"}
